{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **[Chapter 02]**  \n",
        "**┗ 당뇨 환자 병원 재방문 예측**\n",
        "---\n",
        "> **목차(Context)**\n",
        "\n",
        "* 문제상황 및 데이터 살펴보기\n",
        "* 문제해결 프로세스 정의\n",
        "* 🥉Session 1 - 「Data 전처리 및 EDA」\n",
        "* 🥈Session 2 - 「Error analysis」\n",
        "* 🥇Session 3 - 「주제그룹 분석」"
      ],
      "metadata": {
        "id": "f8Id8v-dWg__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ▶ Warnings 제거\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ▶ Google drive mount or 폴더 클릭 후 구글드라이브 연결\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# ▶ 경로 설정 (※ Colab을 활성화시켰다면 보통 Colab Notebooks 폴더가 자동 생성)\n",
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/인공지능/ML_Project_Collection/당뇨환재재입원예측\")\n",
        "os.getcwd()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Xsho6OTtW8sm",
        "outputId": "cc034a5b-c08f-4383-bbed-1a52b2b841c5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/인공지능/ML_Project_Collection/당뇨환재재입원예측'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **문제상황 및 데이터 살펴보기**\n",
        "---\n",
        "> **시나리오** 🏭\n",
        "\n",
        "```\n",
        "당뇨병은 세계 성인 인구의 9% 이상에 영향을 미치고 있으며, 점점 더 증가하고 있다.\n",
        "당뇨 환자는 지속적인 관찰과 치료가 필요하며, 중증 환자는 더 세심한 관리가 필요하다.\n",
        "일단, 우리의 목적은 30일 이내에 재입원해야 하는 환자를 예측하는 모델을 생성하는 것이다.\n",
        "병원에서 중증 환자의 모니터링, 조기 예약, 간호사 호출과 같은 예방 조치를 더 많이 취할 수 있다고 생각해보자.\n",
        "이는, 많은 생명을 구할 수 있는 일이 될지도 모른다.\n",
        "우리는 당뇨 환자의 재입원을 예측하는 모델을 만들어보고, 모델의 성능 향상을 위해 에러의 분포를 분석하여 성능을 향상시켜보자.\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "6kfhtLT7XED8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# openml API를 사용하여 당뇨 환자 데이터 읽어오기\n",
        "from sklearn.datasets import fetch_openml\n",
        "X_orig, y = fetch_openml(data_id=43874, as_frame=True, return_X_y=True)"
      ],
      "metadata": {
        "id": "9aA_X2duXCEK"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_orig.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIc4I9slXT6Q",
        "outputId": "7acc8852-cff8-4920-aeaa-fc9fb16d080c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 101766 entries, 0 to 101765\n",
            "Data columns (total 24 columns):\n",
            " #   Column                    Non-Null Count   Dtype   \n",
            "---  ------                    --------------   -----   \n",
            " 0   race                      101766 non-null  object  \n",
            " 1   gender                    101766 non-null  object  \n",
            " 2   age                       101766 non-null  object  \n",
            " 3   discharge_disposition_id  101766 non-null  object  \n",
            " 4   admission_source_id       101766 non-null  object  \n",
            " 5   time_in_hospital          101766 non-null  int64   \n",
            " 6   medical_specialty         101766 non-null  object  \n",
            " 7   num_lab_procedures        101766 non-null  int64   \n",
            " 8   num_procedures            101766 non-null  int64   \n",
            " 9   num_medications           101766 non-null  int64   \n",
            " 10  primary_diagnosis         101766 non-null  object  \n",
            " 11  number_diagnoses          101766 non-null  int64   \n",
            " 12  max_glu_serum             101766 non-null  object  \n",
            " 13  A1Cresult                 101766 non-null  object  \n",
            " 14  insulin                   101766 non-null  object  \n",
            " 15  change                    101766 non-null  object  \n",
            " 16  diabetesMed               101766 non-null  object  \n",
            " 17  medicare                  101766 non-null  category\n",
            " 18  medicaid                  101766 non-null  category\n",
            " 19  had_emergency             101766 non-null  category\n",
            " 20  had_inpatient_days        101766 non-null  category\n",
            " 21  had_outpatient_days       101766 non-null  category\n",
            " 22  readmitted                101766 non-null  object  \n",
            " 23  readmit_binary            101766 non-null  int64   \n",
            "dtypes: category(5), int64(6), object(13)\n",
            "memory usage: 15.2+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **데이터 살펴보기**\n",
        "\n",
        "* 환자 입원 당시 측정 데이터\n",
        "* 데이터 명세 ⬇\n",
        "\n",
        "|Column|Description|\n",
        "|:---|:---|\n",
        "|Features|환자 입원 기간 내 관련 정보|\n",
        "|Class|readmit_30_days|"
      ],
      "metadata": {
        "id": "J6i227ScXZKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 각 컬럼별로 값을 실제로 확인해본다.\n",
        "X_orig.sample(n=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "38W3jQnzXVXf",
        "outputId": "9f4c5dab-674f-4893-9530-b8efb27b7dfc"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            race  gender            age discharge_disposition_id  \\\n",
              "66230  Caucasian  Female  Over 60 years                    Other   \n",
              "27779  Caucasian  Female  Over 60 years       Discharged to Home   \n",
              "99219      Other    Male  Over 60 years       Discharged to Home   \n",
              "65334  Caucasian  Female    30-60 years                    Other   \n",
              "46032  Caucasian  Female  Over 60 years                    Other   \n",
              "\n",
              "      admission_source_id  time_in_hospital medical_specialty  \\\n",
              "66230            Referral                 3           Missing   \n",
              "27779           Emergency                 8  InternalMedicine   \n",
              "99219           Emergency                 5           Missing   \n",
              "65334            Referral                 8           Missing   \n",
              "46032            Referral                 7             Other   \n",
              "\n",
              "       num_lab_procedures  num_procedures  num_medications primary_diagnosis  \\\n",
              "66230                  59               5               22             Other   \n",
              "27779                  61               0               19             Other   \n",
              "99219                  64               3               18             Other   \n",
              "65334                   6               0                5             Other   \n",
              "46032                  30               3               14             Other   \n",
              "\n",
              "       number_diagnoses max_glu_serum A1Cresult insulin change diabetesMed  \\\n",
              "66230                 9          None      None      No     No         Yes   \n",
              "27779                 9          None        >8  Steady     No         Yes   \n",
              "99219                 8          None      Norm      Up     Ch         Yes   \n",
              "65334                 6          None      None  Steady     No         Yes   \n",
              "46032                 9          None      None      No     Ch         Yes   \n",
              "\n",
              "      medicare medicaid had_emergency had_inpatient_days had_outpatient_days  \\\n",
              "66230    False    False         False              False               False   \n",
              "27779     True    False         False               True               False   \n",
              "99219    False    False         False              False               False   \n",
              "65334     True    False          True               True               False   \n",
              "46032    False    False         False              False                True   \n",
              "\n",
              "      readmitted  readmit_binary  \n",
              "66230        >30               1  \n",
              "27779         NO               0  \n",
              "99219        <30               1  \n",
              "65334        >30               1  \n",
              "46032         NO               0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4610d91b-4ddd-4c50-8a78-82be98554b68\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>race</th>\n",
              "      <th>gender</th>\n",
              "      <th>age</th>\n",
              "      <th>discharge_disposition_id</th>\n",
              "      <th>admission_source_id</th>\n",
              "      <th>time_in_hospital</th>\n",
              "      <th>medical_specialty</th>\n",
              "      <th>num_lab_procedures</th>\n",
              "      <th>num_procedures</th>\n",
              "      <th>num_medications</th>\n",
              "      <th>primary_diagnosis</th>\n",
              "      <th>number_diagnoses</th>\n",
              "      <th>max_glu_serum</th>\n",
              "      <th>A1Cresult</th>\n",
              "      <th>insulin</th>\n",
              "      <th>change</th>\n",
              "      <th>diabetesMed</th>\n",
              "      <th>medicare</th>\n",
              "      <th>medicaid</th>\n",
              "      <th>had_emergency</th>\n",
              "      <th>had_inpatient_days</th>\n",
              "      <th>had_outpatient_days</th>\n",
              "      <th>readmitted</th>\n",
              "      <th>readmit_binary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>66230</th>\n",
              "      <td>Caucasian</td>\n",
              "      <td>Female</td>\n",
              "      <td>Over 60 years</td>\n",
              "      <td>Other</td>\n",
              "      <td>Referral</td>\n",
              "      <td>3</td>\n",
              "      <td>Missing</td>\n",
              "      <td>59</td>\n",
              "      <td>5</td>\n",
              "      <td>22</td>\n",
              "      <td>Other</td>\n",
              "      <td>9</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>&gt;30</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27779</th>\n",
              "      <td>Caucasian</td>\n",
              "      <td>Female</td>\n",
              "      <td>Over 60 years</td>\n",
              "      <td>Discharged to Home</td>\n",
              "      <td>Emergency</td>\n",
              "      <td>8</td>\n",
              "      <td>InternalMedicine</td>\n",
              "      <td>61</td>\n",
              "      <td>0</td>\n",
              "      <td>19</td>\n",
              "      <td>Other</td>\n",
              "      <td>9</td>\n",
              "      <td>None</td>\n",
              "      <td>&gt;8</td>\n",
              "      <td>Steady</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>NO</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99219</th>\n",
              "      <td>Other</td>\n",
              "      <td>Male</td>\n",
              "      <td>Over 60 years</td>\n",
              "      <td>Discharged to Home</td>\n",
              "      <td>Emergency</td>\n",
              "      <td>5</td>\n",
              "      <td>Missing</td>\n",
              "      <td>64</td>\n",
              "      <td>3</td>\n",
              "      <td>18</td>\n",
              "      <td>Other</td>\n",
              "      <td>8</td>\n",
              "      <td>None</td>\n",
              "      <td>Norm</td>\n",
              "      <td>Up</td>\n",
              "      <td>Ch</td>\n",
              "      <td>Yes</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>&lt;30</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65334</th>\n",
              "      <td>Caucasian</td>\n",
              "      <td>Female</td>\n",
              "      <td>30-60 years</td>\n",
              "      <td>Other</td>\n",
              "      <td>Referral</td>\n",
              "      <td>8</td>\n",
              "      <td>Missing</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>Other</td>\n",
              "      <td>6</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Steady</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>&gt;30</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46032</th>\n",
              "      <td>Caucasian</td>\n",
              "      <td>Female</td>\n",
              "      <td>Over 60 years</td>\n",
              "      <td>Other</td>\n",
              "      <td>Referral</td>\n",
              "      <td>7</td>\n",
              "      <td>Other</td>\n",
              "      <td>30</td>\n",
              "      <td>3</td>\n",
              "      <td>14</td>\n",
              "      <td>Other</td>\n",
              "      <td>9</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>No</td>\n",
              "      <td>Ch</td>\n",
              "      <td>Yes</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>NO</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4610d91b-4ddd-4c50-8a78-82be98554b68')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4610d91b-4ddd-4c50-8a78-82be98554b68 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4610d91b-4ddd-4c50-8a78-82be98554b68');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f565e3b0-8b53-4f1b-8a26-c7320d7afc77\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f565e3b0-8b53-4f1b-8a26-c7320d7afc77')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f565e3b0-8b53-4f1b-8a26-c7320d7afc77 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **문제해결 프로세스 정의**\n",
        "---\n",
        "> **문제정의**\n",
        "\n",
        "```\n",
        "▶ 퇴원한 환자들 중에 다시 입원하는 환자들이 늘어남\n",
        "```  \n",
        "\n",
        "> **기대효과**\n",
        "\n",
        "```\n",
        "▶ 재입원 환자 예측을 통해 중증 환자 사전 조치 및 모니터링\n",
        "▶ 중증 환자 생존율 증가\n",
        "```\n",
        "\n",
        "> **해결방안**\n",
        "\n",
        "```\n",
        "▶ Binary classification을 통해 30일 이내 재입원할 환자를 분류\n",
        "▶ Session 1 🥉\n",
        " - 인코딩을 통한 데이터 준비\n",
        " - 베이스 모델 생성하기\n",
        "▶ Session 2 🥈\n",
        " - Class weight가 오류에 미치는 영향 분석\n",
        " - 하이퍼파라미터 튜닝\n",
        "▶ Session 3 🥇\n",
        " - 에러를 서로 다른 주제 그룹으로 나눠보기\n",
        " - Feature importance 알아보기\n",
        "\n",
        "기타: 에러를 줄일 수 있는 몇가지 방법에 대한 이야기\n",
        "```\n",
        "\n",
        "> **성과측정**  \n",
        "\n",
        "```\n",
        "▶ 에러 분석을 통한 점진적 성능 향상\n",
        "```\n",
        "\n",
        "> **현업적용**  \n",
        "\n",
        "```\n",
        "▶ 모델 성능 하락시 분석 포인트 탐색\n",
        "```\n",
        "\n",
        "> **주요 핵심 미리 살펴보기**  \n",
        "\n",
        "```\n",
        "▶ Session 1 → OneHotEncoder를 활용한 Feature 생성\n",
        "▶ Session 2 → LightGBMClassifier를 활용한 베이스 모델 생성, optuna를 활용한 Hyperparameter 튜닝\n",
        "▶ Session 3 → 일반적인 에러 분석, Cohort 에러 분석\n",
        "```"
      ],
      "metadata": {
        "id": "ZkCKZaI-XrKO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Spec Check**\n",
        "---\n",
        "> **Data 가공 명세서**\n",
        "\n",
        "<img src = \"https://drive.google.com/uc?id=10ghjKrqQWBHS4dYzol3IIU1CzJ_vv4Ih\">"
      ],
      "metadata": {
        "id": "6-tGBfdWXtXh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **🥉Session 1**  \n",
        "**┗ Data 전처리 및 EDA**  \n",
        "---"
      ],
      "metadata": {
        "id": "sgOdWreOYV3q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-0. 몇 가지 필요한 Library 설치"
      ],
      "metadata": {
        "id": "CZTIEk7vYXVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install raiwidgets lightgbm optuna shap --quiet"
      ],
      "metadata": {
        "id": "7ovccTfvXfjI"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-1. 필요한 패키지 Import"
      ],
      "metadata": {
        "id": "bcJxwh4VYc0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import raiwidgets\n",
        "print(dir(raiwidgets))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXjHKSePZ0xh",
        "outputId": "be4c9d03-3634-4c62-eaa4-e87b8b18e971"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ErrorAnalysisDashboard', 'ExplanationDashboard', 'FairnessDashboard', 'ModelAnalysisDashboard', 'ModelPerformanceDashboard', 'ResponsibleAIDashboard', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', 'constants', 'dashboard', 'error_analysis_constants', 'error_analysis_dashboard', 'error_analysis_dashboard_input', 'error_handling', 'explanation_constants', 'explanation_dashboard', 'explanation_dashboard_input', 'fairness_dashboard', 'fairness_metric_calculation', 'interfaces', 'model_analysis_dashboard', 'model_performance_dashboard', 'responsibleai_dashboard', 'responsibleai_dashboard_input', 'version']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "pd.set_option('display.max_columns',50)\n",
        "pd.set_option('display.max_rows',50)\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "\n",
        "import lightgbm as lgb\n",
        "import optuna\n",
        "\n",
        "from responsibleai import RAIInsights\n",
        "from raiwidgets import ResponsibleAIDashboard\n",
        "# from raiwidgets.cohort import Cohort, CohortFilter, CohortFilterMethods\n",
        "import shap\n",
        "\n",
        "from IPython.display import Markdown, display\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "u3fF3wE-YZLo"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-2. Target 유의 변수 제거"
      ],
      "metadata": {
        "id": "qIcgNXKtMZIu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "_Target 변수와 상관관계가 높은 feature 2개를 제거한다_"
      ],
      "metadata": {
        "id": "pOgQdsZNMacJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_orig['readmitted'].value_counts()"
      ],
      "metadata": {
        "id": "dDkrijUbYjt3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f3c747b-ca6f-46b8-c8b9-d235096f3f58"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NO     54864\n",
              ">30    35545\n",
              "<30    11357\n",
              "Name: readmitted, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_orig['readmit_binary'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d17FmjKJMe3E",
        "outputId": "0b8b6023-1b3b-4195-cc68-e60a258bf3f2"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    54864\n",
              "1    46902\n",
              "Name: readmit_binary, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "_Actual Target_"
      ],
      "metadata": {
        "id": "nHqNcVKdMmfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfO67TjcMls8",
        "outputId": "6375addb-7cad-44b4-d378-88ecfeda9fbd"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    90409\n",
              "1    11357\n",
              "Name: readmit_30_days, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "위에서 보면 알 수 있듯이 2개의 변수는 y와 직접적으로 상관관계가 보이므로 학습 Feature에 사용할 수 없다.\n",
        "y와 직접적으로 상관관계가 있는 변수를 feature로 사용할 경우, 모든 경우에 100%에 가까운 정확도를 보이게 되며\n",
        "이는 정답을 보여주는 것과 같기 때문에 정상적인모델이라고 할 수 없다.\n",
        "```"
      ],
      "metadata": {
        "id": "n49cdCbGMrce"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-3. 인코딩을 통한 데이터 준비"
      ],
      "metadata": {
        "id": "INt-NlTuM1ba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#인코딩을 위한 함수 정의\n",
        "def transform_data(X_):\n",
        "    #Remove alternative target columns\n",
        "    X = X_.drop(['readmitted', 'readmit_binary'], axis=1)\n",
        "\n",
        "    #Binary encode boolean columns\n",
        "    bool_cols_l = X.select_dtypes(include=[\"category\"]).columns.tolist()\n",
        "    X[bool_cols_l] = X[bool_cols_l].astype(str).replace({\"True\":1, \"False\":0})\n",
        "    print(bool_cols_l)\n",
        "\n",
        "    #One hot encode categorical columns\n",
        "    cat_cols_l = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
        "    print(cat_cols_l)\n",
        "    ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "    ohe_np = ohe.fit_transform(X[cat_cols_l].astype(\"category\"))\n",
        "    X[ohe.get_feature_names_out(cat_cols_l)] = ohe_np.astype(int)\n",
        "\n",
        "    #Drop original categorical columns\n",
        "    X.drop(cat_cols_l, axis=1, inplace=True)\n",
        "\n",
        "    return X"
      ],
      "metadata": {
        "id": "B1-bdnBMMpOU"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ".\n",
        "\n",
        "이 함수는 데이터를 변환하기 위한 것으로 보입니다. 구체적으로 다음과 같은 작업을 수행합니다:\n",
        "\n",
        "1. **대상 변수 제거**: `readmitted`와 `readmit_binary`라는 두 개의 열을 제거합니다. 이 두 열은 대상 변수로 추정됩니다.\n",
        "2. **이진 인코딩**: 카테고리형 데이터를 갖는 모든 열을 선택하고, 이들을 문자열로 변환한 후 \"True\"는 1로, \"False\"는 0으로 변환합니다.\n",
        "3. **원-핫 인코딩**: 문자열 데이터를 갖는 모든 열을 선택하고, 이들을 원-핫 인코딩합니다. 이렇게 생성된 새로운 열들은 원본 데이터프레임에 추가됩니다.\n",
        "4. **원본 범주형 열 제거**: 원-핫 인코딩 후 원본의 범주형 열들은 더 이상 필요하지 않으므로 제거합니다.\n",
        "\n",
        "몇 가지 주의점 및 제안사항:\n",
        "- `X_`는 원본 데이터이며, 함수 내에서 이를 복사하여 `X`에 저장하여 작업을 진행합니다. 이렇게 하면 원본 데이터에 영향을 주지 않고 작업을 수행할 수 있습니다.\n",
        "- `bool_cols_l`와 `cat_cols_l` 두 리스트를 출력하는 코드가 있습니다. 이는 디버깅 또는 확인을 위한 것으로 보이며, 필요하지 않을 경우 제거할 수 있습니다.\n",
        "- 원-핫 인코딩을 할 때 `handle_unknown='ignore'` 옵션을 사용하였습니다. 이는 모델 훈련 시에는 나타나지 않았지만 실제 사용 시에 나타날 수 있는 새로운 범주값을 무시하겠다는 의미입니다.\n",
        "\n",
        "전반적으로 코드는 깔끔하게 작성되었으며, 주어진 작업을 잘 수행할 것으로 보입니다. 특별히 수정할 부분은 보이지 않습니다."
      ],
      "metadata": {
        "id": "0yfJ28TzPuHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = transform_data(X_orig)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O19LX_7SN2_z",
        "outputId": "74e2acfd-f969-4210-c4b9-db62692daac1"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['medicare', 'medicaid', 'had_emergency', 'had_inpatient_days', 'had_outpatient_days']\n",
            "['race', 'gender', 'age', 'discharge_disposition_id', 'admission_source_id', 'medical_specialty', 'primary_diagnosis', 'max_glu_serum', 'A1Cresult', 'insulin', 'change', 'diabetesMed']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Number of Features : {len(X_orig.columns)}')\n",
        "print(X_orig.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xLrWai3OKVi",
        "outputId": "9220e2a3-a5f1-4772-a226-555d37788822"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Features : 24\n",
            "Index(['race', 'gender', 'age', 'discharge_disposition_id',\n",
            "       'admission_source_id', 'time_in_hospital', 'medical_specialty',\n",
            "       'num_lab_procedures', 'num_procedures', 'num_medications',\n",
            "       'primary_diagnosis', 'number_diagnoses', 'max_glu_serum', 'A1Cresult',\n",
            "       'insulin', 'change', 'diabetesMed', 'medicare', 'medicaid',\n",
            "       'had_emergency', 'had_inpatient_days', 'had_outpatient_days',\n",
            "       'readmitted', 'readmit_binary'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Number of Features(After processing) :  {len(X.columns)}')\n",
        "print(X.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2h1x7HJgOtxZ",
        "outputId": "db3bbdef-6342-4f6c-9ef8-86ee56fb659b"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Features(After processing) :  54\n",
            "Index(['time_in_hospital', 'num_lab_procedures', 'num_procedures',\n",
            "       'num_medications', 'number_diagnoses', 'medicare', 'medicaid',\n",
            "       'had_emergency', 'had_inpatient_days', 'had_outpatient_days',\n",
            "       'race_AfricanAmerican', 'race_Asian', 'race_Caucasian', 'race_Hispanic',\n",
            "       'race_Other', 'race_Unknown', 'gender_Female', 'gender_Male',\n",
            "       'gender_Unknown/Invalid', 'age_30 years or younger', 'age_30-60 years',\n",
            "       'age_Over 60 years', 'discharge_disposition_id_Discharged to Home',\n",
            "       'discharge_disposition_id_Other', 'admission_source_id_Emergency',\n",
            "       'admission_source_id_Other', 'admission_source_id_Referral',\n",
            "       'medical_specialty_Cardiology', 'medical_specialty_Emergency/Trauma',\n",
            "       'medical_specialty_Family/GeneralPractice',\n",
            "       'medical_specialty_InternalMedicine', 'medical_specialty_Missing',\n",
            "       'medical_specialty_Other', 'primary_diagnosis_Diabetes',\n",
            "       'primary_diagnosis_Genitourinary Issues',\n",
            "       'primary_diagnosis_Musculoskeletal Issues', 'primary_diagnosis_Other',\n",
            "       'primary_diagnosis_Respiratory Issues', 'max_glu_serum_>200',\n",
            "       'max_glu_serum_>300', 'max_glu_serum_None', 'max_glu_serum_Norm',\n",
            "       'A1Cresult_>7', 'A1Cresult_>8', 'A1Cresult_None', 'A1Cresult_Norm',\n",
            "       'insulin_Down', 'insulin_No', 'insulin_Steady', 'insulin_Up',\n",
            "       'change_Ch', 'change_No', 'diabetesMed_No', 'diabetesMed_Yes'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-4. 데이터셋 분할"
      ],
      "metadata": {
        "id": "JkF5etxuPFjm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rand = 42\n",
        "os.environ['PYTHONHASHSEED']=str(rand)\n",
        "np.random.seed(rand)"
      ],
      "metadata": {
        "id": "3k_mvM0xO8iB"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes_l = ['NO or > 30 days', '< 30 days']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=rand)\n",
        "X_orig_test = X_orig.loc[X_test.index]"
      ],
      "metadata": {
        "id": "-W2WYVQ-PMsZ"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "id": "bvxTECeyPT_a",
        "outputId": "dfe062d3-7568-4bb6-c772-74306dcdfb41"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       time_in_hospital  num_lab_procedures  num_procedures  num_medications  \\\n",
              "15992                10                  45               0               14   \n",
              "10606                 4                  39               0               11   \n",
              "64779                 1                  37               2                7   \n",
              "83257                 8                  45               0               24   \n",
              "4204                 12                  51               0               12   \n",
              "...                 ...                 ...             ...              ...   \n",
              "6265                  2                  35               0               12   \n",
              "54886                 5                  63               2               23   \n",
              "76820                 3                  55               1               33   \n",
              "860                  12                  77               2               21   \n",
              "15795                 1                   2               5               17   \n",
              "\n",
              "       number_diagnoses  medicare  medicaid  had_emergency  \\\n",
              "15992                 9         0         0              0   \n",
              "10606                 5         0         0              0   \n",
              "64779                 5         1         0              0   \n",
              "83257                 9         1         0              1   \n",
              "4204                  7         0         0              0   \n",
              "...                 ...       ...       ...            ...   \n",
              "6265                  9         0         0              0   \n",
              "54886                 9         0         0              0   \n",
              "76820                 9         1         0              0   \n",
              "860                   9         0         0              0   \n",
              "15795                 7         0         0              0   \n",
              "\n",
              "       had_inpatient_days  had_outpatient_days  race_AfricanAmerican  \\\n",
              "15992                   0                    0                     0   \n",
              "10606                   0                    0                     0   \n",
              "64779                   0                    0                     0   \n",
              "83257                   1                    0                     0   \n",
              "4204                    0                    0                     0   \n",
              "...                   ...                  ...                   ...   \n",
              "6265                    0                    0                     0   \n",
              "54886                   0                    0                     0   \n",
              "76820                   0                    0                     0   \n",
              "860                     0                    0                     0   \n",
              "15795                   0                    0                     0   \n",
              "\n",
              "       race_Asian  race_Caucasian  race_Hispanic  race_Other  race_Unknown  \\\n",
              "15992           0               1              0           0             0   \n",
              "10606           0               1              0           0             0   \n",
              "64779           0               1              0           0             0   \n",
              "83257           0               1              0           0             0   \n",
              "4204            0               1              0           0             0   \n",
              "...           ...             ...            ...         ...           ...   \n",
              "6265            0               1              0           0             0   \n",
              "54886           0               1              0           0             0   \n",
              "76820           0               1              0           0             0   \n",
              "860             0               1              0           0             0   \n",
              "15795           0               1              0           0             0   \n",
              "\n",
              "       gender_Female  gender_Male  gender_Unknown/Invalid  \\\n",
              "15992              1            0                       0   \n",
              "10606              1            0                       0   \n",
              "64779              1            0                       0   \n",
              "83257              1            0                       0   \n",
              "4204               1            0                       0   \n",
              "...              ...          ...                     ...   \n",
              "6265               0            1                       0   \n",
              "54886              0            1                       0   \n",
              "76820              0            1                       0   \n",
              "860                1            0                       0   \n",
              "15795              1            0                       0   \n",
              "\n",
              "       age_30 years or younger  age_30-60 years  age_Over 60 years  \\\n",
              "15992                        0                0                  1   \n",
              "10606                        0                0                  1   \n",
              "64779                        0                0                  1   \n",
              "83257                        0                0                  1   \n",
              "4204                         0                0                  1   \n",
              "...                        ...              ...                ...   \n",
              "6265                         0                0                  1   \n",
              "54886                        0                0                  1   \n",
              "76820                        0                0                  1   \n",
              "860                          0                0                  1   \n",
              "15795                        0                1                  0   \n",
              "\n",
              "       discharge_disposition_id_Discharged to Home  \\\n",
              "15992                                            1   \n",
              "10606                                            1   \n",
              "64779                                            1   \n",
              "83257                                            0   \n",
              "4204                                             0   \n",
              "...                                            ...   \n",
              "6265                                             1   \n",
              "54886                                            1   \n",
              "76820                                            0   \n",
              "860                                              0   \n",
              "15795                                            1   \n",
              "\n",
              "       discharge_disposition_id_Other  admission_source_id_Emergency  ...  \\\n",
              "15992                               0                              1  ...   \n",
              "10606                               0                              1  ...   \n",
              "64779                               0                              0  ...   \n",
              "83257                               1                              1  ...   \n",
              "4204                                1                              1  ...   \n",
              "...                               ...                            ...  ...   \n",
              "6265                                0                              1  ...   \n",
              "54886                               0                              0  ...   \n",
              "76820                               1                              0  ...   \n",
              "860                                 1                              1  ...   \n",
              "15795                               0                              0  ...   \n",
              "\n",
              "       medical_specialty_Family/GeneralPractice  \\\n",
              "15992                                         1   \n",
              "10606                                         0   \n",
              "64779                                         0   \n",
              "83257                                         0   \n",
              "4204                                          0   \n",
              "...                                         ...   \n",
              "6265                                          0   \n",
              "54886                                         0   \n",
              "76820                                         0   \n",
              "860                                           1   \n",
              "15795                                         0   \n",
              "\n",
              "       medical_specialty_InternalMedicine  medical_specialty_Missing  \\\n",
              "15992                                   0                          0   \n",
              "10606                                   0                          0   \n",
              "64779                                   1                          0   \n",
              "83257                                   0                          1   \n",
              "4204                                    0                          1   \n",
              "...                                   ...                        ...   \n",
              "6265                                    0                          0   \n",
              "54886                                   0                          1   \n",
              "76820                                   0                          0   \n",
              "860                                     0                          0   \n",
              "15795                                   0                          0   \n",
              "\n",
              "       medical_specialty_Other  primary_diagnosis_Diabetes  \\\n",
              "15992                        0                           0   \n",
              "10606                        1                           0   \n",
              "64779                        0                           0   \n",
              "83257                        0                           0   \n",
              "4204                         0                           0   \n",
              "...                        ...                         ...   \n",
              "6265                         0                           0   \n",
              "54886                        0                           0   \n",
              "76820                        1                           0   \n",
              "860                          0                           0   \n",
              "15795                        0                           0   \n",
              "\n",
              "       primary_diagnosis_Genitourinary Issues  \\\n",
              "15992                                       0   \n",
              "10606                                       0   \n",
              "64779                                       0   \n",
              "83257                                       0   \n",
              "4204                                        0   \n",
              "...                                       ...   \n",
              "6265                                        0   \n",
              "54886                                       0   \n",
              "76820                                       0   \n",
              "860                                         0   \n",
              "15795                                       0   \n",
              "\n",
              "       primary_diagnosis_Musculoskeletal Issues  primary_diagnosis_Other  \\\n",
              "15992                                         0                        1   \n",
              "10606                                         0                        1   \n",
              "64779                                         0                        1   \n",
              "83257                                         0                        1   \n",
              "4204                                          0                        1   \n",
              "...                                         ...                      ...   \n",
              "6265                                          0                        0   \n",
              "54886                                         0                        1   \n",
              "76820                                         1                        0   \n",
              "860                                           0                        0   \n",
              "15795                                         0                        1   \n",
              "\n",
              "       primary_diagnosis_Respiratory Issues  max_glu_serum_>200  \\\n",
              "15992                                     0                   0   \n",
              "10606                                     0                   0   \n",
              "64779                                     0                   0   \n",
              "83257                                     0                   0   \n",
              "4204                                      0                   0   \n",
              "...                                     ...                 ...   \n",
              "6265                                      1                   0   \n",
              "54886                                     0                   0   \n",
              "76820                                     0                   0   \n",
              "860                                       1                   0   \n",
              "15795                                     0                   0   \n",
              "\n",
              "       max_glu_serum_>300  max_glu_serum_None  max_glu_serum_Norm  \\\n",
              "15992                   0                   1                   0   \n",
              "10606                   0                   1                   0   \n",
              "64779                   0                   1                   0   \n",
              "83257                   0                   1                   0   \n",
              "4204                    0                   1                   0   \n",
              "...                   ...                 ...                 ...   \n",
              "6265                    0                   1                   0   \n",
              "54886                   0                   1                   0   \n",
              "76820                   0                   1                   0   \n",
              "860                     0                   1                   0   \n",
              "15795                   0                   1                   0   \n",
              "\n",
              "       A1Cresult_>7  A1Cresult_>8  A1Cresult_None  A1Cresult_Norm  \\\n",
              "15992             0             0               1               0   \n",
              "10606             0             0               1               0   \n",
              "64779             0             0               1               0   \n",
              "83257             0             0               1               0   \n",
              "4204              0             0               1               0   \n",
              "...             ...           ...             ...             ...   \n",
              "6265              0             0               1               0   \n",
              "54886             0             0               1               0   \n",
              "76820             0             0               1               0   \n",
              "860               0             0               1               0   \n",
              "15795             0             0               1               0   \n",
              "\n",
              "       insulin_Down  insulin_No  insulin_Steady  insulin_Up  change_Ch  \\\n",
              "15992             0           1               0           0          1   \n",
              "10606             0           1               0           0          0   \n",
              "64779             0           1               0           0          0   \n",
              "83257             1           0               0           0          1   \n",
              "4204              0           1               0           0          0   \n",
              "...             ...         ...             ...         ...        ...   \n",
              "6265              0           1               0           0          0   \n",
              "54886             0           0               0           1          1   \n",
              "76820             1           0               0           0          1   \n",
              "860               1           0               0           0          1   \n",
              "15795             0           0               1           0          1   \n",
              "\n",
              "       change_No  diabetesMed_No  diabetesMed_Yes  \n",
              "15992          0               0                1  \n",
              "10606          1               1                0  \n",
              "64779          1               1                0  \n",
              "83257          0               0                1  \n",
              "4204           1               0                1  \n",
              "...          ...             ...              ...  \n",
              "6265           1               1                0  \n",
              "54886          0               0                1  \n",
              "76820          0               0                1  \n",
              "860            0               0                1  \n",
              "15795          0               0                1  \n",
              "\n",
              "[71236 rows x 54 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5b26d894-a741-4091-8d4f-872da5599fa9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>time_in_hospital</th>\n",
              "      <th>num_lab_procedures</th>\n",
              "      <th>num_procedures</th>\n",
              "      <th>num_medications</th>\n",
              "      <th>number_diagnoses</th>\n",
              "      <th>medicare</th>\n",
              "      <th>medicaid</th>\n",
              "      <th>had_emergency</th>\n",
              "      <th>had_inpatient_days</th>\n",
              "      <th>had_outpatient_days</th>\n",
              "      <th>race_AfricanAmerican</th>\n",
              "      <th>race_Asian</th>\n",
              "      <th>race_Caucasian</th>\n",
              "      <th>race_Hispanic</th>\n",
              "      <th>race_Other</th>\n",
              "      <th>race_Unknown</th>\n",
              "      <th>gender_Female</th>\n",
              "      <th>gender_Male</th>\n",
              "      <th>gender_Unknown/Invalid</th>\n",
              "      <th>age_30 years or younger</th>\n",
              "      <th>age_30-60 years</th>\n",
              "      <th>age_Over 60 years</th>\n",
              "      <th>discharge_disposition_id_Discharged to Home</th>\n",
              "      <th>discharge_disposition_id_Other</th>\n",
              "      <th>admission_source_id_Emergency</th>\n",
              "      <th>...</th>\n",
              "      <th>medical_specialty_Family/GeneralPractice</th>\n",
              "      <th>medical_specialty_InternalMedicine</th>\n",
              "      <th>medical_specialty_Missing</th>\n",
              "      <th>medical_specialty_Other</th>\n",
              "      <th>primary_diagnosis_Diabetes</th>\n",
              "      <th>primary_diagnosis_Genitourinary Issues</th>\n",
              "      <th>primary_diagnosis_Musculoskeletal Issues</th>\n",
              "      <th>primary_diagnosis_Other</th>\n",
              "      <th>primary_diagnosis_Respiratory Issues</th>\n",
              "      <th>max_glu_serum_&gt;200</th>\n",
              "      <th>max_glu_serum_&gt;300</th>\n",
              "      <th>max_glu_serum_None</th>\n",
              "      <th>max_glu_serum_Norm</th>\n",
              "      <th>A1Cresult_&gt;7</th>\n",
              "      <th>A1Cresult_&gt;8</th>\n",
              "      <th>A1Cresult_None</th>\n",
              "      <th>A1Cresult_Norm</th>\n",
              "      <th>insulin_Down</th>\n",
              "      <th>insulin_No</th>\n",
              "      <th>insulin_Steady</th>\n",
              "      <th>insulin_Up</th>\n",
              "      <th>change_Ch</th>\n",
              "      <th>change_No</th>\n",
              "      <th>diabetesMed_No</th>\n",
              "      <th>diabetesMed_Yes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>15992</th>\n",
              "      <td>10</td>\n",
              "      <td>45</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10606</th>\n",
              "      <td>4</td>\n",
              "      <td>39</td>\n",
              "      <td>0</td>\n",
              "      <td>11</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64779</th>\n",
              "      <td>1</td>\n",
              "      <td>37</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83257</th>\n",
              "      <td>8</td>\n",
              "      <td>45</td>\n",
              "      <td>0</td>\n",
              "      <td>24</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4204</th>\n",
              "      <td>12</td>\n",
              "      <td>51</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6265</th>\n",
              "      <td>2</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54886</th>\n",
              "      <td>5</td>\n",
              "      <td>63</td>\n",
              "      <td>2</td>\n",
              "      <td>23</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76820</th>\n",
              "      <td>3</td>\n",
              "      <td>55</td>\n",
              "      <td>1</td>\n",
              "      <td>33</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>860</th>\n",
              "      <td>12</td>\n",
              "      <td>77</td>\n",
              "      <td>2</td>\n",
              "      <td>21</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15795</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>17</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>71236 rows × 54 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5b26d894-a741-4091-8d4f-872da5599fa9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5b26d894-a741-4091-8d4f-872da5599fa9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5b26d894-a741-4091-8d4f-872da5599fa9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b25856f7-99c6-430a-8460-3c99b46746a5\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b25856f7-99c6-430a-8460-3c99b46746a5')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b25856f7-99c6-430a-8460-3c99b46746a5 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "id": "Vw31OVzgPd55",
        "outputId": "c5282202-f4ba-414c-911c-001c13674454"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       time_in_hospital  num_lab_procedures  num_procedures  num_medications  \\\n",
              "35956                11                  68               0               20   \n",
              "60927                 1                  20               0                7   \n",
              "79920                 4                  21               3               23   \n",
              "50078                12                  28               0               19   \n",
              "44080                 1                  21               0                6   \n",
              "...                 ...                 ...             ...              ...   \n",
              "19901                 1                  45               1                5   \n",
              "9561                  4                  58               0               10   \n",
              "47211                 2                  68               6               16   \n",
              "25232                 6                  61               2                7   \n",
              "22212                12                  76               6               25   \n",
              "\n",
              "       number_diagnoses  medicare  medicaid  had_emergency  \\\n",
              "35956                 5         0         0              0   \n",
              "60927                 8         0         0              0   \n",
              "79920                 7         0         0              0   \n",
              "50078                 7         0         0              0   \n",
              "44080                 7         0         0              0   \n",
              "...                 ...       ...       ...            ...   \n",
              "19901                 4         0         0              0   \n",
              "9561                  9         0         0              0   \n",
              "47211                 9         1         0              0   \n",
              "25232                 5         1         0              0   \n",
              "22212                 9         0         0              0   \n",
              "\n",
              "       had_inpatient_days  had_outpatient_days  race_AfricanAmerican  \\\n",
              "35956                   0                    0                     0   \n",
              "60927                   0                    0                     0   \n",
              "79920                   1                    1                     0   \n",
              "50078                   1                    0                     0   \n",
              "44080                   0                    0                     1   \n",
              "...                   ...                  ...                   ...   \n",
              "19901                   0                    0                     0   \n",
              "9561                    1                    0                     0   \n",
              "47211                   0                    0                     0   \n",
              "25232                   0                    0                     0   \n",
              "22212                   1                    0                     1   \n",
              "\n",
              "       race_Asian  race_Caucasian  race_Hispanic  race_Other  race_Unknown  \\\n",
              "35956           0               1              0           0             0   \n",
              "60927           0               1              0           0             0   \n",
              "79920           0               1              0           0             0   \n",
              "50078           0               1              0           0             0   \n",
              "44080           0               0              0           0             0   \n",
              "...           ...             ...            ...         ...           ...   \n",
              "19901           0               0              1           0             0   \n",
              "9561            0               1              0           0             0   \n",
              "47211           0               1              0           0             0   \n",
              "25232           0               1              0           0             0   \n",
              "22212           0               0              0           0             0   \n",
              "\n",
              "       gender_Female  gender_Male  gender_Unknown/Invalid  \\\n",
              "35956              1            0                       0   \n",
              "60927              0            1                       0   \n",
              "79920              1            0                       0   \n",
              "50078              0            1                       0   \n",
              "44080              1            0                       0   \n",
              "...              ...          ...                     ...   \n",
              "19901              1            0                       0   \n",
              "9561               1            0                       0   \n",
              "47211              0            1                       0   \n",
              "25232              0            1                       0   \n",
              "22212              0            1                       0   \n",
              "\n",
              "       age_30 years or younger  age_30-60 years  age_Over 60 years  \\\n",
              "35956                        0                0                  1   \n",
              "60927                        0                1                  0   \n",
              "79920                        0                0                  1   \n",
              "50078                        0                1                  0   \n",
              "44080                        0                0                  1   \n",
              "...                        ...              ...                ...   \n",
              "19901                        0                1                  0   \n",
              "9561                         0                0                  1   \n",
              "47211                        0                1                  0   \n",
              "25232                        0                0                  1   \n",
              "22212                        0                0                  1   \n",
              "\n",
              "       discharge_disposition_id_Discharged to Home  \\\n",
              "35956                                            1   \n",
              "60927                                            1   \n",
              "79920                                            0   \n",
              "50078                                            1   \n",
              "44080                                            0   \n",
              "...                                            ...   \n",
              "19901                                            1   \n",
              "9561                                             1   \n",
              "47211                                            1   \n",
              "25232                                            1   \n",
              "22212                                            0   \n",
              "\n",
              "       discharge_disposition_id_Other  admission_source_id_Emergency  ...  \\\n",
              "35956                               0                              0  ...   \n",
              "60927                               0                              0  ...   \n",
              "79920                               1                              0  ...   \n",
              "50078                               0                              0  ...   \n",
              "44080                               1                              1  ...   \n",
              "...                               ...                            ...  ...   \n",
              "19901                               0                              1  ...   \n",
              "9561                                0                              1  ...   \n",
              "47211                               0                              1  ...   \n",
              "25232                               0                              0  ...   \n",
              "22212                               1                              1  ...   \n",
              "\n",
              "       medical_specialty_Family/GeneralPractice  \\\n",
              "35956                                         0   \n",
              "60927                                         0   \n",
              "79920                                         0   \n",
              "50078                                         0   \n",
              "44080                                         0   \n",
              "...                                         ...   \n",
              "19901                                         0   \n",
              "9561                                          0   \n",
              "47211                                         0   \n",
              "25232                                         1   \n",
              "22212                                         0   \n",
              "\n",
              "       medical_specialty_InternalMedicine  medical_specialty_Missing  \\\n",
              "35956                                   1                          0   \n",
              "60927                                   0                          1   \n",
              "79920                                   0                          1   \n",
              "50078                                   0                          0   \n",
              "44080                                   0                          1   \n",
              "...                                   ...                        ...   \n",
              "19901                                   0                          0   \n",
              "9561                                    0                          0   \n",
              "47211                                   0                          0   \n",
              "25232                                   0                          0   \n",
              "22212                                   0                          1   \n",
              "\n",
              "       medical_specialty_Other  primary_diagnosis_Diabetes  \\\n",
              "35956                        0                           1   \n",
              "60927                        0                           0   \n",
              "79920                        0                           0   \n",
              "50078                        1                           0   \n",
              "44080                        0                           0   \n",
              "...                        ...                         ...   \n",
              "19901                        1                           0   \n",
              "9561                         0                           0   \n",
              "47211                        0                           0   \n",
              "25232                        0                           0   \n",
              "22212                        0                           0   \n",
              "\n",
              "       primary_diagnosis_Genitourinary Issues  \\\n",
              "35956                                       0   \n",
              "60927                                       0   \n",
              "79920                                       0   \n",
              "50078                                       0   \n",
              "44080                                       0   \n",
              "...                                       ...   \n",
              "19901                                       0   \n",
              "9561                                        0   \n",
              "47211                                       0   \n",
              "25232                                       0   \n",
              "22212                                       0   \n",
              "\n",
              "       primary_diagnosis_Musculoskeletal Issues  primary_diagnosis_Other  \\\n",
              "35956                                         0                        0   \n",
              "60927                                         0                        1   \n",
              "79920                                         1                        0   \n",
              "50078                                         0                        0   \n",
              "44080                                         0                        1   \n",
              "...                                         ...                      ...   \n",
              "19901                                         0                        1   \n",
              "9561                                          0                        1   \n",
              "47211                                         0                        1   \n",
              "25232                                         0                        0   \n",
              "22212                                         0                        1   \n",
              "\n",
              "       primary_diagnosis_Respiratory Issues  max_glu_serum_>200  \\\n",
              "35956                                     0                   0   \n",
              "60927                                     0                   0   \n",
              "79920                                     0                   0   \n",
              "50078                                     1                   0   \n",
              "44080                                     0                   0   \n",
              "...                                     ...                 ...   \n",
              "19901                                     0                   0   \n",
              "9561                                      0                   0   \n",
              "47211                                     0                   0   \n",
              "25232                                     1                   0   \n",
              "22212                                     0                   0   \n",
              "\n",
              "       max_glu_serum_>300  max_glu_serum_None  max_glu_serum_Norm  \\\n",
              "35956                   0                   1                   0   \n",
              "60927                   0                   1                   0   \n",
              "79920                   0                   1                   0   \n",
              "50078                   0                   1                   0   \n",
              "44080                   0                   1                   0   \n",
              "...                   ...                 ...                 ...   \n",
              "19901                   0                   1                   0   \n",
              "9561                    0                   1                   0   \n",
              "47211                   0                   1                   0   \n",
              "25232                   0                   1                   0   \n",
              "22212                   0                   1                   0   \n",
              "\n",
              "       A1Cresult_>7  A1Cresult_>8  A1Cresult_None  A1Cresult_Norm  \\\n",
              "35956             0             0               1               0   \n",
              "60927             0             0               1               0   \n",
              "79920             0             0               1               0   \n",
              "50078             0             0               1               0   \n",
              "44080             0             0               1               0   \n",
              "...             ...           ...             ...             ...   \n",
              "19901             0             0               1               0   \n",
              "9561              0             0               1               0   \n",
              "47211             0             0               1               0   \n",
              "25232             0             0               1               0   \n",
              "22212             0             0               1               0   \n",
              "\n",
              "       insulin_Down  insulin_No  insulin_Steady  insulin_Up  change_Ch  \\\n",
              "35956             0           0               1           0          0   \n",
              "60927             0           1               0           0          0   \n",
              "79920             0           1               0           0          0   \n",
              "50078             0           1               0           0          0   \n",
              "44080             0           1               0           0          0   \n",
              "...             ...         ...             ...         ...        ...   \n",
              "19901             0           1               0           0          0   \n",
              "9561              0           0               1           0          0   \n",
              "47211             0           1               0           0          0   \n",
              "25232             0           1               0           0          0   \n",
              "22212             0           1               0           0          0   \n",
              "\n",
              "       change_No  diabetesMed_No  diabetesMed_Yes  \n",
              "35956          1               0                1  \n",
              "60927          1               0                1  \n",
              "79920          1               0                1  \n",
              "50078          1               0                1  \n",
              "44080          1               0                1  \n",
              "...          ...             ...              ...  \n",
              "19901          1               1                0  \n",
              "9561           1               0                1  \n",
              "47211          1               1                0  \n",
              "25232          1               1                0  \n",
              "22212          1               1                0  \n",
              "\n",
              "[30530 rows x 54 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-13509771-3d33-4529-8451-090019dc2405\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>time_in_hospital</th>\n",
              "      <th>num_lab_procedures</th>\n",
              "      <th>num_procedures</th>\n",
              "      <th>num_medications</th>\n",
              "      <th>number_diagnoses</th>\n",
              "      <th>medicare</th>\n",
              "      <th>medicaid</th>\n",
              "      <th>had_emergency</th>\n",
              "      <th>had_inpatient_days</th>\n",
              "      <th>had_outpatient_days</th>\n",
              "      <th>race_AfricanAmerican</th>\n",
              "      <th>race_Asian</th>\n",
              "      <th>race_Caucasian</th>\n",
              "      <th>race_Hispanic</th>\n",
              "      <th>race_Other</th>\n",
              "      <th>race_Unknown</th>\n",
              "      <th>gender_Female</th>\n",
              "      <th>gender_Male</th>\n",
              "      <th>gender_Unknown/Invalid</th>\n",
              "      <th>age_30 years or younger</th>\n",
              "      <th>age_30-60 years</th>\n",
              "      <th>age_Over 60 years</th>\n",
              "      <th>discharge_disposition_id_Discharged to Home</th>\n",
              "      <th>discharge_disposition_id_Other</th>\n",
              "      <th>admission_source_id_Emergency</th>\n",
              "      <th>...</th>\n",
              "      <th>medical_specialty_Family/GeneralPractice</th>\n",
              "      <th>medical_specialty_InternalMedicine</th>\n",
              "      <th>medical_specialty_Missing</th>\n",
              "      <th>medical_specialty_Other</th>\n",
              "      <th>primary_diagnosis_Diabetes</th>\n",
              "      <th>primary_diagnosis_Genitourinary Issues</th>\n",
              "      <th>primary_diagnosis_Musculoskeletal Issues</th>\n",
              "      <th>primary_diagnosis_Other</th>\n",
              "      <th>primary_diagnosis_Respiratory Issues</th>\n",
              "      <th>max_glu_serum_&gt;200</th>\n",
              "      <th>max_glu_serum_&gt;300</th>\n",
              "      <th>max_glu_serum_None</th>\n",
              "      <th>max_glu_serum_Norm</th>\n",
              "      <th>A1Cresult_&gt;7</th>\n",
              "      <th>A1Cresult_&gt;8</th>\n",
              "      <th>A1Cresult_None</th>\n",
              "      <th>A1Cresult_Norm</th>\n",
              "      <th>insulin_Down</th>\n",
              "      <th>insulin_No</th>\n",
              "      <th>insulin_Steady</th>\n",
              "      <th>insulin_Up</th>\n",
              "      <th>change_Ch</th>\n",
              "      <th>change_No</th>\n",
              "      <th>diabetesMed_No</th>\n",
              "      <th>diabetesMed_Yes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>35956</th>\n",
              "      <td>11</td>\n",
              "      <td>68</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60927</th>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79920</th>\n",
              "      <td>4</td>\n",
              "      <td>21</td>\n",
              "      <td>3</td>\n",
              "      <td>23</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50078</th>\n",
              "      <td>12</td>\n",
              "      <td>28</td>\n",
              "      <td>0</td>\n",
              "      <td>19</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44080</th>\n",
              "      <td>1</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19901</th>\n",
              "      <td>1</td>\n",
              "      <td>45</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9561</th>\n",
              "      <td>4</td>\n",
              "      <td>58</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47211</th>\n",
              "      <td>2</td>\n",
              "      <td>68</td>\n",
              "      <td>6</td>\n",
              "      <td>16</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25232</th>\n",
              "      <td>6</td>\n",
              "      <td>61</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22212</th>\n",
              "      <td>12</td>\n",
              "      <td>76</td>\n",
              "      <td>6</td>\n",
              "      <td>25</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>30530 rows × 54 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-13509771-3d33-4529-8451-090019dc2405')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-13509771-3d33-4529-8451-090019dc2405 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-13509771-3d33-4529-8451-090019dc2405');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-94858b48-1c14-471a-9b41-b6aebdfbfe45\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-94858b48-1c14-471a-9b41-b6aebdfbfe45')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-94858b48-1c14-471a-9b41-b6aebdfbfe45 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-5. Modeling 및 Evaluation 함수 정의\n",
        "모델을 생성하면 성능에 대한 평가를 위해 evaluation 함수를 정의하고 시작하는 것이 좋다.\n",
        "여기에서는 일반적으로 평가척도에서 많이 사용되는 Accuracy, Precision, Recall, F1 score를 구하는 evaluation 함수를 작성하여 사용하도록 하자.\n",
        "이 함수는 Confusion Matrix를 Heatmap으로 그리는 것도 포함한다.\n"
      ],
      "metadata": {
        "id": "fjplZBoIPgs6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-6. Evaluation 함수 정의"
      ],
      "metadata": {
        "id": "wKAKebMFPrRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_class_mdl(fitted_model, X_train, X_test, y_train, y_test, plot=True, pct=True, thresh=0.5):\n",
        "    y_train_pred = fitted_model.predict(X_train).squeeze()\n",
        "    if len(np.unique(y_train_pred)) > 2:\n",
        "        y_train_pred = np.where(y_train_pred > thresh, 1, 0)\n",
        "        y_test_prob = fitted_model.predict(X_test).squeeze()\n",
        "        y_test_pred = np.where(y_test_prob > thresh, 1, 0)\n",
        "    else:\n",
        "        y_test_prob = fitted_model.predict_proba(X_test)[:,1]\n",
        "        y_test_pred = np.where(y_test_prob > thresh, 1, 0)\n",
        "    roc_auc_te = metrics.roc_auc_score(y_test, y_test_prob)\n",
        "\n",
        "    cf_matrix = metrics.confusion_matrix(y_test, y_test_pred)\n",
        "    tn, fp, fn, tp = cf_matrix.ravel()\n",
        "    acc_tr = metrics.accuracy_score(y_train, y_train_pred)\n",
        "    acc_te = metrics.accuracy_score(y_test, y_test_pred)\n",
        "    pre_te = metrics.precision_score(y_test, y_test_pred)\n",
        "    rec_te = metrics.recall_score(y_test, y_test_pred)\n",
        "    f1_te = metrics.f1_score(y_test, y_test_pred)\n",
        "    mcc_te = metrics.matthews_corrcoef(y_test, y_test_pred)\n",
        "\n",
        "    if plot:\n",
        "        print(f\"Accuracy_train:  {acc_tr:.4f}\\t\\tAccuracy_test:   {acc_te:.4f}\")\n",
        "        print(f\"Precision_test:  {pre_te:.4f}\\t\\tRecall_test:     {rec_te:.4f}\")\n",
        "        print(f\"ROC-AUC_test:    {roc_auc_te:.4f}\\t\\tF1_test:         {f1_te:.4f}\\t\\tMCC_test: {mcc_te:.4f}\")\n",
        "        plt.figure(figsize=(6, 5))\n",
        "        if pct:\n",
        "            ax = sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True,\\\n",
        "                        fmt='.2%', cmap='Blues', annot_kws={'size':16})\n",
        "        else:\n",
        "            ax = sns.heatmap(cf_matrix, annot=True,\\\n",
        "                        fmt='d',cmap='Blues', annot_kws={'size':16})\n",
        "        ax.set_xlabel('Predicted', fontsize=12)\n",
        "        ax.set_ylabel('Observed', fontsize=12)\n",
        "        plt.show()\n",
        "\n",
        "        return y_train_pred, y_test_prob, y_test_pred\n",
        "    else:\n",
        "        t = cf_matrix.sum()\n",
        "        metrics_dict = {'accuracy_train':acc_tr , 'accuracy_test':acc_te, 'precision':pre_te, 'recall':rec_te,\\\n",
        "                      'roc_auc':roc_auc_te,  'f1':f1_te, 'mcc': mcc_te, 'tn%':tn/t, 'fp%':fp/t, 'fn%':fn/t, 'tp%':tp/t }\n",
        "        return metrics_dict"
      ],
      "metadata": {
        "id": "pHTfpkhEPjfs"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "물론입니다. `evaluate_class_mdl` 함수를 line-by-line으로 자세하게 분석하겠습니다.\n",
        "\n",
        "1. **함수 정의**:\n",
        "```python\n",
        "def evaluate_class_mdl(fitted_model, X_train, X_test, y_train, y_test, plot=True, pct=True, thresh=0.5):\n",
        "```\n",
        "- `evaluate_class_mdl`라는 이름의 함수를 정의합니다.\n",
        "- 입력 변수로 학습된 모델, 훈련 및 테스트 데이터, 그리고 세 가지 옵션(시각화 여부, 백분율 표시 여부, 임계값)을 받습니다.\n",
        "\n",
        "2. **훈련 데이터 예측**:\n",
        "```python\n",
        "y_train_pred = fitted_model.predict(X_train).squeeze()\n",
        "```\n",
        "- 학습된 모델을 사용하여 훈련 데이터에 대한 예측값을 구하고, 이 값을 `y_train_pred`에 저장합니다.\n",
        "\n",
        "3. **예측값 유형 확인 및 변환**:\n",
        "```python\n",
        "if len(np.unique(y_train_pred)) > 2:\n",
        "    y_train_pred = np.where(y_train_pred > thresh, 1, 0)\n",
        "    y_test_prob = fitted_model.predict(X_test).squeeze()\n",
        "    y_test_pred = np.where(y_test_prob > thresh, 1, 0)\n",
        "```\n",
        "- 만약 훈련 데이터의 예측값이 이진 값(0 또는 1)이 아니라면, 주어진 임계값(`thresh`)를 기준으로 이진 예측값으로 변환합니다.\n",
        "- 테스트 데이터에 대해서도 마찬가지로 확률 값을 구하고, 임계값을 기준으로 이진 예측값을 생성합니다.\n",
        "\n",
        "4. **이진 예측의 경우**:\n",
        "```python\n",
        "else:   \n",
        "    y_test_prob = fitted_model.predict_proba(X_test)[:,1]\n",
        "    y_test_pred = np.where(y_test_prob > thresh, 1, 0)\n",
        "```\n",
        "- 만약 훈련 데이터의 예측값이 이미 이진 값이라면, 테스트 데이터에 대해서 확률 값을 구하고, 임계값을 기준으로 이진 예측값을 생성합니다.\n",
        "\n",
        "5. **ROC-AUC 계산**:\n",
        "```python\n",
        "roc_auc_te = metrics.roc_auc_score(y_test, y_test_prob)\n",
        "```\n",
        "- 테스트 데이터의 실제값과 확률 예측값을 사용하여 ROC-AUC 값을 계산합니다.\n",
        "\n",
        "6. **혼동 행렬 생성**:\n",
        "```python\n",
        "cf_matrix = metrics.confusion_matrix(y_test, y_test_pred)\n",
        "```\n",
        "- 테스트 데이터의 실제값과 이진 예측값을 사용하여 혼동 행렬을 생성합니다.\n",
        "\n",
        "7. **혼동 행렬 값 추출**:\n",
        "```python\n",
        "tn, fp, fn, tp = cf_matrix.ravel()\n",
        "```\n",
        "- 혼동 행렬에서 True Negative, False Positive, False Negative, True Positive 값을 각각 추출합니다.\n",
        "\n",
        "8. **다양한 성능 지표 계산**:\n",
        "```python\n",
        "acc_tr = metrics.accuracy_score(y_train, y_train_pred)\n",
        "acc_te = metrics.accuracy_score(y_test, y_test_pred)\n",
        "pre_te = metrics.precision_score(y_test, y_test_pred)\n",
        "rec_te = metrics.recall_score(y_test, y_test_pred)\n",
        "f1_te = metrics.f1_score(y_test, y_test_pred)\n",
        "mcc_te = metrics.matthews_corrcoef(y_test, y_test_pred)\n",
        "```\n",
        "- 훈련 데이터와 테스트 데이터에 대한 정확도, 테스트 데이터에 대한 정밀도, 재현율, F1 스코어, 매튜 상관 계수와 같은 성능 지표를 계산합니다.\n",
        "\n",
        "9. **결과 출력 및 시각화**:\n",
        "```python\n",
        "if plot:\n",
        "    print(f\"Accuracy_train:  {acc_tr:.4f}\\t\\tAccuracy_test:   {acc_te:.4f}\")\n",
        "    ...\n",
        "    plt.show()\n",
        "    return y_train_pred, y_test_prob, y_test_pred\n",
        "```\n",
        "- `plot=True`인 경우, 성능 지표를 출력하고 혼동 행렬을 시각화하여 표시합니다.\n",
        "- 시각화 결과를 표시한 후, 훈련 데이터 예측값, 테스트 데이터 확률값, 테스트 데이터 예측값을 반환합니다.\n",
        "\n",
        "10. **성능 지표 반환**:\n",
        "```python\n",
        "else:\n",
        "    ...\n",
        "    return metrics_dict\n",
        "```\n",
        "- `plot=False`인 경우, 계산된 성능 지표들을 딕셔너리 형태로 반환합니다.\n",
        "\n",
        "이렇게 `evaluate_class_mdl` 함수는 주어진 학습된 모델과 데이터셋에 대한 성능을 평가하고, 결과를 출력하거나 반환하는 역할을 합니다."
      ],
      "metadata": {
        "id": "6whfutXBQiHU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`.squeeze()`는 NumPy 배열의 메서드로, 차원 중 크기가 1인 차원을 제거하는 역할을 합니다.\n",
        "\n",
        "예를 들어, 만약 배열의 형태(shape)가 `(n, 1)`이라면, `.squeeze()`를 사용하면 배열의 형태는 `(n,)`이 됩니다.\n",
        "\n",
        "`fitted_model.predict(X_train)`의 결과는 때로는 2차원 배열로 반환될 수 있습니다. 예를 들어, `(n, 1)`와 같은 형태로 말이죠. `.squeeze()`를 사용함으로써 결과를 1차원 배열로 변환하여 연산을 간소화하거나, 다른 함수와의 호환성을 높이기 위해 사용될 수 있습니다.\n",
        "\n",
        "간단한 예제로 설명하겠습니다:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# 2차원 배열 생성\n",
        "arr = np.array([[1], [2], [3], [4]])\n",
        "print(arr.shape)  # (4, 1)\n",
        "\n",
        "# .squeeze()를 사용하여 1차원 배열로 변환\n",
        "squeezed_arr = arr.squeeze()\n",
        "print(squeezed_arr.shape)  # (4,)\n",
        "```\n",
        "\n",
        "따라서, `y_train_pred = fitted_model.predict(X_train).squeeze()`에서 `.squeeze()`는 반환된 예측값 배열을 1차원 배열로 변환하기 위해 사용됩니다."
      ],
      "metadata": {
        "id": "0lUaFrpMUGgp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-7. Base 모델 학습"
      ],
      "metadata": {
        "id": "SD_qcCBUP7O-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "clf = lgb.LGBMClassifier(random_state=rand, n_jobs=-1)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "_ =  evaluate_class_mdl(clf, X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 690
        },
        "id": "TykimZ-8PjdQ",
        "outputId": "d2092c68-9ac4-43f7-93b8-a88028d70e30"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.707875 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "Accuracy_train:  0.8888\t\tAccuracy_test:   0.8877\n",
            "Precision_test:  0.0000\t\tRecall_test:     0.0000\n",
            "ROC-AUC_test:    0.6469\t\tF1_test:         0.0000\t\tMCC_test: -0.0029\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "숫자만 봤을 때는 정확도는 매우 높은 것으로 보이지만,\n",
        "Precision과 Recall은 매우 낮아 전반적인 성능은 나쁘다."
      ],
      "metadata": {
        "id": "w35FUmifQGcZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1-7-1. Class Weights"
      ],
      "metadata": {
        "id": "GRd4IjTVQIWS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "모델은 기본적으로 Positive 클래스를 예측할 때 보수적 접근 방법으로 모델을 생성하기 떄문에 너무 많은 위험을 감수하지 않는다.\n",
        "해결책은 Positive 클래스에 더 많은 비중을 두도록 강제하는 Hyperparameter를 활용하는 것이다.\n",
        "클래스 가중치를 부여할 수 있는 방법은 여러 가지가 있지만, 그 중 하나인 `scale_pos_weight`를 활용해보자.\n",
        "\n",
        "보통 많이 사용하는 값은 아래 수식을 통해 계산한다.\n",
        "\n",
        "$\\large{\\frac{\\text{number of negative samples}}{\\text{number of positive samples}}}$"
      ],
      "metadata": {
        "id": "pZ0sK5FHQJzM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "모델이 클래스 불균형 문제를 처리할 때, 기본 설정으로는 Positive 클래스를 예측하는 데 보수적으로 동작하게 됩니다. 이는 Negative 클래스의 데이터가 많을 경우, 단순히 대부분의 예측을 Negative로 하면 정확도가 높게 나오기 때문입니다. 그러나 이런 접근 방식은 Positive 클래스를 잘못 분류하는 경우가 많아질 수 있으며, 특히 Positive 클래스의 예측이 중요한 상황에서는 이런 접근 방식이 문제가 될 수 있습니다.\n",
        "\n",
        "이러한 문제를 해결하기 위해 클래스의 불균형을 해소하는 다양한 방법이 제안되었습니다. 그 중 하나는 클래스에 가중치를 부여하는 것입니다. 여기서 언급된 `scale_pos_weight`는 그러한 하이퍼파라미터 중 하나로, 주로 XGBoost와 같은 알고리즘에서 사용됩니다.\n",
        "\n",
        "`scale_pos_weight`는 Positive 클래스의 가중치를 조절하는 데 사용되며, 값이 클수록 모델은 Positive 클래스에 더 큰 중요도를 부여합니다. 이 하이퍼파라미터의 값을 설정하는 일반적인 방법은 Negative 샘플 수를 Positive 샘플 수로 나눈 값으로 계산하는 것입니다. 이렇게 계산된 값은 Positive 클래스의 예측에 더 큰 중요도를 부여하도록 모델을 유도합니다.\n",
        "\n",
        "예를 들어, Negative 클래스 샘플이 1000개, Positive 클래스 샘플이 100개인 경우 `scale_pos_weight`는 \\( \\frac{1000}{100} = 10 \\)이 됩니다. 이는 모델에 Positive 클래스를 10배 더 중요하게 여기도록 알려주는 것입니다.\n",
        "\n",
        "이렇게 클래스의 가중치를 조절함으로써 모델이 더 균형 잡힌 예측을 하도록 도와줄 수 있습니다."
      ],
      "metadata": {
        "id": "6oXYNtY7Qvjo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def_scale_pos_weight = len(y[y==0]) / len(y[y==1])\n",
        "print(f\"default scale pos weight: {def_scale_pos_weight:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iG0SuyDaPjaj",
        "outputId": "9cd6d8ae-c69c-4fba-d286-828a747b6654"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default scale pos weight: 7.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 값은 실제로 모델의 성능을 향상시키지만 여전히 상당한 수준의 오탐지가 존재한다."
      ],
      "metadata": {
        "id": "2N99T7jjQbx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = lgb.LGBMClassifier(random_state=rand, n_jobs=-1, scale_pos_weight=def_scale_pos_weight)\n",
        "clf.fit(X_train, y_train)\n",
        "_ =  evaluate_class_mdl(clf, X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 690
        },
        "id": "FcHe7XBBPjXk",
        "outputId": "bdd29a74-0ae9-45e0-ac28-dd412c657ea1"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.194974 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "Accuracy_train:  0.6381\t\tAccuracy_test:   0.6086\n",
            "Precision_test:  0.1638\t\tRecall_test:     0.6064\n",
            "ROC-AUC_test:    0.6458\t\tF1_test:         0.2580\t\tMCC_test: 0.1379\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "그리고 두 배로 만들면 오탐지가 두 배가 된다."
      ],
      "metadata": {
        "id": "CMf59AS4RuE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = lgb.LGBMClassifier(random_state=rand, n_jobs=-1, scale_pos_weight=def_scale_pos_weight*2)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "_ = evaluate_class_mdl(clf, X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 690
        },
        "id": "lPb3lbWBPjVd",
        "outputId": "86ccb55b-11cc-4559-8e9b-bbec8879acbf"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.230456 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "Accuracy_train:  0.3354\t\tAccuracy_test:   0.3157\n",
            "Precision_test:  0.1293\t\tRecall_test:     0.8893\n",
            "ROC-AUC_test:    0.6438\t\tF1_test:         0.2258\t\tMCC_test: 0.0997\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "그리고 절반으로 줄이면 반대로 모델이 더 보수적으로 바뀐다."
      ],
      "metadata": {
        "id": "I5EpQq2xR2y0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = lgb.LGBMClassifier(random_state=rand, n_jobs=-1, scale_pos_weight=def_scale_pos_weight/2)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "_ = evaluate_class_mdl(clf, X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 690
        },
        "id": "4YqihI3oPjSx",
        "outputId": "0da1a76e-aef7-4546-efae-5309bbbf1c88"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018121 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "Accuracy_train:  0.8771\t\tAccuracy_test:   0.8617\n",
            "Precision_test:  0.2232\t\tRecall_test:     0.0937\n",
            "ROC-AUC_test:    0.6485\t\tF1_test:         0.1320\t\tMCC_test: 0.0782\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x500 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgEAAAHECAYAAACgK/n7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABOW0lEQVR4nO3dd1hTVx8H8G/C3igoIKKIGxVQEKrWWdTW0Vo7cBWk7q1oVaqiVSvuUpW66qqTah2tA6tU3zpQKmid4BYXy4UCBkzy/kGIpgElMRDwfj/vc5+nnHvuuef6Kvnld8YVyeVyOYiIiEhwxPruABEREekHgwAiIiKBYhBAREQkUAwCiIiIBIpBABERkUAxCCAiIhIoBgFEREQCxSCAiIhIoBgEEBERCZShvjtQGswaD9d3F4hKXFrsIn13gajEWZmW7HdXXX5e5JxeorO2SoogggAiIqJiEQkrQS6spyUiIiIlZgKIiIgKiET67kGpYhBARERUgMMBREREJATMBBARERXgcAAREZFAcTiAiIiIhICZACIiogIcDiAiIhIoDgcQERGREDATQEREVIDDAURERALF4QAiIiISAmYCiIiICnA4gIiISKA4HEBERERCwEwAERFRAYENBzATQEREVEAk1t2hocjISLi6usLU1BR+fn6Ii4t7bf2IiAjUrVsXZmZmcHFxwZgxY/D8+XON7skggIiISM+ioqIQEhKCqVOnIiEhAZ6enujYsSPS0tIKrb9p0yZMnDgRU6dOxaVLl7Bq1SpERUXh22+/1ei+DAKIiIgK6CkTsHDhQgwYMADBwcFwd3fHsmXLYG5ujtWrVxda//jx42jRogV69eoFV1dXdOjQAT179nxj9uC/GAQQEREVEIt0dkgkEmRmZqocEolE7Za5ubmIj4+Hv7//y26IxfD390dsbGyh3WzevDni4+OVH/rXr1/H3r170alTJ80eV6PaREREVCzh4eGwsbFROcLDw9XqZWRkQCqVwsHBQaXcwcEBKSkphbbdq1cvTJ8+He+//z6MjIxQs2ZNtGnThsMBREREWtPhcEBoaCiePHmicoSGhuqkm4cPH8asWbPw008/ISEhAdu3b8eePXswY8YMjdrhEkEiIqICOlwiaGJiAhMTkzfWs7e3h4GBAVJTU1XKU1NT4ejoWOg1U6ZMwVdffYX+/fsDABo1aoSsrCwMHDgQkyZNglhcvO/4zAQQERHpkbGxMby9vRETE6Msk8lkiImJQbNmzQq9Jjs7W+2D3sDAAAAgl8uLfW9mAoiIiAroadvgkJAQBAUFwcfHB76+voiIiEBWVhaCg4MBAIGBgXB2dlbOKejatSsWLlyIxo0bw8/PD1evXsWUKVPQtWtXZTBQHAwCiIiICuhpx8CAgACkp6cjLCwMKSkp8PLyQnR0tHKyYHJysso3/8mTJ0MkEmHy5Mm4e/cuKlWqhK5du+L777/X6L4iuSZ5g3LKrPFwfXeBqMSlxS7SdxeISpyVacl+UzdrP0dnbeUcmKCztkoKMwFEREQFBPYWQQYBREREBfgCISIiIhICZgKIiIgKcDiAiIhIoDgcQERERELATAAREVEBDgcQEREJFIcDiIiISAiYCSAiIirA4QAiIiKBElgQIKynJSIiIiVmAoiIiAoIbGIggwAiIqICHA4gIiIiIWAmgIiIqACHA4iIiASKwwFEREQkBMwEEBERFeBwABERkTCJBBYEcDiAiIhIoJgJICIiUhBaJoBBABERUQFhxQAcDiAiIhIqZgKIiIgUOBxAREQkUEILAjgcQEREJFDMBBARESkILRPAIICIiEhBaEEAhwOIiIgEipkAIiKiAsJKBDAIICIiKsDhACIiIhIEZgKIiIgUhJYJYBBARESkILQggMMBREREZUBkZCRcXV1hamoKPz8/xMXFFVm3TZs2EIlEakfnzp01uieDACIiIoXCPli1PTQRFRWFkJAQTJ06FQkJCfD09ETHjh2RlpZWaP3t27fj/v37yuP8+fMwMDDAF198odF9GQQQEREVEOnw0MDChQsxYMAABAcHw93dHcuWLYO5uTlWr15daP2KFSvC0dFReRw4cADm5uYMAoiIiMoCiUSCzMxMlUMikajVy83NRXx8PPz9/ZVlYrEY/v7+iI2NLda9Vq1ahR49esDCwkKjPjIIICIiUtDlcEB4eDhsbGxUjvDwcLV7ZmRkQCqVwsHBQaXcwcEBKSkpb+xzXFwczp8/j/79+2v8vFwdQEREpKDL1QGhoaEICQlRKTMxMdFZ+wVWrVqFRo0awdfXV+NrGQQQERGVABMTk2J96Nvb28PAwACpqakq5ampqXB0dHzttVlZWdiyZQumT5+uVR85HEBERKSgj9UBxsbG8Pb2RkxMjLJMJpMhJiYGzZo1e+21W7duhUQiQZ8+fbR6XmYCiIiICuhpr6CQkBAEBQXBx8cHvr6+iIiIQFZWFoKDgwEAgYGBcHZ2VptTsGrVKnTr1g12dnZa3ZdBABERkZ4FBAQgPT0dYWFhSElJgZeXF6Kjo5WTBZOTkyEWqybvk5KScPToUfz5559a31ckl8vlb9XzcsCs8XB9d4GoxKXFLtJ3F4hKnJVpyY5iO/TfqrO2Un/WbM2+PjATQEREpMB3BxAREZEgMBNARESkILRMAIMAIiIiBaEFARwOICIiEihmAoiIiAoIKxHAIICIiKgAhwOIiIhIEJgJICIiUhBaJoBBABERkQKDACo3XBwrICTIH+3eqwcXxwoQiURIyXiCownXsGjDXzh3+a5K/UmDOmHy4E6vbdPz0xm4fDP1tXVeZW5qjK5tPNDY3QWN61eDV72qsLY0w7XkdDT85LvX9r1DC3e0b1Yfjd2rwcHOCnkvZLh+Jx3RRy5g8cZDyHj0rNBrAz70wfj+HVHTxR5pD55i3a4TCF+5DzKZ+g7Y5qbGSPhtEp5lS9Cs5xzkvZAW+9mo7Nu35w/EHj+KK5eTkJGejsynmTA1NUX16jXQ9gN/BPTsDXNzC43blclk2Lv7d+zdvQuXLych69kzWNvYoEaNmvigfQd8EdCryGsPH4rBrh2/4eL5c3jy5AmsrKzgUq0amjV/HwMGD1Op++LFC6xcFondv+/Ew4cPUK26KwYMGgr/Dh8W2nZS4iUE9v4SH3/yKSaFaffqWKJXMQgop5o2rI7dS4fD2tIMd1Mf4eCJRMikMnjUrYo+Xf0Q8KEP+n67FtsPnla79t+kOzibdKfQdjOf5WjUj1rVKmFteF+N+792Vl80b1wTeXlS/Jt0GyfP3kAFa3M0beiK8f06ou+nzdF1yBKc/U8g81HLhlgb3hcPn2Qh+sgFeNStismDO8HO1gIhc9T3/J42rAtcHCvgg69/YADwDtr26xac/fc0atRwQ7367rC2scHDBw9w9uwZXLxwDr/v/A0rVq1HpcqVi93ms6dPETJqKBLiT8HC0hKeno1haWWF9LRUJCVeQlbWs0KDgLy8XEz5dgIO/hkNE1NTeHh4oaKdHR5kZOD6tavYsnmDWhCw5MeF2PDLGjhXdcH7LVvj1D9xmPjNGMwWieDfvqNKXalUiu+nh8HW1hYjR4/T7g+M3kxYiQAGAeXVkik9YW1php+3HcWYOb/ixQsZgPxU1pQhnRA64CMsmdITe/4+B0nuC5Vr/zh0Ft8v36uTfjzNlmDdzlicSbyNfxPvwMbKDDsWD3njdffSHuObeduwac8/ePgkS1luX8ESG+Z8jdZN62DD3H7w6j5D5Rt+2NDOkOTmoXXgAlxNToO5qTGObRyPAZ+/jzk/RyP1wVNl3Sbu1TCkR2us2HoUJ/69oZPnpbJlzNjxcKleHTY2tirljx8/wrjRI3DmdDx+WDAHs+YsKFZ7crkcY0cPR0L8KXT/PACjx36jkknIy8vFlcuXC7125ndhOPhnNNq0/QCTp86AbYUKynMymQwXzp9Vqf/wwQNEbd4AN7ea+GXTVpiameHmjevo8Xk3rFi6RC0I2LJpPS5eOI/Z836AlbV1sZ6HNCe04QCuDiiHKtpYwKNOVQDAdz/tVgYAQP4vsZnL9iI7JxcVrM1Rr4Zjifblxp0MDP5uI5ZF/Y3Yf68jKye3WNd9NXENlmw6rBIAAEDGo2foN/kXAEDt6pXxnkcN5TkjQwM0rFUFR+Kv4mpyGgAg+3kuNu/9B4aGBvBp6KqsKxaLsGRyT6Q+yETY4t/f8imprGro4akWAACArW0FDBs5GgBwMvZYsdv7fed2xJ+KQ7Pm7+PbKdPUhhKMjIzh3qCh2nVxJ2Ox549dqFmrNmbP+0ElAAAAsViMRh5eKmVXr15GXl4ePuzcFaZmZgAA1xpuaOLjg+vXruLZs5fDYSn372FZ5GK0bNWmyKECIm0wCCiHJLl5xa774HHh4+pl2d20x0h/lP+Nvqrjy1+mtlZmMDQ0wKPMbJX6BYGEpZmJsmxk73ZoXN8FY2ZvxdOs56XQayprDAwMAABGxsbFviZq8wYAwFd9v9boXlGbNwIAevYOhKGRUbGuefL4MQDA2tpGpbwgqMnJfvn3fM6sGRCJgAmTwjTqF2lOJBLp7CgPOBxQDmXl5OJowlW836QWpg7tojYcMHlwJ5ibGSP66AXcSX2sdr1XfRfMGPkxKlhbIPNZDv5NvIM9f5/Ds2xJKT9J4exsLVDByhwAcD89U1me/ugZsnIkqFvDQaV+PcXPd9MfAwCqOVXE5CGdsCvmDP44rJqCJWHIysrCiqWRAIBWrdsV65oHDzJwOSkRBgYG8PBsjDt3buPg/mjcu3cX5ubmaNjIA63btoORkWpQIZVK8c/JWABAE28fZGSk48/ovbh18waMjY1Rt5472vm3V8sqOFVxBgDcvHFdpfzmjeswMjKCbQVbAMCB/ftw5O/DGDf+Wzg6Omn8Z0GaKS8f3rrCIKCcGjp9E3YuHoL+n7+Pj1o2QMLFZEhlcnjWrYoqlW2wcfdJjJmtPlEOALq0boQurRuplD1+mo2xc7dh0+640uj+a40O/ACGhga4n/4EJ/5V/QW553/n8OWHPhjZpx3W7DgO30au+Orj95D6IBNxZ28CABZNCsCLFzKMKWSiIL2bThw/huh9uyGTyfDwwQOcO3sGWVlZaN6iJUaOHlusNq5cTgKQ/0185/ZtiFgwFy9eqGbdnKu6YP4Pi1G7Tl1l2d07t5Gt+NZ+7uy/mDNruvLnAj8unIdZcxagqd97yrK69erBqUoV/LFrO95v2RoNPTyxc/s2XLmchFZt8oONp5mZmD93Fho09MCXPXtr9WdD9DoMAsqpK7fS0CZoAVbNDEL75vXh7PAybX7x2n38feqKWhr8+p10TFn8O/48egHJ9x8CAOq7OWFscHt0bt0Iq2YEQiaVYcu+U6X6LK9q61cXo7/6AAAwceF2tRn9Uxb9jlY+dTBnbHfMGdsdAJCb9wJfT/4FuXkv8OWH3ujYogFGztqC++lPlNeZGBsi74W00GWEVP5dv34Vu3/fqVL2YacuGDNuAiytrIrVRkF6/knmE8yf8z0+aN8RAwYNRRVnZ1y7egUL5obj/LmzGDF0ALZs2wVb2/x/c0+ePFa2MWPaFHh4eWF0yHi41qiBO7dvI3LxDzh25G+MHT0MG7b8hmrVXQHkzy/4ZsJkfDN2JIYP6a9sw75SJYR8MxEAsChiAR4/fozIZasgFr8cvX2ek6OcR0C6JbRMgEgul5eZ34oZGRlYvXo1YmNjkZKSAgBwdHRE8+bN0bdvX1SqVEmrds0aD9dlN8uEZp5u2LygP6RSGUJ/2IHDcZeRm/cCzbxqYs7Y7qhdvTLW7jyOId9tKlZ7C8Z/jqE92yDt4VPU6jhZ6+V0Lb1r48+fR71xn4DCNKhVBQdWjUYFa3P8tPkwxs7dVmg9O1sLBH3SDG4ulZD2MBNb9p7C5ZupsLUyw5kdU3AtOR0ffP0DAODzDk0wZUhn1HF1QG7eC8ScSMSY2Vtx694DrZ6vLEuLXaTvLujdi7w8pKTcx+FDf2H1ymUQiYB5PyxGE++mb7w2et8eTJ6Yv/TOw9MLq3/ZrHI+KysL3bt+iAcPMjB46Aj0HzQUAHD239P4OjB/yWCVKs7YtmsvjF+ZhyCVStHry09x7eoVfNytO8K++16l3WtXryB63x48fvQQ1aq74uNu3WFjY4szCfEY8PVX6Pv1AAwbOQZSqRQrl0Vi29YtePzoESwsLdGpc1eMGvONoAICK9OSncpWY8wenbV144fOOmurpJSZiYH//PMP6tSpg0WLFsHGxgatWrVCq1atYGNjg0WLFqFevXo4derN31AlEgkyMzNVDrns3VofbmNphi0LB6BSBUv0GLsSv0bHI+3hUzx+moN9R87j42GRyMqRoG+35mjlU7tYbc5cthcvXkhRuaIVfBu5luwDFKKOqwP2LBuOCtbmWLcztsgAAAAePM7CwnUHMXzmZkz/aY9yc6PwkE9ha2WGYTPzf3l3adMI6+d8jfSHT9Fj7EqMn78dzbzcsH/lSFiYFX+yGJUfhkZGqOpSDX0C+2JR5HJkZmZiyrcT8Pz5myeHWpibK/+7++cB6uctLPBR564A8lcDFHh1rL/LJ5+qBABA/gTFgvZeva5AzVq1MWzEaEwKm46vgr6GjY0t8vJy8f2Mqajq4qIMNn5cOA8/r1iK1m3aYeGPP6H7519i269bMGXShDc+G1FRysxwwIgRI/DFF19g2bJlaukYuVyOwYMHY8SIEYiNVf9H9Krw8HB8953qN1ADh6YwcvLVeZ/15aOWDVC5ohWuJafjn/O31M7fvPsA/5y7iTa+ddHOrx7+PnXljW0+ysxG+qNncKpkA2cH2xLoddFqVauM6BUj4WBnjQ1/nMSQ6cXLXrzqfe9aCPz4PYSvjEbi9fws0rjgDniWLcHno5fj8dP8TZCkMhkWT+qBgI+aYvX24i8do/KnoYcnarjVxPVrV3Hp4nk0buLz2vrOVV1e+e+qRdTJL89IT1eWVXF2hkgkglwuh7Pzm67LKFbf16xaiRvXr2HpijUwMTFBVlYWtkZtgodXY0yZNhMA0KpNW6Tcv48D+/fh1s0bqO5a4w2tUnEIbTigzGQC/v33X4wZM6bQ/wNEIhHGjBmDM2fOvLGd0NBQPHnyROUwdPAugR7rT1WnigCAzNcsfct8ln+ugo15kXVeJRaLYG1pCgB4mlV6qwRqVquE/StHwqmSDTbtjsPAqRug6QiVsZEhlkzqgcs30zB31Z/Kco86zki6kaIMAADg+Olr+efqOuvmAahMMzPL//v/8OHDN9atVt0VFhb53+ofP3pUaJ2CcrNXsgbm5hbKD+DHj4u4TlFubv7mf483b97A2lUr0PXjT5UTCW9cu4q8vDx4ejZWqevVOP932+WkxDe2S8UjtCWCZSYIcHR0RFxc0TPT4+Li4ODgUOT5AiYmJrC2tlY5RGIDXXZV7+6lPQYA1HV1UH5wv8rQUAyv+vnfam7dLd7Yd5fWjWBhZgKZTIaEi+rZhZJQo6o99q8YiSqVbbFpdxz6h63XOAAAgAn9O6J29coY/v1m5Oa93B1RLgfM/5P2LxgGKDszYaikPH70CFcu5384VldMxnsdQ0NDtG6bPym1sLQ9AJw8kV/eoKGHSnnB7n5FXhd7XHFdo0LPF5DL5Zg1YyosLa0wetz4lycUHyg5OaqrDgp+Li8fOFT2lJkgYNy4cRg4cCBGjRqF33//HSdPnsTJkyfx+++/Y9SoURg8eDDGjx//5oYE4M9jF/EsWwJzM2P8NKWXyvi2kaEB5o39DNWcKiI374Xy3QEujhXQo1NTmBirjwB1beOBn8Lylx9t2XtKZetdAPBpUB1ntk/Gme2TdfYM1avYYf+KkXB2qICNu09qHQDUc3PE2L7+WLMjFscSrqmcO5N4G/XdnNDM001Z9vVnLZTnqHy7fu0q9u35AxKJeubq1s0bmDBuNHJzc9HIwxO1atdRnovavBGffdIJYYWMpX/dfxAMDY2w47dtOPK/Qyrnflm7CmdOx8PAwABf9lB9d0CPXl/B2toGx478jd+2Rqmc279vD6L37gYABPTq89pn2rVjGxJO/YOQbyaq7IToVrMmjI2Nceivg8rVCM9zcrBvzx8AgLr16r+2XSo+kUh3R3lQplYHREVF4YcffkB8fDyk0vzJfAYGBvD29kZISAi+/PJLrdp9F1cH9OjUFCum9YGRkQHSHj5F/IVbePFCiibu1eDsUAFSqQyjZ/+Kn7cdBZCfGj8ZFYqnWc/xb9Id3Et7DDMTI9Rzc0Lt6vkvVzkcl4TPRy9X2/q3YMY/UPifZdSCAXC0z9/L3MrSFPXdnPBckqfykqI1O49j7Y6X35KOb5qAxvVd8FySh98OJBS5dG/tjuM4fuZ6oecAIGb1GNSoao/G3WfiyX9eftS+eX3sXDwEzyUvEHMyEY521mjayBVXk9PgGxCOnOfF33mxPBDa6oBT/8RhcP8gmJmZo269+qjs4IC8vDykptxH4qWLkMlkqOFWE4t/WgFHpyrK65YvXYKVyyLRxKcpVqz6Ra3d3b/vxPSpkyCTyeDeoCGcquQvEbx54zoMDAwwcVIYPv1M/XfRidhjGDtqGCQSCdxq1kINt5q4czsZSYmXAAD9Bw7B4GEji3yeBw8y8EW3LmjYyAOLflqhdj5y0Q9Ys2oF7Ozt4enVBEmXLuLu3Tvo+FFnfD97vjZ/hOVSSa8OqP1NtM7aujKv7G/xXGYmBgJAQEAAAgICkJeXh4yM/Ak09vb2MCrmNpxCsmXvP7hw9R6G92qL95vURFvfuhCJgJSMTGzeE4efNv8Ppy68TOvfSX2M+Wv+hLd7ddSsVgle9VxgbGSAB4+zsOd/5/Br9Cls3Z+g1bdxz3pVUb2KnUqZqYkRfF/Z9//P45dUzldUzFUwNTFC7y5+Rbb996krRQYB/T5rgeaNa6L3N6vUAgAAOHD8Ej4fvQLfDvwQHZrXR/bzPPwafQoTF+545wIAIapZsxaGjhiNMwnxuHnjOpISL+HFizxY29igqe97aPtBe3zcrbvabP036fJxN9Rwq4l1a37GmYR4XE5Kgo2tDfw7fIg+gcFo2Mij0Ovea9YCm37dgTWrViDuRCz+d+gvWFhaoEXLVujZKxDvNW/x2vsumBuOvLw8TJw0tdDzQ0eMhpW1NbZv+xX/O/wX7CraISi4PwYPG6HR8xG9qkxlAkrKu5gJIPovoWUCSJhKOhNQZ7zuMgGX5zITQEREVG4IbZJlmZkYSERERKWLmQAiIiIFgSUCGAQQEREVEIuFFQVwOICIiEigmAkgIiJSENpwADMBREREAsVMABERkQKXCBIREQmUPt8dEBkZCVdXV5iamsLPz++1L9UDgMePH2PYsGFwcnKCiYkJ6tSpg71792p0T2YCiIiI9CwqKgohISFYtmwZ/Pz8EBERgY4dOyIpKQmVK1dWq5+bm4v27dujcuXK2LZtG5ydnXHr1i3Y2tpqdF8GAURERAr6Gg5YuHAhBgwYgODgYADAsmXLsGfPHqxevRoTJ05Uq7969Wo8fPgQx48fV75fx9XVVeP7cjiAiIhIQSQS6eyQSCTIzMxUOQp79XVubi7i4+Ph7++vLBOLxfD390dsbKxafQD4/fff0axZMwwbNgwODg5o2LAhZs2apXwDb3ExCCAiIioB4eHhsLGxUTnCw8PV6mVkZEAqlcLBwUGl3MHBASkpKYW2ff36dWzbtg1SqRR79+7FlClTsGDBAsycOVOjPnI4gIiISEGXowGhoaEICQlRKTMxMdFJ2zKZDJUrV8aKFStgYGAAb29v3L17F/PmzcPUqYW/jrowDAKIiIgUdDknwMTEpFgf+vb29jAwMEBqaqpKeWpqKhwdHQu9xsnJCUZGRjAwMFCW1a9fHykpKcjNzYWxsXGx+sjhACIiIj0yNjaGt7c3YmJilGUymQwxMTFo1qxZode0aNECV69ehUwmU5ZdvnwZTk5OxQ4AAAYBRERESvraJyAkJAQrV67EunXrcOnSJQwZMgRZWVnK1QKBgYEIDQ1V1h8yZAgePnyIUaNG4fLly9izZw9mzZqFYcOGaXRfDgcQEREp6GuJYEBAANLT0xEWFoaUlBR4eXkhOjpaOVkwOTkZYvHL7+0uLi7Yv38/xowZAw8PDzg7O2PUqFGYMGGCRvcVyeVyuU6fpAwyazxc310gKnFpsYv03QWiEmdlWrIJbO8Zh3TWVvyUtjprq6QwE0BERKQgsFcHMAggIiIqwBcIERERkSAwE0BERKQgsEQAgwAiIqICHA4gIiIiQWAmgIiISEFgiQAGAURERAU4HEBERESCwEwAERGRgsASAQwCiIiICnA4gIiIiASBmQAiIiIFgSUCGAQQEREV4HAAERERCQIzAURERApCywQwCCAiIlIQWAzA4QAiIiKhYiaAiIhIgcMBREREAiWwGIDDAURERELFTAAREZEChwOIiIgESmAxAIcDiIiIhIqZACIiIgWxwFIBDAKIiIgUBBYDcDiAiIhIqJgJICIiUuDqACIiIoESCysG4HAAERGRUDETQEREpMDhACIiIoESWAzA4QAiIiKhKnYmYPr06Ro3LhKJMGXKFI2vIyIi0gcRhJUKKHYQMG3aNLWygrETuVyuVi6XyxkEEBFRuaLP1QGRkZGYN28eUlJS4OnpicWLF8PX17fQumvXrkVwcLBKmYmJCZ4/f67RPYs9HCCTyVSO27dvo1GjRujZsyfi4uLw5MkTPHnyBCdPnkSPHj3g6emJ27dva9QZIiIiIYqKikJISAimTp2KhIQEeHp6omPHjkhLSyvyGmtra9y/f1953Lp1S+P7aj0nYNiwYahduzY2bNgAHx8fWFlZwcrKCk2bNsXGjRtRs2ZNDBs2TNvmiYiISp1IJNLZoYmFCxdiwIABCA4Ohru7O5YtWwZzc3OsXr36tX11dHRUHg4ODho/r9ZBwF9//YV27doVef6DDz5ATEyMts0TERGVOpFId0dx5ebmIj4+Hv7+/soysVgMf39/xMbGFnnds2fPUL16dbi4uOCTTz7BhQsXNH5erYMAU1PT13bu+PHjMDU11bZ5IiKick0ikSAzM1PlkEgkavUyMjIglUrVvsk7ODggJSWl0Lbr1q2L1atXY9euXdiwYQNkMhmaN2+OO3fuaNRHrYOA3r17Y+PGjRg5ciSuXLminCtw5coVjBgxAps2bULv3r21bZ6IiKjUiUUinR3h4eGwsbFROcLDw3XSz2bNmiEwMBBeXl5o3bo1tm/fjkqVKmH58uUataP1ZkFz5sxBRkYGlixZgsjISIjF+fGETCaDXC5Hz549MWfOHG2bJyIiKnW63CwoNDQUISEhKmUmJiZq9ezt7WFgYIDU1FSV8tTUVDg6OhbrXkZGRmjcuDGuXr2qUR+1DgKMjY2xfv16fPPNN9izZw+Sk5MBANWrV8dHH30ET09PbZsmIiIq90xMTAr90P8vY2NjeHt7IyYmBt26dQOQ/4U6JiYGw4cPL9a9pFIpzp07h06dOmnUx7feNtjDwwMeHh5v2wwREZHe6evdASEhIQgKCoKPjw98fX0RERGBrKws5V4AgYGBcHZ2Vg4nTJ8+He+99x5q1aqFx48fY968ebh16xb69++v0X3fOgg4ceIEDh06hLS0NAwdOhS1a9dGdnY2EhMTUadOHVhaWr7tLYiIiEqFvt4dEBAQgPT0dISFhSElJQVeXl6Ijo5WThZMTk5WDrsDwKNHjzBgwACkpKSgQoUK8Pb2xvHjx+Hu7q7RfUXy/273V0y5ubno0aMHdu3apdwd8MCBA2jXrh2eP3+OqlWrYsyYMZg0aZI2zeuUWePipVOIyrO02EX67gJRibMyLdlX3nyxNkFnbW3t20RnbZUUrf80p0yZgt27d2Pp0qVISkpS2TrY1NQUX3zxBXbt2qWTThIREZUGXa4OKA+0DgI2b96MIUOGYODAgahYsaLa+fr16+P69etv1TkiIqLSJNLhUR5oHQSkpaWhUaNGRZ43MDBAdna2ts0TERFRCdN6YqCLiwsSExOLPH/s2DHUqlVL2+aJiIhKnb5WB+iL1pmAXr16Yfny5SpbBxf84a1cuRK//vorAgMD376HREREpUQs0t1RHmidCZg0aRJOnDiBVq1aoX79+hCJRBgzZgwePnyIO3fuoFOnThgzZowu+0pEREQ6pHUmwNjYGNHR0VizZg3c3NxQr149SCQSeHh4YO3atfjjjz9gYGCgy74SERGVKH29Slhf3mqzIJFIhD59+qBPnz666g8REZHelJPPbp3ROhMwfvx4nD59Wpd9ISIiolKkdRCwePFi+Pj4oHbt2pgyZQrOnTuny34RERGVOqENB7zVPgFr1qxBnTp1MHfuXHh5eaFBgwaYMWMGkpKSdNlHIiKiUiG01QFaBwFWVlYIDAzEnj17kJqaihUrVqBq1aqYMWMG3N3d4eXlhdmzZ+uyr0RERKRDOnkTg62tLfr164f9+/fj/v37WLBgAW7cuFEmXh5ERERUXEIbDnjrVwkXyMvLw759+xAVFYU//vgDz549g4uLi66aJyIiKnHl46Nbd94qCHjx4gX+/PNPREVFYdeuXcjMzISTkxOCg4MREBCA5s2b66qfREREpGNaBwH9+vXDzp078ejRI9jb26Nnz57o0aMHWrVqVW7SIERERK8qL68A1hWtg4CdO3fi008/RUBAANq1a8fdAYmIqNwTWAygXRAgkUiwfPly1KlTBx4eHrruExEREZUCrVYHGBsbo3fv3jh+/Liu+0NERKQ3XB1QDCKRCLVr10ZGRoau+0NERKQ35eSzW2e03ifg22+/xZIlS7g7IBERUTml9cTAEydOwM7ODg0bNkSbNm3g6uoKMzMzlToikQg//vjjW3eSiIioNAhtdYBILpfLtblQLH5zEkEkEkEqlWrTvE6ZNR6u7y4Qlbi02EX67gJRibMy1clGt0Uauv2iztr6qbu7ztoqKVpnAmQymS77QURERKVMZ9sGExERlXflZVa/rrx1EHDixAkcOnQIaWlpGDp0KGrXro3s7GwkJiaiTp06sLS01EU/38rlmAX67gJRiTMyLNk0KZEQCO1fkdbPm5ubi+7du6NFixaYNGkSFi1ahNu3b+c3KhajQ4cOnBRIRERUhmkdBEyZMgW7d+/G0qVLkZSUhFfnF5qamuKLL77Arl27dNJJIiKi0iC0zYK0DgI2b96MIUOGYODAgahYsaLa+fr16+P69etv1TkiIqLSJBbp7igPtA4C0tLS0KhRoyLPGxgYIDs7W9vmiYiIqIRpPTHQxcUFiYmJRZ4/duwYatWqpW3zREREpa68fIPXFa0zAb169cLy5csRGxurLCsYA1m5ciV+/fVXBAYGvn0PiYiISonQ5gRonQmYNGkSTpw4gVatWqF+/foQiUQYM2YMHj58iDt37qBTp04YM2aMLvtKREREOqR1JsDY2BjR0dFYs2YN3NzcUK9ePUgkEnh4eGDt2rX4448/YGBgoMu+EhERlSihTQx8q82CRCIR+vTpgz59+uiqP0RERHpTTrL4OqPTzZHkcjn++usv7Nu3D0+fPtVl00RERO+0yMhIuLq6wtTUFH5+foiLiyvWdVu2bIFIJEK3bt00vqfWQcCkSZPQtm1b5c9yuRwdOnRA+/bt0blzZzRq1AjXrl3TtnkiIqJSJxaJdHZoIioqCiEhIZg6dSoSEhLg6emJjh07Ii0t7bXX3bx5E+PGjUPLli21e16trgLw22+/wdfXV/nztm3bEBMTg5kzZ2L37t2QSqWYNm2ats0TERGVOrEOD00sXLgQAwYMQHBwMNzd3bFs2TKYm5tj9erVRV4jlUrRu3dvfPfdd3Bzc9Pwjvm0DgLu3r2rsg/A9u3b4e7ujtDQUHTq1AlDhgzB4cOHtW2eiIioXJNIJMjMzFQ5JBKJWr3c3FzEx8fD399fWSYWi+Hv76+yDP+/pk+fjsqVK6Nfv35a91HrIMDQ0FD5MHK5HDExMfjwww+V5x0cHJCRkaF1x4iIiEqbSKS7Izw8HDY2NipHeHi42j0zMjIglUrh4OCgUu7g4ICUlJRC+3n06FGsWrUKK1eufKvn1Xp1QMOGDbFhwwb07t0bO3bswIMHD9C5c2fl+Vu3bsHe3v6tOkdERFSaNB3Lf53Q0FCEhISolJmYmLx1u0+fPsVXX32FlStXvvXnrNZBQFhYGLp27arsQIsWLVQmCu7ZswdNmzZ9q84RERGVVyYmJsX60Le3t4eBgQFSU1NVylNTU+Ho6KhW/9q1a7h58ya6du2qLJPJZADys/RJSUmoWbNmsfqodRDQvn17JCQk4MCBA7C1tUVAQIDy3KNHj9CqVSt88skn2jZPRERU6vSxT4CxsTG8vb0RExOjXOYnk8kQExOD4cOHq9WvV68ezp07p1I2efJkPH36FD/++CNcXFyKfe+32izI3d0d7u7uauUVKlTADz/88DZNExERlTp97fQXEhKCoKAg+Pj4wNfXFxEREcjKykJwcDAAIDAwEM7OzggPD4epqSkaNmyocr2trS0AqJW/yVsFAQBw/vx57N27Fzdv3gQAuLq64qOPPnrta4aJiIjopYCAAKSnpyMsLAwpKSnw8vJCdHS0crJgcnIyxGKd7u8HABDJ5XK5NhdKJBIMGjQI69evh1wuV3ZOJpNBJBKhd+/e+Pnnn2FsbKzTDmvj9kP1JRlE75pK1m8/4YiorDN966+urzf9wFWdtRXWvtabK+mZ1mHFhAkT8Msvv2DIkCG4dOkSnj9/DolEgkuXLmHw4MHYsGEDxo8fr8u+EhERlShdLhEsD7TOBNjb26Nz585Yt25doee/+uor7Nu3r0zsFcBMAAkBMwEkBCWdCZhxUHeZgCn+73AmIC8vD++9916R55s3b44XL15o2zwREVGpE9qrhLUOAjp27Ij9+/cXeT46OhodOnTQtnkiIqJSJ9Lh/8qDYidWHj58qPLzjBkz8OWXX6J79+4YNmyY8j0CV65cQWRkJG7duoWoqCjd9paIiIh0pthBgL29PUT/mekgl8tx7tw57Nq1S60cABo0aMAhASIiKjfKSxpfV4odBISFhakFAURERO8SBgFFmDZtWqHlWVlZyMzMhJWVFSwtLXXVLyIiIiphWk0MvHnzJoYOHYrq1avD2toaVatWhY2NDapVq4Zhw4Ypdw8kIiIqT0Qikc6O8kDjfQJ27dqFr776Cs+ePYOrqys8PDxgZWWFp0+f4uzZs7h58yYsLCywYcOGMvMCIe4TQELAfQJICEp6n4AF/7uus7bGtnbTWVslRaM/zosXLyIgIABubm5Yvnw5WrZsqVbnyJEjGDx4MHr06IH4+PhCXzBERERE+qfRcMCsWbNgb2+Po0ePFhoAAEDLli1x5MgR2NnZITw8XCedJCIiKg1C2zZYoyDg0KFD6NevHypWrPjaehUrVsTXX3+Nv/766606R0REVJrEIpHOjvJAoyDgwYMHcHV1LVbdGjVq4MGDB9r0iYiIiEqBRnMC7O3tcePGjWLVvXHjBuzt7bXqFBERkT4IbZ8AjTIBbdq0wapVq9S2EP6vhw8fYtWqVWjTps3b9I2IiKhUcU7Aa3z77bd48OABWrVqhePHjxda5/jx42jdujUePHiA0NBQnXSSiIiIdE+j4QB3d3ds2rQJgYGBaNmyJVxdXeHp6amyT8CNGzdgamqKDRs2oEGDBiXVbyIiIp0Tl5O3/+mKxpsFAcD169cxd+5c7N69G/fu3VOWOzk5oUuXLvjmm2+UbxUsC7hZEAkBNwsiISjpzYJ+On5TZ20Nbe6qs7ZKilZ/nG5ubli2bBkAIDMzE0+fPoWVlRWsra112jkiIiIqOW8dU1lbW/PDn4iI3glCWx1QwokVIiKi8qO8bPKjK1q9RZCIiIjKP2YCiIiIFASWCGAQQEREVIDDAURERCQIzAQQEREpCCwRwCCAiIiogNDS40J7XiIiIlJgJoCIiEhBJLDxAAYBRERECsIKATgcQEREJFjMBBARESkIbZ8ABgFEREQKwgoBOBxAREQkWAwCiIiIFEQi3R2aioyMhKurK0xNTeHn54e4uLgi627fvh0+Pj6wtbWFhYUFvLy8sH79eo3vySCAiIhIQSQS6ezQRFRUFEJCQjB16lQkJCTA09MTHTt2RFpaWqH1K1asiEmTJiE2NhZnz55FcHAwgoODsX//fs2eVy6XyzW6ohy6/VCi7y4QlbhK1ib67gJRiTMt4Zlsm0/f1VlbPRs7F7uun58fmjZtiiVLlgAAZDIZXFxcMGLECEycOLFYbTRp0gSdO3fGjBkzin1fZgKIiIgUxDo8JBIJMjMzVQ6JRP1LaW5uLuLj4+Hv7/+yH2Ix/P39ERsb+8Y+y+VyxMTEICkpCa1atdL4eYmIiAi6HQ4IDw+HjY2NyhEeHq52z4yMDEilUjg4OKiUOzg4ICUlpci+PnnyBJaWljA2Nkbnzp2xePFitG/fXqPn5RJBIiKiEhAaGoqQkBCVMhMT3Q3bWVlZ4cyZM3j27BliYmIQEhICNzc3tGnTpthtMAggIiJS0OU+ASYmJsX60Le3t4eBgQFSU1NVylNTU+Ho6FjkdWKxGLVq1QIAeHl54dKlSwgPD9coCOBwABERkYI+VgcYGxvD29sbMTExyjKZTIaYmBg0a9as2O3IZLJC5xy8DjMBREREehYSEoKgoCD4+PjA19cXERERyMrKQnBwMAAgMDAQzs7OyjkF4eHh8PHxQc2aNSGRSLB3716sX78eS5cu1ei+DAKIiIgU9JUeDwgIQHp6OsLCwpCSkgIvLy9ER0crJwsmJydDLH7Zu6ysLAwdOhR37tyBmZkZ6tWrhw0bNiAgIECj+3KfAKJ3BPcJICEo6X0Cdpwteja+pj71KHo8v6zgnAAiIiKB4nAAERGRgtDeIsgggIiISEGbF/+UZxwOICIiEihmAoiIiBTEAhsQYBBARESkwOEAIiIiEgRmAoiIiBREHA4gIiISJg4HEBERkSAwE0BERKTA1QFEREQCxeEAIiIiEgRmAoiIiBSElglgEEBERKQgtCWCHA4gIiISKGYCiIiIFMTCSgQwCCAiIirA4QAiIiISBGYCiIiIFLg6gMqF27du4FRcLK4kXsTlxItIvnUDMqkUfQcOR5/gga+9Nj7uBH7b8gsSL57H85wcODg6oWVbf/QM7A8zc3ON+pGTk43jfx/ClaRLuJx4EVeTLiE7OwtVnF3wy7Y9RV43d8Zk/Ln39ze27+XdFPOXrFIpi9m/B5vW/Yx7d5JhW9EOH3bphj7Bg2BgYFBo//r36g4zczMsXfsrjIyMNHo+Krtu3riO48eP4dKFC7h48QJuXL8GqVSKYSNGYeDgoVq3eyL2ONavW4Pz584iJycHTlWqwL99R/TrPxDmFhZq9f+JO4n+wYGvbXNS2DR8GdBTrXzv7j+wcsUy3E6+BTs7e3Tr/hkGDh5a6N/l7OxsfPZJF5iZmyNq63YYGRtr/YxUNKENBzAIKKf+2P4rtv+6UePrtm1ej2WL5kEkEqGRZxPYVrTD+X8TsGndzzhy6CAilq+DjW2FYrd393YywqeFatyPhp6NX3v+rz/34sWLF/Bq4qtSfuLo/xA+LRRWVtbwa94KV68kYv2qZch88gQjxqr3Y83yJUhLvY+IZesYALxjft2yGRs3/KLTNtevW4v5c8MhEonQxNsHdnZ2SIiPx88rluHggf1Yu34TKlSoWOi1dnb2aPF+y0LPubrWUCv73+FDCJ0wDtbWNmjZug2SEi9h2U9L8PjxY4ROmqJWP3JRBO7fv4e16zcxACCdYRBQTrnWrIUvegWhVp16qF3XHZvWrcTB6N2vveZK0iUsXzwfYgMDzJy3CL7N8n9hPX+egynfjMTpUycRMXcGps5aWOx+mJtboGPnbqhdtz5q1amHZ8+eYvK44W+8rtPHn6HTx58Vei7xwjn8ufd3iMVidOj8icq5NSsiYWRkhMU/b0DVaq7IycnGsOCe+GPHr+jddwAq2tkr6yZduoCd2zaj66dfooGHV7GficqHWrXrICj4a9Sr54767u74eeVy7P59l9btXbp0EQvmzYaBgQEWRS7F+y1bAwBycnIwavgQnDwRi5nfTcOCiEWFXl/DzQ0zZs0u9v0iF/8IIyMjrN8cBVfXGsjOzkavgM+wNWozBgwcDPtKlZR1L5w/h82bNuDLgJ7watxE62ekNxPa6gBODCynOn38GQaNGIsPOnZGNdcaEIvf/H/l5l9WQS6Xo2PnT5QBAACYmpph3LffQSwW48ihg0i+eaPY/ahS1QXfTJ6Obl/0REPPxjA1M9PqeV61748dAABv32ao7OCoLM/Ly8ON61fg0dgHVau5AgDMzMzxwYedIZNKkXjxnLKuVCrFD7O/Q8WKdug3dNRb94nKnu6ff4GQcRPQqUtX1HCrCbHo7X6drV65HHK5HJ90664MAADAzMwM02Z8D7FYjIMH9uPG9Wtv23Xk5ebi6pXL8G7qq8wSmJubo3OXjyGVSnH+3FllXalUiunTwmBnb4+RY8a+9b3p9UQ6/F95wCBAIPLy8hB3/G8AwAcdOqmdd3Cqovy2fPR/MaXZNRWS589x6GA0AOCjrp+qnHv2NBMyqRRW1jYq5dbWtgCAnOxsZdlvW9bj6uVEjBj7LSwsLEu201Tu5eXm4u+//wcA+KhzF7XzVao4K7+B/xVz8K3vl/n0KaRSKWxsVP8u29jaAsgf/y+w4Ze1SLx0EaGTwmBpyb/LpFscDhCIO8k38fz5cwBAnXoNCq1Tp14DnDuTgKuXE0uzayr+PnQA2VnPYGNbAc1atlU5V6GiHUxNTZF887pKefKt/J/tKzkAAFLu38UvP/+E91t/gBat25VOx6lcu3nrJp7n5AAAGjRsWGgd9wYNkRB/ComXLhZ6/sGDDCz7aQnS0tJgYmKMGjXc0LJVGzhVqaJW187ODqZmZrhxTTWrUJBlqOyQ/3f57t07+ClyMdr5t0e7D/y1fj4qPq4OoHdSyr27AABLK6tCZzgDQKXK+an3lPt3S61f/xW9eycAwP/DLoVO5GvWsg0OHYjGts2/4KOPu+PS+bPYv3sXbCtURP2GHgCAH+fOhNjAEMMLmShIVJi7d+4AAKysrYvMHDk6OqnU/a8b169jaeRilTJDw+/Ro1cfjBn7DQwNVX/dtmnTDtH79uCXtWvQ/fMvcPbfM9i1Yzsq2tnBw9MLAPD99GkwNDAodKIglQyBxQAMAoQiOzsLQP74f1HMzPPPZWc9K5U+/de9u3dw9vQpAOpDAQX6DRmFM/H/YNmi+Vi2aD4AwNDQEBOnzYKxsTH++nMv/jlxDCO/mQT7SpWV1+VKJDAwNCx06RVRdlb+vw+z18xpMVcsn332n38fllZW6PNVENr5t0f16q6wsLTEndvJ2LljO7Zs2ogNv6xFTnY2wr6boXLdyDEh+CfuJBbMm40F8/InFBoaGmHW7HkwNjbGvj27cezoEUyaMhWVKzsor5NIJDDk32XSkXIVBNy+fRtTp07F6tWri6wjkUggkUj+UwaYmJiUdPfoLUXv3gG5XI567g3h6lar0DqOTs74eeN2RO/egbt3bqNCBTvl5MinmZn4KWIuGng0RtdPvwQAHDoYjXUrI3En+RYMDQ3RxLcZRowNhVOVqqX5aPQOq1/fHfXru6uU1a5TF99MCEXjJt4YO3oEftv2K77s0Qv16tdX1nF2rorfdv2Bndt/w+3kZFS0s0NnxSTHzCdPMHfOLHg1boIvFPsLRO/bi5+W/IhbN2/C0NAIzZo3x8RJU1C1qkupPu+7Tiyw8YByFQQ8fPgQ69ate20QEB4eju+++06lbPT4SQiZIOx0mrl5/hDA8+c5RdbJyc4/Z66HiXQymQwHFJsHfdi1+2vr2thWQECfr9XKly9egKxnTxEyMQwikQjH/j6E76eMR0PPxug3ZBQeZmRg9fLFGDesP37euF3jjZHo3VUwRJaTU/S/j4LJepYa/Pvwb98BdevVR1LiJfzv8F8qQQAAVKhQEcH9Bqhdt2D+HDzNzETYtBkQiUQ49NdBTBg3Bo2beGPk6LHISE/HkkU/YEBwEH7b+UeRQ3ykOWGFAGUsCPj999fvIHf9+vXXngeA0NBQhISEqJSlZb1Vt94JDk75k5OePX2K7KysQn9ppKelAAAcHdUnMpW0UyePIz0tFaampmjb/kONr/834RT279mJPsEDUb1GTQDAlvWrYGpmhhlzF8PK2hoAIBaL8eO8mYj5cy+6dPtcp89A5ZezszMA4GlmJrKynhU6LyAl5T4AoIqibnG5udVEUuIlpKamFqv+qX/isGvHdgwcPBQ1a+VnxFb/vBJmZuZYtGQprBUrCsQGYnw/fRr27tmNz78M0KhPRAXKVBDQrVs3iEQiyOXyIuuI3pCqMTExUUv9P3khKaK2cLhUrwFTU1M8f/4clxMvwMvbV63O5cQLAIBadeurnStpBRMCW7XroPGSvtzcXETMmQ6Xaq7oGfTyW9W1y0lwrVlLGQAAL3cqvHZFfysgqOxxda0BUzMzPM/JwYXz5+Hr955anYsXzgMA6rsXvrqmKI8fPwYAWBTj23pubi5mfBcG1xo10H/gYGV5UuIl1KpVWxkAAEDjJt7Kc6RDAksFlKl9ApycnLB9+3bIZLJCj4SEBH13sdwyMjKCb/NWAICYP/eqnU+9fw8Xzv0LAHi/9Qel2rcnTx4j9sghAEVPCHydTWtX4s7tWxg9YQqMX9lOVSQSKZd9FSgYDikvG3lQ6TAyNkarVvkbBO3bo77z5r17d/HvmdMAoNFSvdTUVJxOyJ/s2rBRozfWX7l8KW7dvIkpU6er/V3+71BFwc9v+mJEmuFmQXrk7e2N+Pj4Is+/KUtAr9cz8GuIRCLs37MLcbFHleXPn+dg/qypkEmlaNnWH9X+s8954oVzCA74GMEBH5dIv2KidyMvLw9Vq1VHIy9vja69deMaojasxkcfd4dHYx+Vc7Xq1kfyzes4/+9pZdmenb8BAGrrIdtB+rd54wZ80uVDTAodr3bu6/4DIRKJsGvndhw78reyPCcnB9OmTIJUKoV/+46o4VZT5bqN69fh0aOHau1dTkrEyGGD8fz5c7i4VEPbdq8PHq5dvYo1q1ai+2dfwNunqcq5evXdcf36NZxOePn78betUfnn3FUnJRJpQiQvQ5+qR44cQVZWFj78sPAx4aysLJw6dQqtW7cu9HxRbj9894YDriRdxI/zvlf+fP/uHTx5/AiVKjvA7pWlcd/NjoCd/cs9yF99gZBHYx/YVqiI8/8m4EFGOlyquRb6AqEzCf9g3LB+AICDsWfxX1MnjMaDB+kA8pdaJd+8DiNjY9SsXVdZp9PH3Yt8V8CgwC9w7UoS+g8djR5fqU/4K4pcLsfowX1x/+5trN68E5ZW1irn42KPYtLYYTA2NkET3/fw6EEGEi+eh3PVali+futrl0uWR5WshbUC5tLFC/h+xstJwHduJ+PRo0dwcHRUWVL3w6IlqKT4N7E0cjGW/bQEPk19sWrterU2X32BkLdPU1S0s8Pp+FNIT0+Ha40ahb5A6P33fJCdnY269erB2bkqRGIx7txORuKlS5DJZHByqoKflv8Mt5o11e5XQC6Xo+9XvXDn9m3s+GMvrK1V/y4fO/I3hg0ZCBMTE7zXrDkyMjJw/txZVKtWHb9u3/XapY3vGtMSHsSOu/5EZ235utm8uZKelak5AS1bFv4GrgIWFhYaBwDvqqysLCReOKdWnp6WivS0lxOQ8vJyVc5/3vMr1KhZG9s2r8t/lfDzHFR2cELPwH7oGdhfq1nGVy8nIjXlnkpZXm6uSv+avtei0GsvJ17EtStJEBsYoP1HXTW6756d23Dh7GlMmTlfLQAAAN9m72PG3EVYv3o5Tp04BhPFpMNBI8a9cwGAED179gznzv6rVp6akoLUlBTlz7m5uWp1ivJVUF/UrlMHv6xdjfPnziEnJxuOTlXQb0B39BswsND5Kv0HDsaZ0wm4dvUqTsQeR05ODiwsLOHp1Rht232Az78MeOM8l21bo3DmdALmLYxQCwAAoEXLVvhxyVIsXxqJY0ePwNTMDB9+1Bljx08QVABQGvSZxI+MjMS8efOQkpICT09PLF68GL6+6vO3AGDlypX45ZdfcP58/lwVb29vzJo1q8j6RSlTmYCS8i5mAoj+S2iZABKmks4E/KPDTEBTDTIBUVFRCAwMxLJly+Dn54eIiAhs3boVSUlJqFy5slr93r17o0WLFmjevDlMTU0xZ84c7NixAxcuXFCudikOBgFE7wgGASQEJR4E3NBhEFCj+EGAn58fmjZtiiVLlgDI3zvFxcUFI0aMwMSJE994vVQqRYUKFbBkyRIEBgYW+75lajiAiIhIn3Q5q7+wHWwLW8aem5uL+Ph4hIa+fN+JWCyGv78/YmNji3Wv7Oxs5OXloWLFim+u/IoytTqAiIjoXREeHg4bGxuVIzw8XK1eRkYGpFIpHBwcVModHByQ8sr8lteZMGECqlSpAn9/zd42yUwAERGRgi63XShsB9uSeI/N7NmzsWXLFhw+fBimpqYaXcsggIiIqAQUlvovjL29PQwMDNS2lk5NTYWjo+Nrr50/fz5mz56NgwcPwsPDQ+M+cjiAiIhIQaTDo7iMjY3h7e2NmJgYZZlMJkNMTAyaNWtW5HVz587FjBkzEB0dDR8fnyLrvQ4zAURERAX0tFFASEgIgoKC4OPjA19fX0RERCArKwvBwcEAgMDAQDg7OyvnFMyZMwdhYWHYtGkTXF1dlXMHLC0tYWlZ/PevMAggIiLSs4CAAKSnpyMsLAwpKSnw8vJCdHS0crJgcnIyxOKXyfulS5ciNzcXn3+u+jbUqVOnYtq0acW+L/cJIHpHcJ8AEoKS3ifg9K2nOmurcXUrnbVVUpgJICIiUhDaSxk5MZCIiEigmAkgIiJSEFgigEEAERGRksCiAA4HEBERCRQzAURERAq6fIFQecAggIiISIGrA4iIiEgQmAkgIiJSEFgigEEAERGRksCiAA4HEBERCRQzAURERApcHUBERCRQXB1AREREgsBMABERkYLAEgEMAoiIiJQEFgVwOICIiEigmAkgIiJS4OoAIiIigeLqACIiIhIEZgKIiIgUBJYIYBBARESkJLAogMMBREREAsVMABERkQJXBxAREQkUVwcQERGRIDATQEREpCCwRACDACIiIiWBRQEcDiAiIhIoZgKIiIgUuDqAiIhIoLg6gIiIiASBmQAiIiIFgSUCGAQQEREpCSwK4HAAERGRQDEIICIiUhDp8H+aioyMhKurK0xNTeHn54e4uLgi6164cAGfffYZXF1dIRKJEBERodXzMgggIiJSEIl0d2giKioKISEhmDp1KhISEuDp6YmOHTsiLS2t0PrZ2dlwc3PD7Nmz4ejoqP3zyuVyudZXlxO3H0r03QWiElfJ2kTfXSAqcaYlPJMtWYefF9UqFv/fpJ+fH5o2bYolS5YAAGQyGVxcXDBixAhMnDjxtde6urpi9OjRGD16tMZ9ZCaAiIhIQaTDQyKRIDMzU+WQSNSDjNzcXMTHx8Pf319ZJhaL4e/vj9jY2BJ7VoBBABERkZIuhwPCw8NhY2OjcoSHh6vdMyMjA1KpFA4ODirlDg4OSElJKdHn5RJBIiKiEhAaGoqQkBCVMhOTsjVsxyCAiIhISXcbBZiYGBfrQ9/e3h4GBgZITU1VKU9NTX2rSX/FweEAIiIiBX2sDjA2Noa3tzdiYmKUZTKZDDExMWjWrFkJPOVLzAQQERHpWUhICIKCguDj4wNfX19EREQgKysLwcHBAIDAwEA4Ozsr5xTk5ubi4sWLyv++e/cuzpw5A0tLS9SqVavY92UQQEREpKCvXYMDAgKQnp6OsLAwpKSkwMvLC9HR0crJgsnJyRCLXybv7927h8aNGyt/nj9/PubPn4/WrVvj8OHDxb4v9wkgekdwnwASgpLeJ+D+k1ydteVkY6yztkoK5wQQEREJFIcDiIiIFLTZ8788YxBARERUQFgxAIcDiIiIhIqZACIiIgWBJQIYBBARERXQ9BXA5R2HA4iIiASKmQAiIiIFrg4gIiISKmHFABwOICIiEipmAoiIiBQElghgEEBERFSAqwOIiIhIEJgJICIiUuDqACIiIoHicAAREREJAoMAIiIigeJwABERkQKHA4iIiEgQmAkgIiJS4OoAIiIigeJwABEREQkCMwFEREQKAksEMAggIiJSElgUwOEAIiIigWImgIiISIGrA4iIiASKqwOIiIhIEJgJICIiUhBYIoBBABERkZLAogAOBxAREQkUMwFEREQKXB1AREQkUFwdQERERIIgksvlcn13gt4tEokE4eHhCA0NhYmJib67Q1Qi+Pec3gUMAkjnMjMzYWNjgydPnsDa2lrf3SEqEfx7Tu8CDgcQEREJFIMAIiIigWIQQEREJFAMAkjnTExMMHXqVE6Wonca/57Tu4ATA4mIiASKmQAiIiKBYhBAREQkUAwCiIiIBIpBABERkUAxCCCdi4yMhKurK0xNTeHn54e4uDh9d4lIZ/7++2907doVVapUgUgkws6dO/XdJSKtMQggnYqKikJISAimTp2KhIQEeHp6omPHjkhLS9N314h0IisrC56enoiMjNR3V4jeGpcIkk75+fmhadOmWLJkCQBAJpPBxcUFI0aMwMSJE/XcOyLdEolE2LFjB7p166bvrhBphZkA0pnc3FzEx8fD399fWSYWi+Hv74/Y2Fg99oyIiArDIIB0JiMjA1KpFA4ODirlDg4OSElJ0VOviIioKAwCiIiIBIpBAOmMvb09DAwMkJqaqlKempoKR0dHPfWKiIiKwiCAdMbY2Bje3t6IiYlRlslkMsTExKBZs2Z67BkRERXGUN8doHdLSEgIgoKC4OPjA19fX0RERCArKwvBwcH67hqRTjx79gxXr15V/nzjxg2cOXMGFStWRLVq1fTYMyLNcYkg6dySJUswb948pKSkwMvLC4sWLYKfn5++u0WkE4cPH0bbtm3VyoOCgrB27drS7xDRW2AQQEREJFCcE0BERCRQDAKIiIgEikEAERGRQDEIICIiEigGAURERALFIICIiEigGAQQEREJFIMAonLI1dUVffv2Vf58+PBhiEQiHD58WG99+q//9pGIyh4GAURaWLt2LUQikfIwNTVFnTp1MHz4cLUXKJVle/fuxbRp0/TdDSLSE747gOgtTJ8+HTVq1MDz589x9OhRLF26FHv37sX58+dhbm5eav1o1aoVcnJyYGxsrNF1e/fuRWRkJAMBIoFiEED0Fj766CP4+PgAAPr37w87OzssXLgQu3btQs+ePdXqZ2VlwcLCQuf9EIvFMDU11Xm7RPRu43AAkQ61a9cOQP6b5fr27QtLS0tcu3YNnTp1gpWVFXr37g0g/xXLERERaNCgAUxNTeHg4IBBgwbh0aNHKu3J5XLMnDkTVatWhbm5Odq2bYsLFy6o3beoOQEnT55Ep06dUKFCBVhYWMDDwwM//vgjAKBv376IjIwEAJWhjQK67iMRlT3MBBDp0LVr1wAAdnZ2AIAXL16gY8eOeP/99zF//nzlEMGgQYOwdu1aBAcHY+TIkbhx4waWLFmC06dP49ixYzAyMgIAhIWFYebMmejUqRM6deqEhIQEdOjQAbm5uW/sy4EDB9ClSxc4OTlh1KhRcHR0xKVLl7B7926MGjUKgwYNwr1793DgwAGsX79e7frS6CMR6ZmciDS2Zs0aOQD5wYMH5enp6fLbt2/Lt2zZIrezs5ObmZnJ79y5Iw8KCpIDkE+cOFHl2iNHjsgByDdu3KhSHh0drVKelpYmNzY2lnfu3Fkuk8mU9b799ls5AHlQUJCy7NChQ3IA8kOHDsnlcrn8xYsX8ho1asirV68uf/Tokcp9Xm1r2LBh8sJ+DZREH4mo7OFwANFb8Pf3R6VKleDi4oIePXrA0tISO3bsgLOzs7LOkCFDVK7ZunUrbGxs0L59e2RkZCgPb29vWFpa4tChQwCAgwcPIjc3FyNGjFBJ048ePfqN/Tp9+jRu3LiB0aNHw9bWVuXcq20VpTT6SET6x+EAorcQGRmJOnXqwNDQEA4ODqhbty7E4pextaGhIapWrapyzZUrV/DkyRNUrly50DbT0tIAALdu3QIA1K5dW+V8pUqVUKFChdf2q2BYomHDhpo9UCn2kYj0j0EA0Vvw9fVVrg4ojImJiUpQAORPuKtcuTI2btxY6DWVKlXSaR+1UR76SERvj0EAUSmrWbMmDh48iBYtWsDMzKzIetWrVweQ/63czc1NWZ6enq42Q7+wewDA+fPn4e/vX2S9ooYGSqOPRKR/nBNAVMq+/PJLSKVSzJgxQ+3cixcv8PjxYwD58w2MjIywePFiyOVyZZ2IiIg33qNJkyaoUaMGIiIilO0VeLWtgj0L/lunNPpIRPrHTABRKWvdujUGDRqE8PBwnDlzBh06dICRkRGuXLmCrVu34scff8Tnn3+OSpUqYdy4cQgPD0eXLl3QqVMnnD59Gvv27YO9vf1r7yEWi7F06VJ07doVXl5eCA4OhpOTExITE3HhwgXs378fAODt7Q0AGDlyJDp27AgDAwP06NGjVPpIRGWAnlcnEJVLBUsE//nnnyLrBAUFyS0sLIo8v2LFCrm3t7fczMxMbmVlJW/UqJF8/Pjx8nv37inrSKVS+XfffSd3cnKSm5mZydu0aSM/f/68vHr16q9dIljg6NGj8vbt28utrKzkFhYWcg8PD/nixYuV51+8eCEfMWKEvFKlSnKRSKS2XFCXfSSiskckl7+SwyMiIiLB4JwAIiIigWIQQEREJFAMAoiIiASKQQAREZFAMQggIiISKAYBREREAsUggIiISKAYBBAREQkUgwAiIiKBYhBAREQkUAwCiIiIBIpBABERkUAxCCAiIhKo/wOKzS1OlCfHzgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1-7-2. Hyperparameter tuning"
      ],
      "metadata": {
        "id": "lY4P8DBaR662"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "보수적 접근 방법은 다음과 같은 특징 때문에 실제 적용이 너무 어렵다.\n",
        "\n",
        "- True Positive(정탐)만 Maximize\n",
        "- False Positive(미탐) 최소화\n",
        "- 보통은 Recall이나 Precision을 더 선호\n",
        "- 미탐보다 오탐이 훨씬 더 많음"
      ],
      "metadata": {
        "id": "5RjhZvnqR77N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "현재 모델의 결과를 보면, 거의 모든 것이 Positive 클래스에 속할 것으로 예상되는 문제가 있다."
      ],
      "metadata": {
        "id": "Lh3VEe9tR9PK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이를 해결하기 위해 몇 가지 제약 조건을 정의해야 한다.\n",
        "예를 들어, 재입원하지 않은 모든 데이터을 포함할 수 있도록 적어도 충분한 true negative(정음성)이 있어야한다."
      ],
      "metadata": {
        "id": "EslePTjiR-ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "min_tn = X_orig[X_orig['readmitted']=='NO'].shape[0] / X_orig.shape[0]\n",
        "print(f\"Patients % that never readmitted (Minimum % of True Negatives): {min_tn:.2%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6-WmyHXPjQE",
        "outputId": "7c06be7a-2878-44b8-b49a-86aade303efa"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Patients % that never readmitted (Minimum % of True Negatives): 53.91%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "또한 우리는 30일이 지나서 재입원한 환자도 False Positive에 포함되도록 비율을 조정해야 한다."
      ],
      "metadata": {
        "id": "0qhI2GPZSSHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_fp = X_orig[X_orig['readmitted']=='>30'].shape[0] / X_orig.shape[0]\n",
        "print(f\"Patients % that were readmitted over 30 days later (Maximum % of False Positives): {max_fp:.2%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdkvBEwNPjNN",
        "outputId": "cb5bdc8d-f96f-4d2f-e996-a706dd1a4157"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Patients % that were readmitted over 30 days later (Maximum % of False Positives): 34.93%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define optimization fuction**"
      ],
      "metadata": {
        "id": "gxBsEvH-SfPW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "함수에서 `scale_pos_weight`를 사용한 클래스 가중치 외에도 `max_depth`(트리 깊이를 제한하기 위해) 및 `reg_lambda` 및 `reg_alpha`를 사용한 L1/L2 정규화에 대한 최상의 하이퍼파라미터를 찾습니다. 이렇게 하면 모델이 일반화되는 데 도움이 됩니다. 목적 함수는 두 제약 조건이 충족되지 않는 경우를 제외하고 `recall`을 출력합니다. 이 경우 0을 반환합니다."
      ],
      "metadata": {
        "id": "AcrAqZ8bSgaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize_lgb(trial):\n",
        "    params = {\n",
        "        'max_depth': trial.suggest_int('max_depth', 2, 11),\n",
        "        'scale_pos_weight': trial.suggest_float('scale_pos_weight', def_scale_pos_weight/2, def_scale_pos_weight*2),\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True)\n",
        "    }\n",
        "    if params['max_depth'] == 11:\n",
        "        params['max_depth'] = -1\n",
        "\n",
        "    clf = lgb.LGBMClassifier(random_state=rand, n_jobs=-1, **params)\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    metrics_dict = evaluate_class_mdl(clf, X_train, X_test, y_train, y_test, plot=False)\n",
        "\n",
        "    if (metrics_dict['tn%'] < min_tn) or (metrics_dict['fp%'] > max_fp):\n",
        "        return 0\n",
        "\n",
        "    return metrics_dict['recall']"
      ],
      "metadata": {
        "id": "aeQDZkQ6PjLC"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. **함수 정의**:\n",
        "```python\n",
        "def optimize_lgb(trial):\n",
        "```\n",
        "- `optimize_lgb`라는 이름의 함수를 정의합니다.\n",
        "- `trial`이라는 인자를 받습니다. 이 인자는 Optuna 라이브러리에서 하이퍼파라미터 최적화를 위한 각 시도를 나타냅니다.\n",
        "\n",
        "2. **하이퍼파라미터 설정**:\n",
        "```python\n",
        "params = {\n",
        "    'max_depth': trial.suggest_int('max_depth', 2, 11),\n",
        "    'scale_pos_weight': trial.suggest_float('scale_pos_weight', def_scale_pos_weight/2, def_scale_pos_weight*2),\n",
        "    'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
        "    'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True)\n",
        "}\n",
        "```\n",
        "- `params` 딕셔너리 안에 LightGBM 모델의 여러 하이퍼파라미터를 설정합니다.\n",
        "- `trial.suggest_...` 메서드를 사용하여 각 하이퍼파라미터의 범위와 타입을 지정합니다.\n",
        "\n",
        "3. **max_depth 값 수정**:\n",
        "```python\n",
        "if params['max_depth'] == 11:\n",
        "    params['max_depth'] = -1\n",
        "```\n",
        "- 만약 `max_depth` 값이 11이라면, 그 값을 -1로 변경합니다. LightGBM에서 `max_depth` 값이 -1인 경우, 제한 없이 깊게 트리를 성장시키라는 의미입니다.\n",
        "\n",
        "4. **모델 학습**:\n",
        "```python\n",
        "clf = lgb.LGBMClassifier(random_state=rand, n_jobs=-1, **params)\n",
        "clf.fit(X_train, y_train)\n",
        "```\n",
        "- 설정된 하이퍼파라미터를 사용하여 LightGBM 분류기를 초기화하고 훈련 데이터로 학습시킵니다.\n",
        "\n",
        "5. **모델 성능 평가**:\n",
        "```python\n",
        "metrics_dict = evaluate_class_mdl(clf, X_train, X_test, y_train, y_test, plot=False)\n",
        "```\n",
        "- 앞서 주어진 `evaluate_class_mdl` 함수를 사용하여 학습된 모델의 성능을 평가하고, 그 결과를 `metrics_dict`에 저장합니다.\n",
        "\n",
        "6. **조건 확인 및 반환**:\n",
        "```python\n",
        "if (metrics_dict['tn%'] < min_tn) or (metrics_dict['fp%'] > max_fp):\n",
        "    return 0\n",
        "```\n",
        "- 만약 True Negative의 비율이 `min_tn`보다 작거나, False Positive의 비율이 `max_fp`보다 크다면, 0을 반환합니다. 이는 해당 하이퍼파라미터 설정에서 원하는 성능을 얻지 못했음을 나타냅니다.\n",
        "\n",
        "7. **결과 반환**:\n",
        "```python\n",
        "return metrics_dict['recall']\n",
        "```\n",
        "- 최종적으로, `recall` 값을 반환합니다. 이 값은 Optuna 라이브러리가 하이퍼파라미터를 최적화하는 데 사용합니다.\n",
        "\n",
        "함수의 주요 목적은 주어진 하이퍼파라미터 설정에서의 모델 성능을 평가하고, 그 성능을 기반으로 최적의 하이퍼파라미터를 찾는 것입니다. Optuna와 같은 라이브러리는 이 함수를 여러 번 호출하며 다양한 하이퍼파라미터 조합을 시도하여 최적의 값을 찾습니다."
      ],
      "metadata": {
        "id": "rYgL3YFNTqrB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run optimization trials**"
      ],
      "metadata": {
        "id": "JlV7dYqSTH0C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`optuna`로 100번의 시도를 실행하는 데 몇 분 정도 걸린다."
      ],
      "metadata": {
        "id": "AeNFMZEZTKAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "opt_study = optuna.create_study(direction='maximize')\n",
        "opt_study.optimize(optimize_lgb, n_trials=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEXBFCzEPjIR",
        "outputId": "5dc193e9-6190-4a51-c7dd-bec692e14b4a"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:05,379] A new study created in memory with name: no-name-65fc8817-6724-47c1-9d2a-c887397b7fae\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.775424 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:08,195] Trial 0 finished with value: 0.0 and parameters: {'max_depth': 7, 'scale_pos_weight': 15.121805714925035, 'reg_lambda': 6.718374415691349e-08, 'reg_alpha': 3.584673017187385e-05}. Best is trial 0 with value: 0.0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005327 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:08,839] Trial 1 finished with value: 0.5527007299270073 and parameters: {'max_depth': 8, 'scale_pos_weight': 7.285324078943653, 'reg_lambda': 0.2537414656103463, 'reg_alpha': 3.894389036781523e-08}. Best is trial 1 with value: 0.5527007299270073.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005579 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:09,449] Trial 2 finished with value: 0.0 and parameters: {'max_depth': 9, 'scale_pos_weight': 13.068285593462033, 'reg_lambda': 0.0009014336482409883, 'reg_alpha': 1.7633980509296692}. Best is trial 1 with value: 0.5527007299270073.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005803 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:10,057] Trial 3 finished with value: 0.0 and parameters: {'max_depth': 8, 'scale_pos_weight': 13.33316888664271, 'reg_lambda': 6.350742461788189e-05, 'reg_alpha': 0.0008573226392481219}. Best is trial 1 with value: 0.5527007299270073.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005325 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:10,699] Trial 4 finished with value: 0.0 and parameters: {'max_depth': 10, 'scale_pos_weight': 9.944959740045801, 'reg_lambda': 9.502061550462396e-05, 'reg_alpha': 6.580814546308679}. Best is trial 1 with value: 0.5527007299270073.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007007 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:13,257] Trial 5 finished with value: 0.49664233576642336 and parameters: {'max_depth': 5, 'scale_pos_weight': 6.670155697044471, 'reg_lambda': 9.459018499810325, 'reg_alpha': 0.1458054377559234}. Best is trial 1 with value: 0.5527007299270073.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007150 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:14,134] Trial 6 finished with value: 0.0 and parameters: {'max_depth': 3, 'scale_pos_weight': 15.649387498443033, 'reg_lambda': 0.007425256981131767, 'reg_alpha': 3.733802702415331}. Best is trial 1 with value: 0.5527007299270073.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.226336 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:15,223] Trial 7 finished with value: 0.0 and parameters: {'max_depth': 7, 'scale_pos_weight': 14.349023680218632, 'reg_lambda': 0.0008347291761900468, 'reg_alpha': 7.347510426851715e-07}. Best is trial 1 with value: 0.5527007299270073.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005742 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:16,319] Trial 8 finished with value: 0.0 and parameters: {'max_depth': 4, 'scale_pos_weight': 15.478299387010061, 'reg_lambda': 0.0005009862477421742, 'reg_alpha': 0.07185091264594592}. Best is trial 1 with value: 0.5527007299270073.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.555636 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:18,199] Trial 9 finished with value: 0.0 and parameters: {'max_depth': 5, 'scale_pos_weight': 9.305589006188278, 'reg_lambda': 5.723253915589503e-05, 'reg_alpha': 0.24013529844052278}. Best is trial 1 with value: 0.5527007299270073.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005900 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:18,960] Trial 10 finished with value: 0.12058394160583942 and parameters: {'max_depth': 11, 'scale_pos_weight': 4.209490206261985, 'reg_lambda': 4.8992511070006035, 'reg_alpha': 1.0587759705473821e-08}. Best is trial 1 with value: 0.5527007299270073.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007760 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:19,728] Trial 11 finished with value: 0.4916788321167883 and parameters: {'max_depth': 5, 'scale_pos_weight': 6.656379827459399, 'reg_lambda': 7.063822717434679, 'reg_alpha': 0.015940383369689247}. Best is trial 1 with value: 0.5527007299270073.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007015 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:21,282] Trial 12 finished with value: 0.5708029197080292 and parameters: {'max_depth': 6, 'scale_pos_weight': 7.445341876549438, 'reg_lambda': 0.2954115430679669, 'reg_alpha': 0.0013860634949149628}. Best is trial 12 with value: 0.5708029197080292.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.108749 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:21,994] Trial 13 finished with value: 0.0 and parameters: {'max_depth': 2, 'scale_pos_weight': 8.144461797956358, 'reg_lambda': 0.18825844553923904, 'reg_alpha': 4.399332836294035e-05}. Best is trial 12 with value: 0.5708029197080292.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005104 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:22,606] Trial 14 finished with value: 0.5868613138686132 and parameters: {'max_depth': 9, 'scale_pos_weight': 7.595879249275127, 'reg_lambda': 0.08899201678601817, 'reg_alpha': 0.0019180091626405076}. Best is trial 14 with value: 0.5868613138686132.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005276 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:23,261] Trial 15 finished with value: 0.0 and parameters: {'max_depth': 9, 'scale_pos_weight': 11.643686086007147, 'reg_lambda': 0.07463769688987422, 'reg_alpha': 0.002001494905706914}. Best is trial 14 with value: 0.5868613138686132.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005518 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:23,890] Trial 16 finished with value: 0.2773722627737226 and parameters: {'max_depth': 6, 'scale_pos_weight': 5.125531244383088, 'reg_lambda': 0.024187808062662613, 'reg_alpha': 0.003987840686630677}. Best is trial 14 with value: 0.5868613138686132.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005489 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:24,765] Trial 17 finished with value: 0.0 and parameters: {'max_depth': 11, 'scale_pos_weight': 8.45152334348036, 'reg_lambda': 0.6340422130858446, 'reg_alpha': 0.00020738788953331765}. Best is trial 14 with value: 0.5868613138686132.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005426 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:26,132] Trial 18 finished with value: 0.0 and parameters: {'max_depth': 9, 'scale_pos_weight': 10.88532542388841, 'reg_lambda': 0.013270049481035497, 'reg_alpha': 0.00810373252559796}. Best is trial 14 with value: 0.5868613138686132.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005260 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:27,245] Trial 19 finished with value: 0.3605839416058394 and parameters: {'max_depth': 6, 'scale_pos_weight': 5.7251145079611625, 'reg_lambda': 1.2053442304318456, 'reg_alpha': 0.0002674974279405276}. Best is trial 14 with value: 0.5868613138686132.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005102 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:28,317] Trial 20 finished with value: 0.0 and parameters: {'max_depth': 8, 'scale_pos_weight': 8.389758857590017, 'reg_lambda': 0.7066200479195752, 'reg_alpha': 7.815822457092896e-06}. Best is trial 14 with value: 0.5868613138686132.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006622 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:29,207] Trial 21 finished with value: 0.5013138686131386 and parameters: {'max_depth': 8, 'scale_pos_weight': 6.780549867326911, 'reg_lambda': 0.12651043746302318, 'reg_alpha': 1.2567721767867842e-06}. Best is trial 14 with value: 0.5868613138686132.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061278 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:30,462] Trial 22 finished with value: 0.5935766423357665 and parameters: {'max_depth': 10, 'scale_pos_weight': 7.691212531642549, 'reg_lambda': 0.4346572438786779, 'reg_alpha': 0.000583558670160461}. Best is trial 22 with value: 0.5935766423357665.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006179 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:31,512] Trial 23 finished with value: 0.5953284671532847 and parameters: {'max_depth': 10, 'scale_pos_weight': 7.687551972379928, 'reg_lambda': 0.9291342559835776, 'reg_alpha': 0.0012332568838189692}. Best is trial 23 with value: 0.5953284671532847.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.132509 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:36,872] Trial 24 finished with value: 0.0 and parameters: {'max_depth': 10, 'scale_pos_weight': 9.25048432379706, 'reg_lambda': 1.6166307199046988, 'reg_alpha': 0.012772043134607131}. Best is trial 23 with value: 0.5953284671532847.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006092 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:39,249] Trial 25 finished with value: 0.6108029197080292 and parameters: {'max_depth': 10, 'scale_pos_weight': 7.879395574884191, 'reg_lambda': 0.061015912579973706, 'reg_alpha': 0.0003427384767486089}. Best is trial 25 with value: 0.6108029197080292.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011973 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:43,673] Trial 26 finished with value: 0.385985401459854 and parameters: {'max_depth': 10, 'scale_pos_weight': 5.941348414856366, 'reg_lambda': 2.0329558884776313, 'reg_alpha': 0.00026783243418783073}. Best is trial 25 with value: 0.6108029197080292.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039659 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:45,926] Trial 27 finished with value: 0.0 and parameters: {'max_depth': 11, 'scale_pos_weight': 8.682000903984507, 'reg_lambda': 0.039929424955663456, 'reg_alpha': 8.368495725309467e-05}. Best is trial 25 with value: 0.6108029197080292.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005739 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:46,799] Trial 28 finished with value: 0.0 and parameters: {'max_depth': 10, 'scale_pos_weight': 9.852258480082817, 'reg_lambda': 0.004750393099177101, 'reg_alpha': 0.0004970329185434069}. Best is trial 25 with value: 0.6108029197080292.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007161 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:48,245] Trial 29 finished with value: 0.5944525547445255 and parameters: {'max_depth': 11, 'scale_pos_weight': 7.759917231579983, 'reg_lambda': 2.5892195538142087, 'reg_alpha': 2.6629508074998857e-05}. Best is trial 25 with value: 0.6108029197080292.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006056 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:48,879] Trial 30 finished with value: 0.0 and parameters: {'max_depth': 11, 'scale_pos_weight': 8.976087711656465, 'reg_lambda': 2.426511510642956, 'reg_alpha': 3.480275567982472e-05}. Best is trial 25 with value: 0.6108029197080292.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005543 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:49,529] Trial 31 finished with value: 0.6014598540145986 and parameters: {'max_depth': 10, 'scale_pos_weight': 7.878507969274607, 'reg_lambda': 0.6320124931389447, 'reg_alpha': 1.2113850462006202e-05}. Best is trial 25 with value: 0.6108029197080292.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006278 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:50,213] Trial 32 finished with value: 0.6081751824817518 and parameters: {'max_depth': 11, 'scale_pos_weight': 7.920541364055442, 'reg_lambda': 2.1319952795975703, 'reg_alpha': 1.4228585453067834e-05}. Best is trial 25 with value: 0.6108029197080292.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005808 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:50,884] Trial 33 finished with value: 0.5293430656934307 and parameters: {'max_depth': 9, 'scale_pos_weight': 7.052896993033975, 'reg_lambda': 0.36402627806376886, 'reg_alpha': 5.546521816571866e-06}. Best is trial 25 with value: 0.6108029197080292.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005449 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:51,532] Trial 34 finished with value: 0.6090510948905109 and parameters: {'max_depth': 10, 'scale_pos_weight': 8.035951023656683, 'reg_lambda': 0.05037644972813387, 'reg_alpha': 0.00010849973367353079}. Best is trial 25 with value: 0.6108029197080292.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005840 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:52,213] Trial 35 finished with value: 0.0 and parameters: {'max_depth': 7, 'scale_pos_weight': 8.392313678444472, 'reg_lambda': 0.04242756795996777, 'reg_alpha': 0.0001236583425873287}. Best is trial 25 with value: 0.6108029197080292.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005475 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:52,853] Trial 36 finished with value: 0.0 and parameters: {'max_depth': 9, 'scale_pos_weight': 9.43212767908369, 'reg_lambda': 0.004082746280186121, 'reg_alpha': 1.2520561799238527e-05}. Best is trial 25 with value: 0.6108029197080292.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006133 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:53,516] Trial 37 finished with value: 0.0 and parameters: {'max_depth': 8, 'scale_pos_weight': 10.55900511542066, 'reg_lambda': 0.16495447876404556, 'reg_alpha': 2.742197349591006e-06}. Best is trial 25 with value: 0.6108029197080292.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006379 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:56,567] Trial 38 finished with value: 0.4216058394160584 and parameters: {'max_depth': 10, 'scale_pos_weight': 6.208884839484374, 'reg_lambda': 0.021489772553523863, 'reg_alpha': 5.80866621105285e-05}. Best is trial 25 with value: 0.6108029197080292.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007726 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:57,610] Trial 39 finished with value: 0.5246715328467153 and parameters: {'max_depth': 11, 'scale_pos_weight': 6.9357934470081855, 'reg_lambda': 9.80766975669001, 'reg_alpha': 1.5833484256999063e-07}. Best is trial 25 with value: 0.6108029197080292.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006814 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:58,698] Trial 40 finished with value: 0.0 and parameters: {'max_depth': 8, 'scale_pos_weight': 8.775744025823586, 'reg_lambda': 0.0750223326881274, 'reg_alpha': 1.3880472832072269e-05}. Best is trial 25 with value: 0.6108029197080292.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011308 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:59,791] Trial 41 finished with value: 0.6154744525547445 and parameters: {'max_depth': 10, 'scale_pos_weight': 7.926867998628939, 'reg_lambda': 0.8284413271486493, 'reg_alpha': 0.00010748920048463551}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007066 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:00,552] Trial 42 finished with value: 0.0 and parameters: {'max_depth': 10, 'scale_pos_weight': 8.052292145285698, 'reg_lambda': 0.3166062780177792, 'reg_alpha': 9.167612973654155e-05}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007229 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:01,300] Trial 43 finished with value: 0.5594160583941605 and parameters: {'max_depth': 11, 'scale_pos_weight': 7.305704005181168, 'reg_lambda': 4.017530257338179, 'reg_alpha': 1.9173828281910887e-05}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007032 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:02,475] Trial 44 finished with value: 0.0 and parameters: {'max_depth': 9, 'scale_pos_weight': 8.021743140877806, 'reg_lambda': 1.187244851050583, 'reg_alpha': 3.5666927550167333e-06}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007048 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:05,112] Trial 45 finished with value: 0.4548905109489051 and parameters: {'max_depth': 11, 'scale_pos_weight': 6.427817022018748, 'reg_lambda': 4.1377017037091735, 'reg_alpha': 5.431896305763212e-05}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010148 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:05,744] Trial 46 finished with value: 0.5275912408759124 and parameters: {'max_depth': 10, 'scale_pos_weight': 7.034096275972473, 'reg_lambda': 0.15000807689582135, 'reg_alpha': 0.00015306981681839838}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006952 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:06,363] Trial 47 finished with value: 0.0 and parameters: {'max_depth': 10, 'scale_pos_weight': 9.082937691687734, 'reg_lambda': 0.5725697783866187, 'reg_alpha': 1.0046081828680355e-05}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006083 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:07,064] Trial 48 finished with value: 0.5672992700729927 and parameters: {'max_depth': 7, 'scale_pos_weight': 7.290725679233404, 'reg_lambda': 9.9175755896143, 'reg_alpha': 0.0005547367147994663}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006352 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:07,739] Trial 49 finished with value: 0.0 and parameters: {'max_depth': 9, 'scale_pos_weight': 8.24775844709045, 'reg_lambda': 0.05719636037789557, 'reg_alpha': 1.2831057290023223e-06}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006140 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:08,352] Trial 50 finished with value: 0.0 and parameters: {'max_depth': 4, 'scale_pos_weight': 9.478301620530884, 'reg_lambda': 0.0020412311078588322, 'reg_alpha': 2.526153030203372e-05}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006835 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:09,084] Trial 51 finished with value: 0.5915328467153285 and parameters: {'max_depth': 10, 'scale_pos_weight': 7.651686369456091, 'reg_lambda': 0.9371874893532784, 'reg_alpha': 0.0010185393010008535}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006175 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:09,756] Trial 52 finished with value: 0.604087591240876 and parameters: {'max_depth': 11, 'scale_pos_weight': 7.901791321478852, 'reg_lambda': 0.20654298926942996, 'reg_alpha': 0.00030735155089074164}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005913 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:10,389] Trial 53 finished with value: 0.4762043795620438 and parameters: {'max_depth': 11, 'scale_pos_weight': 6.60706884260888, 'reg_lambda': 0.2646127762077629, 'reg_alpha': 0.00018272852694786414}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006238 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:11,059] Trial 54 finished with value: 0.0 and parameters: {'max_depth': 11, 'scale_pos_weight': 8.398355627413132, 'reg_lambda': 0.01219325002861966, 'reg_alpha': 0.00032635745493888365}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006601 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:11,723] Trial 55 finished with value: 0.0 and parameters: {'max_depth': 9, 'scale_pos_weight': 8.758007742311822, 'reg_lambda': 0.10060276715426268, 'reg_alpha': 4.120615901714219e-05}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005860 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:12,387] Trial 56 finished with value: 0.0 and parameters: {'max_depth': 11, 'scale_pos_weight': 7.980855825115882, 'reg_lambda': 0.2464730676628332, 'reg_alpha': 9.292007726985918e-05}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006641 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:13,344] Trial 57 finished with value: 0.5687591240875912 and parameters: {'max_depth': 10, 'scale_pos_weight': 7.4494729536595585, 'reg_lambda': 4.06516527740153, 'reg_alpha': 0.002508233307933831}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006019 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:14,588] Trial 58 finished with value: 0.5632116788321168 and parameters: {'max_depth': 2, 'scale_pos_weight': 7.144842930336576, 'reg_lambda': 0.5502774295839646, 'reg_alpha': 0.0008236900453238108}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011078 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:17,266] Trial 59 finished with value: 0.33985401459854014 and parameters: {'max_depth': 10, 'scale_pos_weight': 5.619298531296808, 'reg_lambda': 0.03349037743863412, 'reg_alpha': 0.0002871030361862076}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007647 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:18,034] Trial 60 finished with value: 0.4691970802919708 and parameters: {'max_depth': 9, 'scale_pos_weight': 6.531626205316045, 'reg_lambda': 1.6417328355315755, 'reg_alpha': 1.9104334332541163e-05}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006468 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:18,819] Trial 61 finished with value: 0.5982481751824817 and parameters: {'max_depth': 10, 'scale_pos_weight': 7.824631769566554, 'reg_lambda': 0.8175700858825345, 'reg_alpha': 0.0012170915768700108}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007870 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:20,084] Trial 62 finished with value: 0.6014598540145986 and parameters: {'max_depth': 10, 'scale_pos_weight': 7.867999273862871, 'reg_lambda': 0.13319073912231114, 'reg_alpha': 0.003997181585792564}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006372 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:20,847] Trial 63 finished with value: 0.0 and parameters: {'max_depth': 11, 'scale_pos_weight': 8.653678383948467, 'reg_lambda': 0.09611726127653285, 'reg_alpha': 0.0046547588058868895}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006449 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:21,566] Trial 64 finished with value: 0.0 and parameters: {'max_depth': 11, 'scale_pos_weight': 8.168391452185482, 'reg_lambda': 0.19701316144727335, 'reg_alpha': 0.00013534314640187053}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031954 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:22,395] Trial 65 finished with value: 0.5605839416058395 and parameters: {'max_depth': 10, 'scale_pos_weight': 7.3271151087457875, 'reg_lambda': 0.018786434220302028, 'reg_alpha': 0.0003868890167129254}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008045 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:23,506] Trial 66 finished with value: 0.0 and parameters: {'max_depth': 8, 'scale_pos_weight': 9.145156481984149, 'reg_lambda': 0.39069709409748504, 'reg_alpha': 0.023410120649334613}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007407 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:24,260] Trial 67 finished with value: 0.5086131386861313 and parameters: {'max_depth': 11, 'scale_pos_weight': 6.871516434261479, 'reg_lambda': 0.054559201928517466, 'reg_alpha': 3.443319312793974e-05}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008145 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:25,186] Trial 68 finished with value: 0.6008759124087591 and parameters: {'max_depth': 10, 'scale_pos_weight': 7.717638950074808, 'reg_lambda': 1.8736586439700451, 'reg_alpha': 6.837879073468386e-05}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006752 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:25,834] Trial 69 finished with value: 0.0 and parameters: {'max_depth': 9, 'scale_pos_weight': 8.54883170674196, 'reg_lambda': 0.11328191615429135, 'reg_alpha': 0.0006385767939315269}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005593 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:26,502] Trial 70 finished with value: 0.0 and parameters: {'max_depth': 10, 'scale_pos_weight': 9.616990788324213, 'reg_lambda': 0.5772346323233838, 'reg_alpha': 0.0025463766297841935}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006439 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:27,182] Trial 71 finished with value: 0.6055474452554744 and parameters: {'max_depth': 10, 'scale_pos_weight': 7.795480028078215, 'reg_lambda': 2.62753175513866, 'reg_alpha': 7.351383269272804e-05}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005881 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:27,853] Trial 72 finished with value: 0.0 and parameters: {'max_depth': 10, 'scale_pos_weight': 8.066378055337982, 'reg_lambda': 2.6605945997262896, 'reg_alpha': 0.00016717992165914038}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006000 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:28,545] Trial 73 finished with value: 0.0 and parameters: {'max_depth': 11, 'scale_pos_weight': 8.964855095343657, 'reg_lambda': 5.617292350499451, 'reg_alpha': 7.105574090774774e-06}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006169 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:30,435] Trial 74 finished with value: 0.5620437956204379 and parameters: {'max_depth': 9, 'scale_pos_weight': 7.460479651672464, 'reg_lambda': 1.267417027574727, 'reg_alpha': 6.323028580447011e-05}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006086 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:31,310] Trial 75 finished with value: 0.0 and parameters: {'max_depth': 11, 'scale_pos_weight': 8.433310044549069, 'reg_lambda': 0.23380293091417786, 'reg_alpha': 1.9308843762136446e-05}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005569 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:31,964] Trial 76 finished with value: 0.6052554744525548 and parameters: {'max_depth': 9, 'scale_pos_weight': 7.839164858229969, 'reg_lambda': 0.7990193520708064, 'reg_alpha': 0.00021655758070583282}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005502 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:32,647] Trial 77 finished with value: 0.5194160583941606 and parameters: {'max_depth': 9, 'scale_pos_weight': 6.9276410860606905, 'reg_lambda': 0.9801414150945221, 'reg_alpha': 0.00023863133334491772}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005645 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:33,591] Trial 78 finished with value: 0.574014598540146 and parameters: {'max_depth': 10, 'scale_pos_weight': 7.481294008504661, 'reg_lambda': 2.884513370868218, 'reg_alpha': 0.00012496953714717958}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.073955 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:34,921] Trial 79 finished with value: 0.0 and parameters: {'max_depth': 11, 'scale_pos_weight': 8.230362925966384, 'reg_lambda': 5.631942116278893, 'reg_alpha': 3.9645876413158894e-05}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005757 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:35,704] Trial 80 finished with value: 0.0 and parameters: {'max_depth': 10, 'scale_pos_weight': 8.89648472340216, 'reg_lambda': 0.36759650314535486, 'reg_alpha': 0.0004616503578698285}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011991 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:37,881] Trial 81 finished with value: 0.602919708029197 and parameters: {'max_depth': 10, 'scale_pos_weight': 7.805153419156508, 'reg_lambda': 0.16755501667855388, 'reg_alpha': 8.222466164727654e-05}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015949 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:38,993] Trial 82 finished with value: 0.5424817518248175 and parameters: {'max_depth': 9, 'scale_pos_weight': 7.1584826835780655, 'reg_lambda': 0.4924666696220804, 'reg_alpha': 6.924975115702649e-05}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007500 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:39,772] Trial 83 finished with value: 0.6032116788321168 and parameters: {'max_depth': 11, 'scale_pos_weight': 7.889010958174492, 'reg_lambda': 1.4028857883967776, 'reg_alpha': 0.00025197685179391036}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007157 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:40,532] Trial 84 finished with value: 0.5886131386861314 and parameters: {'max_depth': 11, 'scale_pos_weight': 7.602651961788849, 'reg_lambda': 2.3359688512610326, 'reg_alpha': 0.00020951742793310716}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006113 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:41,254] Trial 85 finished with value: 0.0 and parameters: {'max_depth': 11, 'scale_pos_weight': 8.565920572764668, 'reg_lambda': 1.2366721692314002, 'reg_alpha': 0.0003761453918835419}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006756 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:42,022] Trial 86 finished with value: 0.494014598540146 and parameters: {'max_depth': 11, 'scale_pos_weight': 6.737612085468752, 'reg_lambda': 0.06366006932916675, 'reg_alpha': 0.0001090087994260334}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.326379 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:43,085] Trial 87 finished with value: 0.0 and parameters: {'max_depth': 3, 'scale_pos_weight': 8.144670758866617, 'reg_lambda': 0.18819024195881853, 'reg_alpha': 0.000813293897743886}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008177 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:43,877] Trial 88 finished with value: 0.42656934306569344 and parameters: {'max_depth': 10, 'scale_pos_weight': 6.325309673865998, 'reg_lambda': 0.998989802218437, 'reg_alpha': 4.905196733339637e-05}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016497 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:44,635] Trial 89 finished with value: 0.6002919708029197 and parameters: {'max_depth': 11, 'scale_pos_weight': 7.7993910055973545, 'reg_lambda': 6.171060302870469, 'reg_alpha': 0.00021873424548637526}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006036 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:45,413] Trial 90 finished with value: 0.5392700729927007 and parameters: {'max_depth': 10, 'scale_pos_weight': 7.139463120755624, 'reg_lambda': 3.3170683178430673, 'reg_alpha': 2.9164757404188786e-05}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008203 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:46,212] Trial 91 finished with value: 0.6014598540145986 and parameters: {'max_depth': 10, 'scale_pos_weight': 7.854320181505575, 'reg_lambda': 1.768374355673571, 'reg_alpha': 1.2802510124049133e-05}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021330 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:47,191] Trial 92 finished with value: 0.0 and parameters: {'max_depth': 9, 'scale_pos_weight': 8.286593990771946, 'reg_lambda': 0.5701419317666285, 'reg_alpha': 7.845510313417208e-05}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005296 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:47,853] Trial 93 finished with value: 0.5783941605839416 and parameters: {'max_depth': 10, 'scale_pos_weight': 7.4378140638152805, 'reg_lambda': 0.8102043717334427, 'reg_alpha': 0.00010299535571464251}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008169 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:48,473] Trial 94 finished with value: 0.0 and parameters: {'max_depth': 11, 'scale_pos_weight': 8.759308154042156, 'reg_lambda': 0.3479682764541228, 'reg_alpha': 0.00034636923351772373}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006145 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:49,139] Trial 95 finished with value: 0.0 and parameters: {'max_depth': 10, 'scale_pos_weight': 7.956820332151581, 'reg_lambda': 0.18778937302358178, 'reg_alpha': 0.0005358925044599509}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005980 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:49,761] Trial 96 finished with value: 0.0 and parameters: {'max_depth': 11, 'scale_pos_weight': 9.284191397977573, 'reg_lambda': 1.5825777615844385, 'reg_alpha': 0.00016834777761329364}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006481 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:50,430] Trial 97 finished with value: 0.0 and parameters: {'max_depth': 9, 'scale_pos_weight': 8.393648848275925, 'reg_lambda': 0.7759674301651267, 'reg_alpha': 2.654376931541534e-05}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007238 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:53,307] Trial 98 finished with value: 0.5813138686131387 and parameters: {'max_depth': 5, 'scale_pos_weight': 7.563833580644345, 'reg_lambda': 0.2594434653133516, 'reg_alpha': 0.0013742018436481433}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005726 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:54,025] Trial 99 finished with value: 0.548029197080292 and parameters: {'max_depth': 10, 'scale_pos_weight': 7.2179807314460405, 'reg_lambda': 0.1147236492477531, 'reg_alpha': 5.395714032260445e-05}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "CPU times: user 11min 25s, sys: 1.19 s, total: 11min 26s\n",
            "Wall time: 1min 48s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "해당 코드는 `optuna` 라이브러리를 사용하여 `optimize_lgb` 함수의 결과를 최적화하는 과정을 수행합니다. `optuna`는 하이퍼파라미터 최적화를 위한 Python 라이브러리입니다.\n",
        "\n",
        "### 코드 분석:\n",
        "\n",
        "1. **시간 측정**:\n",
        "```python\n",
        "%%time\n",
        "```\n",
        "- `%%time`은 Jupyter 노트북의 매직 명령어로, 해당 셀의 실행 시간을 측정합니다.\n",
        "\n",
        "2. **Optuna 스터디 생성**:\n",
        "```python\n",
        "opt_study = optuna.create_study(direction='maximize')\n",
        "```\n",
        "- `optuna.create_study` 함수를 사용하여 새로운 최적화 스터디를 생성합니다.\n",
        "- `direction='maximize'`는 최적화의 목표가 최대화임을 나타냅니다. 즉, `optimize_lgb` 함수의 반환값을 최대로 만드는 하이퍼파라미터 조합을 찾는 것이 목표입니다.\n",
        "\n",
        "3. **최적화 시작**:\n",
        "```python\n",
        "opt_study.optimize(optimize_lgb, n_trials=100)\n",
        "```\n",
        "- `opt_study.optimize` 함수를 사용하여 `optimize_lgb` 함수를 대상으로 최적화를 시작합니다.\n",
        "- `n_trials=100`은 총 100번의 시도(trial)를 통해 최적의 하이퍼파라미터를 찾는다는 의미입니다. 각 시도마다 `optimize_lgb` 함수는 다른 하이퍼파라미터 조합으로 호출되며, 그 결과를 바탕으로 최적의 조합을 탐색합니다.\n",
        "\n",
        "### 결론:\n",
        "\n",
        "이 코드는 `optuna`를 사용하여 `optimize_lgb` 함수가 반환하는 값을 최대로 하는 하이퍼파라미터 조합을 찾는 과정을 수행합니다. 최적화 과정은 총 100번의 시도를 통해 이루어지며, 각 시도의 실행 시간을 `%%time`을 통해 측정합니다."
      ],
      "metadata": {
        "id": "9zxmUc7qWhMa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`study`가 끝나면 다음과 같이 최상의 결과를 얻을 수 있는 `model hyperparameter`를 출력할 수 있다."
      ],
      "metadata": {
        "id": "_qUyODylTYt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_params = opt_study.best_params\n",
        "print(best_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvNwR7FePjF_",
        "outputId": "ff7aaa86-fc2b-4166-cb6f-309e55940b8c"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'max_depth': 10, 'scale_pos_weight': 7.926867998628939, 'reg_lambda': 0.8284413271486493, 'reg_alpha': 0.00010748920048463551}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1-7-3 Train Tuned Model"
      ],
      "metadata": {
        "id": "cHkAoDT1TkVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "clf = lgb.LGBMClassifier(random_state=rand, n_jobs=-1,\\\n",
        "                         **best_params)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "_, y_prob, y_pred = evaluate_class_mdl(clf, X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 764
        },
        "id": "qgMVyYQzPjDX",
        "outputId": "249f1062-baca-496d-e9da-397d0883f02e"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005038 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "Accuracy_train:  0.6379\t\tAccuracy_test:   0.6103\n",
            "Precision_test:  0.1661\t\tRecall_test:     0.6155\n",
            "ROC-AUC_test:    0.6480\t\tF1_test:         0.2616\t\tMCC_test: 0.1441\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **🥈Session 2**  \n",
        "**┗ Error analysis**  \n",
        "---"
      ],
      "metadata": {
        "id": "K_TL7jjpUKAY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) Error Analysis 소개"
      ],
      "metadata": {
        "id": "4rUtWJ9QULY3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2-1. 현재까지의 결과가 최선일까?"
      ],
      "metadata": {
        "id": "GpEu7Oo0UMxH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Confusion Matrix`를 자세히 살펴보면 `False Positive`보다 `True Positive`가 60% 더 많지만 `False Positive`가 결과적으로 더 심각한 비용을 발생시킨다.\n",
        "그러나 우리가 설정한 제약으로 인해 `True Positive`의 절반 이상이 결국 재입원으로 분류될 것다. (30일 이상 지난 환자도 재분류하는 제약)"
      ],
      "metadata": {
        "id": "ieYxPv_XUNxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_orig_test['readmitted'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXLFwIvXPjAu",
        "outputId": "fdadff73-c550-48f8-b270-666b08a16700"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NO     16461\n",
              ">30    10644\n",
              "<30     3425\n",
              "Name: readmitted, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "대부분의 모델 목표가 30일 이상 재입원을 예상하므로 긍정적인 클래스가 이 그룹을 독점적으로 나타낸다.\n",
        "30일 이상 재입원 하지 않은 것에 대한 `False Positive` 오분류는 `NO` 에 대한 오분류만큼 나쁘지 않다.\n",
        "그렇다면 30일 이후 재입원에 대한 오탐지 비율은 얼마일까요?"
      ],
      "metadata": {
        "id": "LN6GExJ8U-Tg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds_df = pd.DataFrame({'readmitted':X_orig_test['readmitted'],\\\n",
        "                         'y_true':y_test.astype(int),\\\n",
        "                         'y_pred':y_pred})\n",
        "preds_df[(preds_df.y_true==0) & (preds_df.y_pred==1)].value_counts(normalize=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhQmctydPi97",
        "outputId": "1fa73263-fc23-4674-f29e-fed2a56bd141"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "readmitted  y_true  y_pred\n",
              "NO          0       1         0.503639\n",
              ">30         0       1         0.496361\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 코드 분석:\n",
        "\n",
        "1. **데이터프레임 생성**:\n",
        "```python\n",
        "preds_df = pd.DataFrame({'readmitted':X_orig_test['readmitted'],\n",
        "                         'y_true':y_test.astype(int),\n",
        "                         'y_pred':y_pred})\n",
        "```\n",
        "- 새로운 `preds_df` 데이터프레임을 생성합니다.\n",
        "- 이 데이터프레임에는 3개의 열이 포함됩니다:\n",
        "    - `readmitted`: `X_orig_test` 데이터프레임에서 `readmitted` 열의 값을 가져옵니다.\n",
        "    - `y_true`: 실제 테스트 데이터의 라벨 값을 정수로 변환하여 저장합니다.\n",
        "    - `y_pred`: 모델로부터 얻은 예측값을 저장합니다.\n",
        "\n",
        "2. **조건에 맞는 데이터 필터링**:\n",
        "```python\n",
        "preds_df[(preds_df.y_true==0) & (preds_df.y_pred==1)]\n",
        "```\n",
        "- `preds_df`에서 실제 값(`y_true`)이 0이고 예측값(`y_pred`)이 1인 행만 선택합니다. 이는 모델이 Negative(0)로 분류해야 하는 샘플을 Positive(1)로 잘못 예측한 경우를 나타냅니다.\n",
        "\n",
        "3. **값의 빈도 계산**:\n",
        "```python\n",
        ".value_counts(normalize=True)\n",
        "```\n",
        "- 선택된 행의 각 값의 빈도를 계산합니다.\n",
        "- `normalize=True` 옵션은 결과를 전체 행의 수로 나눠 백분율로 반환하도록 합니다.\n",
        "\n",
        "### 결론:\n",
        "\n",
        "이 코드는 주어진 예측 결과와 실제 라벨을 바탕으로 모델이 Negative 클래스를 Positive로 잘못 분류한 경우의 빈도를 백분율로 계산하고 출력합니다. 이는 모델의 False Positive 비율을 파악하는 데 도움이 됩니다."
      ],
      "metadata": {
        "id": "0aog_mSTW2f7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "실제로 오탐지의 거의 절반(전체 테스트 샘플의 34.72%)이 결국 재입원하였으므로 사전 예방 조치를 취하는 것이 나쁜 것이 아니라고 판단된다.\n",
        "우리가 고려해야 할 잘못된 결과는 나머지 절반에 대한 부분이다."
      ],
      "metadata": {
        "id": "PLKvcIJ-V3iJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2-2. Error를 주제 그룹으로 나눠보자"
      ],
      "metadata": {
        "id": "xm_LT52DWFHm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2-2-1. Error를 그룹별로 비교해보기 위해 함수를 하나 만들어보자"
      ],
      "metadata": {
        "id": "z_e2oj1vWGed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_header_font():\n",
        "    return [dict(selector=\"th\", props=[(\"font-size\", \"14pt\")])]\n",
        "\n",
        "def metrics_by_group(s):\n",
        "    accuracy = metrics.accuracy_score(s.y_true, s.y_pred) * 100\n",
        "    precision = metrics.precision_score(s.y_true, s.y_pred) * 100\n",
        "    recall = metrics.recall_score(s.y_true, s.y_pred) * 100\n",
        "    f1 = metrics.f1_score(s.y_true, s.y_pred) * 100\n",
        "    roc_auc, fnr, fpr = np.nan, np.nan, np.nan\n",
        "    if len(np.unique(s.y_true)) == 2:\n",
        "        roc_auc = metrics.roc_auc_score(s.y_true, s.y_prob) * 100\n",
        "        tn, fp, fn, tp = metrics.confusion_matrix(s.y_true, s.y_pred).ravel()\n",
        "        fnr = (fn/(tp+fn)) * 100\n",
        "        fpr = (fp/(tn+fp)) * 100\n",
        "    support = len(s.y_true)\n",
        "\n",
        "    return pd.Series((support, accuracy, precision, recall, f1, roc_auc, fnr, fpr),\\\n",
        "                 index=['support', 'accuracy', 'precision', 'recall', 'f1', 'roc-auc', 'fnr', 'fpr'])\n",
        "\n",
        "def error_breakdown_by_group(mdl, y_true, y_prob, y_pred, orig_df, group_col, exclude_groups=None):\n",
        "\n",
        "    print(f\"Error breakdown for group '{group_col}'\")\n",
        "\n",
        "    predict_df = pd.DataFrame({group_col: orig_df[group_col].tolist(),\\\n",
        "                              'y_true': y_true,\n",
        "                              'y_pred': y_pred,\n",
        "                              'y_prob': y_prob}, index=y_true.index)\n",
        "    if exclude_groups is not None:\n",
        "        predict_df = predict_df[~predict_df[group_col].isin(exclude_groups)]\n",
        "\n",
        "    group_metrics_df = predict_df.groupby([group_col]).apply(metrics_by_group)\n",
        "\n",
        "    html = group_metrics_df.sort_values(by='support', ascending=False).style.\\\n",
        "            format({'support':'{:,.0f}', 'accuracy':'{:.1f}%', 'precision':'{:.1f}%', 'recall':'{:.1f}%',\\\n",
        "                    'f1':'{:.1f}%', 'roc-auc':'{:.1f}%', 'fnr':'{:.1f}%', 'fpr':'{:.1f}%'}).\\\n",
        "            set_properties(**{'font-size': '13pt'}).set_table_styles(set_header_font()).\\\n",
        "            highlight_max(subset=['accuracy','precision','recall','f1','roc-auc']).\\\n",
        "            highlight_min(subset=['fnr','fpr'])\n",
        "\n",
        "    return html\n",
        "\n",
        "def compare_confusion_matrices(y_true_1, y_pred_1, y_true_2, y_pred_2, group_1, group_2,\\\n",
        "                               plot=True, compare_fpr=False):\n",
        "\n",
        "    conf_matrix_1 = metrics.confusion_matrix(y_true_1, y_pred_1)\n",
        "    conf_matrix_2 = metrics.confusion_matrix(y_true_2, y_pred_2)\n",
        "\n",
        "    if plot:\n",
        "        fig, ax = plt.subplots(1,2,figsize=(12,5))\n",
        "        sns.heatmap(conf_matrix_1/np.sum(conf_matrix_1), annot=True,\\\n",
        "                    fmt='.2%', cmap='Blues', annot_kws={'size':16}, ax=ax[0])\n",
        "        ax[0].set_title(group_1 + ' Confusion Matrix', fontsize=14)\n",
        "        ax[0].set_xlabel('Predicted', fontsize=12)\n",
        "        ax[0].set_ylabel('Observed', fontsize=12)\n",
        "        sns.heatmap(conf_matrix_2/np.sum(conf_matrix_2), annot=True,\\\n",
        "                    fmt='.2%', cmap='Blues', annot_kws={'size':16}, ax=ax[1])\n",
        "        ax[1].set_title(group_2 + ' Confusion Matrix', fontsize=14)\n",
        "        ax[1].set_xlabel('Predicted', fontsize=12)\n",
        "        ax[1].set_ylabel('Observed', fontsize=12)\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "QV5lK-oAPfa5"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### 1. `set_header_font` 함수\n",
        "```python\n",
        "def set_header_font():\n",
        "    return [dict(selector=\"th\", props=[(\"font-size\", \"14pt\")])]\n",
        "```\n",
        "- 테이블의 헤더에 대한 폰트 사이즈를 설정하는 함수입니다.\n",
        "- \"14pt\" 크기의 폰트를 사용하도록 설정값을 반환합니다.\n",
        "\n",
        "### 2. `metrics_by_group` 함수\n",
        "```python\n",
        "def metrics_by_group(s):\n",
        "```\n",
        "- 주어진 그룹의 데이터를 기반으로 다양한 평가 지표를 계산하는 함수입니다.\n",
        "\n",
        "```python\n",
        "accuracy = metrics.accuracy_score(s.y_true, s.y_pred) * 100\n",
        "precision = metrics.precision_score(s.y_true, s.y_pred) * 100\n",
        "recall = metrics.recall_score(s.y_true, s.y_pred) * 100\n",
        "f1 = metrics.f1_score(s.y_true, s.y_pred) * 100\n",
        "```\n",
        "- 정확도, 정밀도, 재현율, F1 스코어를 계산합니다.\n",
        "\n",
        "```python\n",
        "if len(np.unique(s.y_true)) == 2:\n",
        "    ...\n",
        "```\n",
        "- 라벨 값이 2개인 경우 (이진 분류)에만 ROC-AUC, FNR, FPR을 계산합니다.\n",
        "\n",
        "### 3. `error_breakdown_by_group` 함수\n",
        "```python\n",
        "def error_breakdown_by_group(mdl, y_true, y_prob, y_pred, orig_df, group_col, exclude_groups=None):\n",
        "```\n",
        "- 주어진 그룹별로 예측 오류를 분석하는 함수입니다.\n",
        "\n",
        "```python\n",
        "predict_df = pd.DataFrame({...})\n",
        "```\n",
        "- 예측 결과와 실제 라벨, 그룹 정보를 포함하는 새로운 데이터프레임을 생성합니다.\n",
        "\n",
        "```python\n",
        "if exclude_groups is not None:\n",
        "    ...\n",
        "```\n",
        "- 제외할 그룹이 지정된 경우 해당 그룹을 데이터프레임에서 제외합니다.\n",
        "\n",
        "```python\n",
        "group_metrics_df = predict_df.groupby([group_col]).apply(metrics_by_group)\n",
        "```\n",
        "- 그룹별로 `metrics_by_group` 함수를 적용하여 각 그룹의 평가 지표를 계산합니다.\n",
        "\n",
        "### 4. `compare_confusion_matrices` 함수\n",
        "```python\n",
        "def compare_confusion_matrices(y_true_1, y_pred_1, y_true_2, y_pred_2, group_1, group_2, plot=True, compare_fpr=False):\n",
        "```\n",
        "- 두 개의 그룹의 혼동 행렬을 비교하는 함수입니다.\n",
        "\n",
        "```python\n",
        "conf_matrix_1 = metrics.confusion_matrix(y_true_1, y_pred_1)\n",
        "conf_matrix_2 = metrics.confusion_matrix(y_true_2, y_pred_2)\n",
        "```\n",
        "- 두 그룹에 대한 혼동 행렬을 계산합니다.\n",
        "\n",
        "```python\n",
        "if plot:\n",
        "    ...\n",
        "```\n",
        "- `plot=True`인 경우, 두 그룹의 혼동 행렬을 히트맵으로 시각화하여 표시합니다.\n",
        "\n",
        "### 결론:\n",
        "이 코드는 주어진 데이터의 그룹별 예측 오류와 성능 지표를 분석하고 시각화하는 여러 유틸리티 함수들을 정의합니다. 각 함수는 특정 평가 지표를 계산하거나, 그룹별 예측 오류를 분석하거나, 혼동 행렬을 비교 및 시각화하는 역할을 수행합니다."
      ],
      "metadata": {
        "id": "0m0-ZJ7kXgjs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2-2-2. 그룹 별 에러 분석"
      ],
      "metadata": {
        "id": "us-X0rJGXNfI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FgW-D15mSin_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LtJ6ZcFxSilj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VJAIivQiSijI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JmjFwdoNSigb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}