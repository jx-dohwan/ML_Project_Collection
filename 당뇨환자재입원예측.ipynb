{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **[Chapter 02]**  \n",
        "**â”— ë‹¹ë‡¨ í™˜ì ë³‘ì› ì¬ë°©ë¬¸ ì˜ˆì¸¡**\n",
        "---\n",
        "> **ëª©ì°¨(Context)**\n",
        "\n",
        "* ë¬¸ì œìƒí™© ë° ë°ì´í„° ì‚´í´ë³´ê¸°\n",
        "* ë¬¸ì œí•´ê²° í”„ë¡œì„¸ìŠ¤ ì •ì˜\n",
        "* ğŸ¥‰Session 1 - ã€ŒData ì „ì²˜ë¦¬ ë° EDAã€\n",
        "* ğŸ¥ˆSession 2 - ã€ŒError analysisã€\n",
        "* ğŸ¥‡Session 3 - ã€Œì£¼ì œê·¸ë£¹ ë¶„ì„ã€"
      ],
      "metadata": {
        "id": "f8Id8v-dWg__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# â–¶ Warnings ì œê±°\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# â–¶ Google drive mount or í´ë” í´ë¦­ í›„ êµ¬ê¸€ë“œë¼ì´ë¸Œ ì—°ê²°\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# â–¶ ê²½ë¡œ ì„¤ì • (â€» Colabì„ í™œì„±í™”ì‹œì¼°ë‹¤ë©´ ë³´í†µ Colab Notebooks í´ë”ê°€ ìë™ ìƒì„±)\n",
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/á„‹á…µá†«á„€á…©á†¼á„Œá…µá„‚á…³á†¼/ML_Project_Collection/á„ƒá…¡á†¼á„‚á…­á„’á…ªá†«á„Œá…¢á„Œá…¢á„‹á…µá†¸á„‹á…¯á†«á„‹á…¨á„á…³á†¨\")\n",
        "os.getcwd()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Xsho6OTtW8sm",
        "outputId": "cc034a5b-c08f-4383-bbed-1a52b2b841c5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/á„‹á…µá†«á„€á…©á†¼á„Œá…µá„‚á…³á†¼/ML_Project_Collection/á„ƒá…¡á†¼á„‚á…­á„’á…ªá†«á„Œá…¢á„Œá…¢á„‹á…µá†¸á„‹á…¯á†«á„‹á…¨á„á…³á†¨'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ë¬¸ì œìƒí™© ë° ë°ì´í„° ì‚´í´ë³´ê¸°**\n",
        "---\n",
        "> **ì‹œë‚˜ë¦¬ì˜¤** ğŸ­\n",
        "\n",
        "```\n",
        "ë‹¹ë‡¨ë³‘ì€ ì„¸ê³„ ì„±ì¸ ì¸êµ¬ì˜ 9% ì´ìƒì— ì˜í–¥ì„ ë¯¸ì¹˜ê³  ìˆìœ¼ë©°, ì ì  ë” ì¦ê°€í•˜ê³  ìˆë‹¤.\n",
        "ë‹¹ë‡¨ í™˜ìëŠ” ì§€ì†ì ì¸ ê´€ì°°ê³¼ ì¹˜ë£Œê°€ í•„ìš”í•˜ë©°, ì¤‘ì¦ í™˜ìëŠ” ë” ì„¸ì‹¬í•œ ê´€ë¦¬ê°€ í•„ìš”í•˜ë‹¤.\n",
        "ì¼ë‹¨, ìš°ë¦¬ì˜ ëª©ì ì€ 30ì¼ ì´ë‚´ì— ì¬ì…ì›í•´ì•¼ í•˜ëŠ” í™˜ìë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì„ ìƒì„±í•˜ëŠ” ê²ƒì´ë‹¤.\n",
        "ë³‘ì›ì—ì„œ ì¤‘ì¦ í™˜ìì˜ ëª¨ë‹ˆí„°ë§, ì¡°ê¸° ì˜ˆì•½, ê°„í˜¸ì‚¬ í˜¸ì¶œê³¼ ê°™ì€ ì˜ˆë°© ì¡°ì¹˜ë¥¼ ë” ë§ì´ ì·¨í•  ìˆ˜ ìˆë‹¤ê³  ìƒê°í•´ë³´ì.\n",
        "ì´ëŠ”, ë§ì€ ìƒëª…ì„ êµ¬í•  ìˆ˜ ìˆëŠ” ì¼ì´ ë ì§€ë„ ëª¨ë¥¸ë‹¤.\n",
        "ìš°ë¦¬ëŠ” ë‹¹ë‡¨ í™˜ìì˜ ì¬ì…ì›ì„ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì„ ë§Œë“¤ì–´ë³´ê³ , ëª¨ë¸ì˜ ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´ ì—ëŸ¬ì˜ ë¶„í¬ë¥¼ ë¶„ì„í•˜ì—¬ ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼œë³´ì.\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "6kfhtLT7XED8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# openml APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¹ë‡¨ í™˜ì ë°ì´í„° ì½ì–´ì˜¤ê¸°\n",
        "from sklearn.datasets import fetch_openml\n",
        "X_orig, y = fetch_openml(data_id=43874, as_frame=True, return_X_y=True)"
      ],
      "metadata": {
        "id": "9aA_X2duXCEK"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_orig.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIc4I9slXT6Q",
        "outputId": "7acc8852-cff8-4920-aeaa-fc9fb16d080c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 101766 entries, 0 to 101765\n",
            "Data columns (total 24 columns):\n",
            " #   Column                    Non-Null Count   Dtype   \n",
            "---  ------                    --------------   -----   \n",
            " 0   race                      101766 non-null  object  \n",
            " 1   gender                    101766 non-null  object  \n",
            " 2   age                       101766 non-null  object  \n",
            " 3   discharge_disposition_id  101766 non-null  object  \n",
            " 4   admission_source_id       101766 non-null  object  \n",
            " 5   time_in_hospital          101766 non-null  int64   \n",
            " 6   medical_specialty         101766 non-null  object  \n",
            " 7   num_lab_procedures        101766 non-null  int64   \n",
            " 8   num_procedures            101766 non-null  int64   \n",
            " 9   num_medications           101766 non-null  int64   \n",
            " 10  primary_diagnosis         101766 non-null  object  \n",
            " 11  number_diagnoses          101766 non-null  int64   \n",
            " 12  max_glu_serum             101766 non-null  object  \n",
            " 13  A1Cresult                 101766 non-null  object  \n",
            " 14  insulin                   101766 non-null  object  \n",
            " 15  change                    101766 non-null  object  \n",
            " 16  diabetesMed               101766 non-null  object  \n",
            " 17  medicare                  101766 non-null  category\n",
            " 18  medicaid                  101766 non-null  category\n",
            " 19  had_emergency             101766 non-null  category\n",
            " 20  had_inpatient_days        101766 non-null  category\n",
            " 21  had_outpatient_days       101766 non-null  category\n",
            " 22  readmitted                101766 non-null  object  \n",
            " 23  readmit_binary            101766 non-null  int64   \n",
            "dtypes: category(5), int64(6), object(13)\n",
            "memory usage: 15.2+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **ë°ì´í„° ì‚´í´ë³´ê¸°**\n",
        "\n",
        "* í™˜ì ì…ì› ë‹¹ì‹œ ì¸¡ì • ë°ì´í„°\n",
        "* ë°ì´í„° ëª…ì„¸ â¬‡\n",
        "\n",
        "|Column|Description|\n",
        "|:---|:---|\n",
        "|Features|í™˜ì ì…ì› ê¸°ê°„ ë‚´ ê´€ë ¨ ì •ë³´|\n",
        "|Class|readmit_30_days|"
      ],
      "metadata": {
        "id": "J6i227ScXZKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ê° ì»¬ëŸ¼ë³„ë¡œ ê°’ì„ ì‹¤ì œë¡œ í™•ì¸í•´ë³¸ë‹¤.\n",
        "X_orig.sample(n=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "38W3jQnzXVXf",
        "outputId": "9f4c5dab-674f-4893-9530-b8efb27b7dfc"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            race  gender            age discharge_disposition_id  \\\n",
              "66230  Caucasian  Female  Over 60 years                    Other   \n",
              "27779  Caucasian  Female  Over 60 years       Discharged to Home   \n",
              "99219      Other    Male  Over 60 years       Discharged to Home   \n",
              "65334  Caucasian  Female    30-60 years                    Other   \n",
              "46032  Caucasian  Female  Over 60 years                    Other   \n",
              "\n",
              "      admission_source_id  time_in_hospital medical_specialty  \\\n",
              "66230            Referral                 3           Missing   \n",
              "27779           Emergency                 8  InternalMedicine   \n",
              "99219           Emergency                 5           Missing   \n",
              "65334            Referral                 8           Missing   \n",
              "46032            Referral                 7             Other   \n",
              "\n",
              "       num_lab_procedures  num_procedures  num_medications primary_diagnosis  \\\n",
              "66230                  59               5               22             Other   \n",
              "27779                  61               0               19             Other   \n",
              "99219                  64               3               18             Other   \n",
              "65334                   6               0                5             Other   \n",
              "46032                  30               3               14             Other   \n",
              "\n",
              "       number_diagnoses max_glu_serum A1Cresult insulin change diabetesMed  \\\n",
              "66230                 9          None      None      No     No         Yes   \n",
              "27779                 9          None        >8  Steady     No         Yes   \n",
              "99219                 8          None      Norm      Up     Ch         Yes   \n",
              "65334                 6          None      None  Steady     No         Yes   \n",
              "46032                 9          None      None      No     Ch         Yes   \n",
              "\n",
              "      medicare medicaid had_emergency had_inpatient_days had_outpatient_days  \\\n",
              "66230    False    False         False              False               False   \n",
              "27779     True    False         False               True               False   \n",
              "99219    False    False         False              False               False   \n",
              "65334     True    False          True               True               False   \n",
              "46032    False    False         False              False                True   \n",
              "\n",
              "      readmitted  readmit_binary  \n",
              "66230        >30               1  \n",
              "27779         NO               0  \n",
              "99219        <30               1  \n",
              "65334        >30               1  \n",
              "46032         NO               0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4610d91b-4ddd-4c50-8a78-82be98554b68\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>race</th>\n",
              "      <th>gender</th>\n",
              "      <th>age</th>\n",
              "      <th>discharge_disposition_id</th>\n",
              "      <th>admission_source_id</th>\n",
              "      <th>time_in_hospital</th>\n",
              "      <th>medical_specialty</th>\n",
              "      <th>num_lab_procedures</th>\n",
              "      <th>num_procedures</th>\n",
              "      <th>num_medications</th>\n",
              "      <th>primary_diagnosis</th>\n",
              "      <th>number_diagnoses</th>\n",
              "      <th>max_glu_serum</th>\n",
              "      <th>A1Cresult</th>\n",
              "      <th>insulin</th>\n",
              "      <th>change</th>\n",
              "      <th>diabetesMed</th>\n",
              "      <th>medicare</th>\n",
              "      <th>medicaid</th>\n",
              "      <th>had_emergency</th>\n",
              "      <th>had_inpatient_days</th>\n",
              "      <th>had_outpatient_days</th>\n",
              "      <th>readmitted</th>\n",
              "      <th>readmit_binary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>66230</th>\n",
              "      <td>Caucasian</td>\n",
              "      <td>Female</td>\n",
              "      <td>Over 60 years</td>\n",
              "      <td>Other</td>\n",
              "      <td>Referral</td>\n",
              "      <td>3</td>\n",
              "      <td>Missing</td>\n",
              "      <td>59</td>\n",
              "      <td>5</td>\n",
              "      <td>22</td>\n",
              "      <td>Other</td>\n",
              "      <td>9</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>&gt;30</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27779</th>\n",
              "      <td>Caucasian</td>\n",
              "      <td>Female</td>\n",
              "      <td>Over 60 years</td>\n",
              "      <td>Discharged to Home</td>\n",
              "      <td>Emergency</td>\n",
              "      <td>8</td>\n",
              "      <td>InternalMedicine</td>\n",
              "      <td>61</td>\n",
              "      <td>0</td>\n",
              "      <td>19</td>\n",
              "      <td>Other</td>\n",
              "      <td>9</td>\n",
              "      <td>None</td>\n",
              "      <td>&gt;8</td>\n",
              "      <td>Steady</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>NO</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99219</th>\n",
              "      <td>Other</td>\n",
              "      <td>Male</td>\n",
              "      <td>Over 60 years</td>\n",
              "      <td>Discharged to Home</td>\n",
              "      <td>Emergency</td>\n",
              "      <td>5</td>\n",
              "      <td>Missing</td>\n",
              "      <td>64</td>\n",
              "      <td>3</td>\n",
              "      <td>18</td>\n",
              "      <td>Other</td>\n",
              "      <td>8</td>\n",
              "      <td>None</td>\n",
              "      <td>Norm</td>\n",
              "      <td>Up</td>\n",
              "      <td>Ch</td>\n",
              "      <td>Yes</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>&lt;30</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65334</th>\n",
              "      <td>Caucasian</td>\n",
              "      <td>Female</td>\n",
              "      <td>30-60 years</td>\n",
              "      <td>Other</td>\n",
              "      <td>Referral</td>\n",
              "      <td>8</td>\n",
              "      <td>Missing</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>Other</td>\n",
              "      <td>6</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Steady</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>&gt;30</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46032</th>\n",
              "      <td>Caucasian</td>\n",
              "      <td>Female</td>\n",
              "      <td>Over 60 years</td>\n",
              "      <td>Other</td>\n",
              "      <td>Referral</td>\n",
              "      <td>7</td>\n",
              "      <td>Other</td>\n",
              "      <td>30</td>\n",
              "      <td>3</td>\n",
              "      <td>14</td>\n",
              "      <td>Other</td>\n",
              "      <td>9</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>No</td>\n",
              "      <td>Ch</td>\n",
              "      <td>Yes</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>NO</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4610d91b-4ddd-4c50-8a78-82be98554b68')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4610d91b-4ddd-4c50-8a78-82be98554b68 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4610d91b-4ddd-4c50-8a78-82be98554b68');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f565e3b0-8b53-4f1b-8a26-c7320d7afc77\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f565e3b0-8b53-4f1b-8a26-c7320d7afc77')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f565e3b0-8b53-4f1b-8a26-c7320d7afc77 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **ë¬¸ì œí•´ê²° í”„ë¡œì„¸ìŠ¤ ì •ì˜**\n",
        "---\n",
        "> **ë¬¸ì œì •ì˜**\n",
        "\n",
        "```\n",
        "â–¶ í‡´ì›í•œ í™˜ìë“¤ ì¤‘ì— ë‹¤ì‹œ ì…ì›í•˜ëŠ” í™˜ìë“¤ì´ ëŠ˜ì–´ë‚¨\n",
        "```  \n",
        "\n",
        "> **ê¸°ëŒ€íš¨ê³¼**\n",
        "\n",
        "```\n",
        "â–¶ ì¬ì…ì› í™˜ì ì˜ˆì¸¡ì„ í†µí•´ ì¤‘ì¦ í™˜ì ì‚¬ì „ ì¡°ì¹˜ ë° ëª¨ë‹ˆí„°ë§\n",
        "â–¶ ì¤‘ì¦ í™˜ì ìƒì¡´ìœ¨ ì¦ê°€\n",
        "```\n",
        "\n",
        "> **í•´ê²°ë°©ì•ˆ**\n",
        "\n",
        "```\n",
        "â–¶ Binary classificationì„ í†µí•´ 30ì¼ ì´ë‚´ ì¬ì…ì›í•  í™˜ìë¥¼ ë¶„ë¥˜\n",
        "â–¶ Session 1 ğŸ¥‰\n",
        " - ì¸ì½”ë”©ì„ í†µí•œ ë°ì´í„° ì¤€ë¹„\n",
        " - ë² ì´ìŠ¤ ëª¨ë¸ ìƒì„±í•˜ê¸°\n",
        "â–¶ Session 2 ğŸ¥ˆ\n",
        " - Class weightê°€ ì˜¤ë¥˜ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ë¶„ì„\n",
        " - í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\n",
        "â–¶ Session 3 ğŸ¥‡\n",
        " - ì—ëŸ¬ë¥¼ ì„œë¡œ ë‹¤ë¥¸ ì£¼ì œ ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆ ë³´ê¸°\n",
        " - Feature importance ì•Œì•„ë³´ê¸°\n",
        "\n",
        "ê¸°íƒ€: ì—ëŸ¬ë¥¼ ì¤„ì¼ ìˆ˜ ìˆëŠ” ëª‡ê°€ì§€ ë°©ë²•ì— ëŒ€í•œ ì´ì•¼ê¸°\n",
        "```\n",
        "\n",
        "> **ì„±ê³¼ì¸¡ì •**  \n",
        "\n",
        "```\n",
        "â–¶ ì—ëŸ¬ ë¶„ì„ì„ í†µí•œ ì ì§„ì  ì„±ëŠ¥ í–¥ìƒ\n",
        "```\n",
        "\n",
        "> **í˜„ì—…ì ìš©**  \n",
        "\n",
        "```\n",
        "â–¶ ëª¨ë¸ ì„±ëŠ¥ í•˜ë½ì‹œ ë¶„ì„ í¬ì¸íŠ¸ íƒìƒ‰\n",
        "```\n",
        "\n",
        "> **ì£¼ìš” í•µì‹¬ ë¯¸ë¦¬ ì‚´í´ë³´ê¸°**  \n",
        "\n",
        "```\n",
        "â–¶ Session 1 â†’ OneHotEncoderë¥¼ í™œìš©í•œ Feature ìƒì„±\n",
        "â–¶ Session 2 â†’ LightGBMClassifierë¥¼ í™œìš©í•œ ë² ì´ìŠ¤ ëª¨ë¸ ìƒì„±, optunaë¥¼ í™œìš©í•œ Hyperparameter íŠœë‹\n",
        "â–¶ Session 3 â†’ ì¼ë°˜ì ì¸ ì—ëŸ¬ ë¶„ì„, Cohort ì—ëŸ¬ ë¶„ì„\n",
        "```"
      ],
      "metadata": {
        "id": "ZkCKZaI-XrKO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Spec Check**\n",
        "---\n",
        "> **Data ê°€ê³µ ëª…ì„¸ì„œ**\n",
        "\n",
        "<img src = \"https://drive.google.com/uc?id=10ghjKrqQWBHS4dYzol3IIU1CzJ_vv4Ih\">"
      ],
      "metadata": {
        "id": "6-tGBfdWXtXh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ğŸ¥‰Session 1**  \n",
        "**â”— Data ì „ì²˜ë¦¬ ë° EDA**  \n",
        "---"
      ],
      "metadata": {
        "id": "sgOdWreOYV3q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-0. ëª‡ ê°€ì§€ í•„ìš”í•œ Library ì„¤ì¹˜"
      ],
      "metadata": {
        "id": "CZTIEk7vYXVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install raiwidgets lightgbm optuna shap --quiet"
      ],
      "metadata": {
        "id": "7ovccTfvXfjI"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-1. í•„ìš”í•œ íŒ¨í‚¤ì§€ Import"
      ],
      "metadata": {
        "id": "bcJxwh4VYc0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import raiwidgets\n",
        "print(dir(raiwidgets))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXjHKSePZ0xh",
        "outputId": "be4c9d03-3634-4c62-eaa4-e87b8b18e971"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ErrorAnalysisDashboard', 'ExplanationDashboard', 'FairnessDashboard', 'ModelAnalysisDashboard', 'ModelPerformanceDashboard', 'ResponsibleAIDashboard', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', 'constants', 'dashboard', 'error_analysis_constants', 'error_analysis_dashboard', 'error_analysis_dashboard_input', 'error_handling', 'explanation_constants', 'explanation_dashboard', 'explanation_dashboard_input', 'fairness_dashboard', 'fairness_metric_calculation', 'interfaces', 'model_analysis_dashboard', 'model_performance_dashboard', 'responsibleai_dashboard', 'responsibleai_dashboard_input', 'version']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "pd.set_option('display.max_columns',50)\n",
        "pd.set_option('display.max_rows',50)\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "\n",
        "import lightgbm as lgb\n",
        "import optuna\n",
        "\n",
        "from responsibleai import RAIInsights\n",
        "from raiwidgets import ResponsibleAIDashboard\n",
        "# from raiwidgets.cohort import Cohort, CohortFilter, CohortFilterMethods\n",
        "import shap\n",
        "\n",
        "from IPython.display import Markdown, display\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "u3fF3wE-YZLo"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-2. Target ìœ ì˜ ë³€ìˆ˜ ì œê±°"
      ],
      "metadata": {
        "id": "qIcgNXKtMZIu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "_Target ë³€ìˆ˜ì™€ ìƒê´€ê´€ê³„ê°€ ë†’ì€ feature 2ê°œë¥¼ ì œê±°í•œë‹¤_"
      ],
      "metadata": {
        "id": "pOgQdsZNMacJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_orig['readmitted'].value_counts()"
      ],
      "metadata": {
        "id": "dDkrijUbYjt3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f3c747b-ca6f-46b8-c8b9-d235096f3f58"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NO     54864\n",
              ">30    35545\n",
              "<30    11357\n",
              "Name: readmitted, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_orig['readmit_binary'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d17FmjKJMe3E",
        "outputId": "0b8b6023-1b3b-4195-cc68-e60a258bf3f2"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    54864\n",
              "1    46902\n",
              "Name: readmit_binary, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "_Actual Target_"
      ],
      "metadata": {
        "id": "nHqNcVKdMmfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfO67TjcMls8",
        "outputId": "6375addb-7cad-44b4-d378-88ecfeda9fbd"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    90409\n",
              "1    11357\n",
              "Name: readmit_30_days, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "ìœ„ì—ì„œ ë³´ë©´ ì•Œ ìˆ˜ ìˆë“¯ì´ 2ê°œì˜ ë³€ìˆ˜ëŠ” yì™€ ì§ì ‘ì ìœ¼ë¡œ ìƒê´€ê´€ê³„ê°€ ë³´ì´ë¯€ë¡œ í•™ìŠµ Featureì— ì‚¬ìš©í•  ìˆ˜ ì—†ë‹¤.\n",
        "yì™€ ì§ì ‘ì ìœ¼ë¡œ ìƒê´€ê´€ê³„ê°€ ìˆëŠ” ë³€ìˆ˜ë¥¼ featureë¡œ ì‚¬ìš©í•  ê²½ìš°, ëª¨ë“  ê²½ìš°ì— 100%ì— ê°€ê¹Œìš´ ì •í™•ë„ë¥¼ ë³´ì´ê²Œ ë˜ë©°\n",
        "ì´ëŠ” ì •ë‹µì„ ë³´ì—¬ì£¼ëŠ” ê²ƒê³¼ ê°™ê¸° ë•Œë¬¸ì— ì •ìƒì ì¸ëª¨ë¸ì´ë¼ê³  í•  ìˆ˜ ì—†ë‹¤.\n",
        "```"
      ],
      "metadata": {
        "id": "n49cdCbGMrce"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-3. ì¸ì½”ë”©ì„ í†µí•œ ë°ì´í„° ì¤€ë¹„"
      ],
      "metadata": {
        "id": "INt-NlTuM1ba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ì¸ì½”ë”©ì„ ìœ„í•œ í•¨ìˆ˜ ì •ì˜\n",
        "def transform_data(X_):\n",
        "    #Remove alternative target columns\n",
        "    X = X_.drop(['readmitted', 'readmit_binary'], axis=1)\n",
        "\n",
        "    #Binary encode boolean columns\n",
        "    bool_cols_l = X.select_dtypes(include=[\"category\"]).columns.tolist()\n",
        "    X[bool_cols_l] = X[bool_cols_l].astype(str).replace({\"True\":1, \"False\":0})\n",
        "    print(bool_cols_l)\n",
        "\n",
        "    #One hot encode categorical columns\n",
        "    cat_cols_l = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
        "    print(cat_cols_l)\n",
        "    ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "    ohe_np = ohe.fit_transform(X[cat_cols_l].astype(\"category\"))\n",
        "    X[ohe.get_feature_names_out(cat_cols_l)] = ohe_np.astype(int)\n",
        "\n",
        "    #Drop original categorical columns\n",
        "    X.drop(cat_cols_l, axis=1, inplace=True)\n",
        "\n",
        "    return X"
      ],
      "metadata": {
        "id": "B1-bdnBMMpOU"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ".\n",
        "\n",
        "ì´ í•¨ìˆ˜ëŠ” ë°ì´í„°ë¥¼ ë³€í™˜í•˜ê¸° ìœ„í•œ ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:\n",
        "\n",
        "1. **ëŒ€ìƒ ë³€ìˆ˜ ì œê±°**: `readmitted`ì™€ `readmit_binary`ë¼ëŠ” ë‘ ê°œì˜ ì—´ì„ ì œê±°í•©ë‹ˆë‹¤. ì´ ë‘ ì—´ì€ ëŒ€ìƒ ë³€ìˆ˜ë¡œ ì¶”ì •ë©ë‹ˆë‹¤.\n",
        "2. **ì´ì§„ ì¸ì½”ë”©**: ì¹´í…Œê³ ë¦¬í˜• ë°ì´í„°ë¥¼ ê°–ëŠ” ëª¨ë“  ì—´ì„ ì„ íƒí•˜ê³ , ì´ë“¤ì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•œ í›„ \"True\"ëŠ” 1ë¡œ, \"False\"ëŠ” 0ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
        "3. **ì›-í•« ì¸ì½”ë”©**: ë¬¸ìì—´ ë°ì´í„°ë¥¼ ê°–ëŠ” ëª¨ë“  ì—´ì„ ì„ íƒí•˜ê³ , ì´ë“¤ì„ ì›-í•« ì¸ì½”ë”©í•©ë‹ˆë‹¤. ì´ë ‡ê²Œ ìƒì„±ëœ ìƒˆë¡œìš´ ì—´ë“¤ì€ ì›ë³¸ ë°ì´í„°í”„ë ˆì„ì— ì¶”ê°€ë©ë‹ˆë‹¤.\n",
        "4. **ì›ë³¸ ë²”ì£¼í˜• ì—´ ì œê±°**: ì›-í•« ì¸ì½”ë”© í›„ ì›ë³¸ì˜ ë²”ì£¼í˜• ì—´ë“¤ì€ ë” ì´ìƒ í•„ìš”í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì œê±°í•©ë‹ˆë‹¤.\n",
        "\n",
        "ëª‡ ê°€ì§€ ì£¼ì˜ì  ë° ì œì•ˆì‚¬í•­:\n",
        "- `X_`ëŠ” ì›ë³¸ ë°ì´í„°ì´ë©°, í•¨ìˆ˜ ë‚´ì—ì„œ ì´ë¥¼ ë³µì‚¬í•˜ì—¬ `X`ì— ì €ì¥í•˜ì—¬ ì‘ì—…ì„ ì§„í–‰í•©ë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ì›ë³¸ ë°ì´í„°ì— ì˜í–¥ì„ ì£¼ì§€ ì•Šê³  ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "- `bool_cols_l`ì™€ `cat_cols_l` ë‘ ë¦¬ìŠ¤íŠ¸ë¥¼ ì¶œë ¥í•˜ëŠ” ì½”ë“œê°€ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ë””ë²„ê¹… ë˜ëŠ” í™•ì¸ì„ ìœ„í•œ ê²ƒìœ¼ë¡œ ë³´ì´ë©°, í•„ìš”í•˜ì§€ ì•Šì„ ê²½ìš° ì œê±°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "- ì›-í•« ì¸ì½”ë”©ì„ í•  ë•Œ `handle_unknown='ignore'` ì˜µì…˜ì„ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ í›ˆë ¨ ì‹œì—ëŠ” ë‚˜íƒ€ë‚˜ì§€ ì•Šì•˜ì§€ë§Œ ì‹¤ì œ ì‚¬ìš© ì‹œì— ë‚˜íƒ€ë‚  ìˆ˜ ìˆëŠ” ìƒˆë¡œìš´ ë²”ì£¼ê°’ì„ ë¬´ì‹œí•˜ê² ë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤.\n",
        "\n",
        "ì „ë°˜ì ìœ¼ë¡œ ì½”ë“œëŠ” ê¹”ë”í•˜ê²Œ ì‘ì„±ë˜ì—ˆìœ¼ë©°, ì£¼ì–´ì§„ ì‘ì—…ì„ ì˜ ìˆ˜í–‰í•  ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤. íŠ¹ë³„íˆ ìˆ˜ì •í•  ë¶€ë¶„ì€ ë³´ì´ì§€ ì•ŠìŠµë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "0yfJ28TzPuHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = transform_data(X_orig)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O19LX_7SN2_z",
        "outputId": "74e2acfd-f969-4210-c4b9-db62692daac1"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['medicare', 'medicaid', 'had_emergency', 'had_inpatient_days', 'had_outpatient_days']\n",
            "['race', 'gender', 'age', 'discharge_disposition_id', 'admission_source_id', 'medical_specialty', 'primary_diagnosis', 'max_glu_serum', 'A1Cresult', 'insulin', 'change', 'diabetesMed']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Number of Features : {len(X_orig.columns)}')\n",
        "print(X_orig.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xLrWai3OKVi",
        "outputId": "9220e2a3-a5f1-4772-a226-555d37788822"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Features : 24\n",
            "Index(['race', 'gender', 'age', 'discharge_disposition_id',\n",
            "       'admission_source_id', 'time_in_hospital', 'medical_specialty',\n",
            "       'num_lab_procedures', 'num_procedures', 'num_medications',\n",
            "       'primary_diagnosis', 'number_diagnoses', 'max_glu_serum', 'A1Cresult',\n",
            "       'insulin', 'change', 'diabetesMed', 'medicare', 'medicaid',\n",
            "       'had_emergency', 'had_inpatient_days', 'had_outpatient_days',\n",
            "       'readmitted', 'readmit_binary'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Number of Features(After processing) :  {len(X.columns)}')\n",
        "print(X.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2h1x7HJgOtxZ",
        "outputId": "db3bbdef-6342-4f6c-9ef8-86ee56fb659b"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Features(After processing) :  54\n",
            "Index(['time_in_hospital', 'num_lab_procedures', 'num_procedures',\n",
            "       'num_medications', 'number_diagnoses', 'medicare', 'medicaid',\n",
            "       'had_emergency', 'had_inpatient_days', 'had_outpatient_days',\n",
            "       'race_AfricanAmerican', 'race_Asian', 'race_Caucasian', 'race_Hispanic',\n",
            "       'race_Other', 'race_Unknown', 'gender_Female', 'gender_Male',\n",
            "       'gender_Unknown/Invalid', 'age_30 years or younger', 'age_30-60 years',\n",
            "       'age_Over 60 years', 'discharge_disposition_id_Discharged to Home',\n",
            "       'discharge_disposition_id_Other', 'admission_source_id_Emergency',\n",
            "       'admission_source_id_Other', 'admission_source_id_Referral',\n",
            "       'medical_specialty_Cardiology', 'medical_specialty_Emergency/Trauma',\n",
            "       'medical_specialty_Family/GeneralPractice',\n",
            "       'medical_specialty_InternalMedicine', 'medical_specialty_Missing',\n",
            "       'medical_specialty_Other', 'primary_diagnosis_Diabetes',\n",
            "       'primary_diagnosis_Genitourinary Issues',\n",
            "       'primary_diagnosis_Musculoskeletal Issues', 'primary_diagnosis_Other',\n",
            "       'primary_diagnosis_Respiratory Issues', 'max_glu_serum_>200',\n",
            "       'max_glu_serum_>300', 'max_glu_serum_None', 'max_glu_serum_Norm',\n",
            "       'A1Cresult_>7', 'A1Cresult_>8', 'A1Cresult_None', 'A1Cresult_Norm',\n",
            "       'insulin_Down', 'insulin_No', 'insulin_Steady', 'insulin_Up',\n",
            "       'change_Ch', 'change_No', 'diabetesMed_No', 'diabetesMed_Yes'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-4. ë°ì´í„°ì…‹ ë¶„í• "
      ],
      "metadata": {
        "id": "JkF5etxuPFjm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rand = 42\n",
        "os.environ['PYTHONHASHSEED']=str(rand)\n",
        "np.random.seed(rand)"
      ],
      "metadata": {
        "id": "3k_mvM0xO8iB"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes_l = ['NO or > 30 days', '< 30 days']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=rand)\n",
        "X_orig_test = X_orig.loc[X_test.index]"
      ],
      "metadata": {
        "id": "-W2WYVQ-PMsZ"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "id": "bvxTECeyPT_a",
        "outputId": "dfe062d3-7568-4bb6-c772-74306dcdfb41"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       time_in_hospital  num_lab_procedures  num_procedures  num_medications  \\\n",
              "15992                10                  45               0               14   \n",
              "10606                 4                  39               0               11   \n",
              "64779                 1                  37               2                7   \n",
              "83257                 8                  45               0               24   \n",
              "4204                 12                  51               0               12   \n",
              "...                 ...                 ...             ...              ...   \n",
              "6265                  2                  35               0               12   \n",
              "54886                 5                  63               2               23   \n",
              "76820                 3                  55               1               33   \n",
              "860                  12                  77               2               21   \n",
              "15795                 1                   2               5               17   \n",
              "\n",
              "       number_diagnoses  medicare  medicaid  had_emergency  \\\n",
              "15992                 9         0         0              0   \n",
              "10606                 5         0         0              0   \n",
              "64779                 5         1         0              0   \n",
              "83257                 9         1         0              1   \n",
              "4204                  7         0         0              0   \n",
              "...                 ...       ...       ...            ...   \n",
              "6265                  9         0         0              0   \n",
              "54886                 9         0         0              0   \n",
              "76820                 9         1         0              0   \n",
              "860                   9         0         0              0   \n",
              "15795                 7         0         0              0   \n",
              "\n",
              "       had_inpatient_days  had_outpatient_days  race_AfricanAmerican  \\\n",
              "15992                   0                    0                     0   \n",
              "10606                   0                    0                     0   \n",
              "64779                   0                    0                     0   \n",
              "83257                   1                    0                     0   \n",
              "4204                    0                    0                     0   \n",
              "...                   ...                  ...                   ...   \n",
              "6265                    0                    0                     0   \n",
              "54886                   0                    0                     0   \n",
              "76820                   0                    0                     0   \n",
              "860                     0                    0                     0   \n",
              "15795                   0                    0                     0   \n",
              "\n",
              "       race_Asian  race_Caucasian  race_Hispanic  race_Other  race_Unknown  \\\n",
              "15992           0               1              0           0             0   \n",
              "10606           0               1              0           0             0   \n",
              "64779           0               1              0           0             0   \n",
              "83257           0               1              0           0             0   \n",
              "4204            0               1              0           0             0   \n",
              "...           ...             ...            ...         ...           ...   \n",
              "6265            0               1              0           0             0   \n",
              "54886           0               1              0           0             0   \n",
              "76820           0               1              0           0             0   \n",
              "860             0               1              0           0             0   \n",
              "15795           0               1              0           0             0   \n",
              "\n",
              "       gender_Female  gender_Male  gender_Unknown/Invalid  \\\n",
              "15992              1            0                       0   \n",
              "10606              1            0                       0   \n",
              "64779              1            0                       0   \n",
              "83257              1            0                       0   \n",
              "4204               1            0                       0   \n",
              "...              ...          ...                     ...   \n",
              "6265               0            1                       0   \n",
              "54886              0            1                       0   \n",
              "76820              0            1                       0   \n",
              "860                1            0                       0   \n",
              "15795              1            0                       0   \n",
              "\n",
              "       age_30 years or younger  age_30-60 years  age_Over 60 years  \\\n",
              "15992                        0                0                  1   \n",
              "10606                        0                0                  1   \n",
              "64779                        0                0                  1   \n",
              "83257                        0                0                  1   \n",
              "4204                         0                0                  1   \n",
              "...                        ...              ...                ...   \n",
              "6265                         0                0                  1   \n",
              "54886                        0                0                  1   \n",
              "76820                        0                0                  1   \n",
              "860                          0                0                  1   \n",
              "15795                        0                1                  0   \n",
              "\n",
              "       discharge_disposition_id_Discharged to Home  \\\n",
              "15992                                            1   \n",
              "10606                                            1   \n",
              "64779                                            1   \n",
              "83257                                            0   \n",
              "4204                                             0   \n",
              "...                                            ...   \n",
              "6265                                             1   \n",
              "54886                                            1   \n",
              "76820                                            0   \n",
              "860                                              0   \n",
              "15795                                            1   \n",
              "\n",
              "       discharge_disposition_id_Other  admission_source_id_Emergency  ...  \\\n",
              "15992                               0                              1  ...   \n",
              "10606                               0                              1  ...   \n",
              "64779                               0                              0  ...   \n",
              "83257                               1                              1  ...   \n",
              "4204                                1                              1  ...   \n",
              "...                               ...                            ...  ...   \n",
              "6265                                0                              1  ...   \n",
              "54886                               0                              0  ...   \n",
              "76820                               1                              0  ...   \n",
              "860                                 1                              1  ...   \n",
              "15795                               0                              0  ...   \n",
              "\n",
              "       medical_specialty_Family/GeneralPractice  \\\n",
              "15992                                         1   \n",
              "10606                                         0   \n",
              "64779                                         0   \n",
              "83257                                         0   \n",
              "4204                                          0   \n",
              "...                                         ...   \n",
              "6265                                          0   \n",
              "54886                                         0   \n",
              "76820                                         0   \n",
              "860                                           1   \n",
              "15795                                         0   \n",
              "\n",
              "       medical_specialty_InternalMedicine  medical_specialty_Missing  \\\n",
              "15992                                   0                          0   \n",
              "10606                                   0                          0   \n",
              "64779                                   1                          0   \n",
              "83257                                   0                          1   \n",
              "4204                                    0                          1   \n",
              "...                                   ...                        ...   \n",
              "6265                                    0                          0   \n",
              "54886                                   0                          1   \n",
              "76820                                   0                          0   \n",
              "860                                     0                          0   \n",
              "15795                                   0                          0   \n",
              "\n",
              "       medical_specialty_Other  primary_diagnosis_Diabetes  \\\n",
              "15992                        0                           0   \n",
              "10606                        1                           0   \n",
              "64779                        0                           0   \n",
              "83257                        0                           0   \n",
              "4204                         0                           0   \n",
              "...                        ...                         ...   \n",
              "6265                         0                           0   \n",
              "54886                        0                           0   \n",
              "76820                        1                           0   \n",
              "860                          0                           0   \n",
              "15795                        0                           0   \n",
              "\n",
              "       primary_diagnosis_Genitourinary Issues  \\\n",
              "15992                                       0   \n",
              "10606                                       0   \n",
              "64779                                       0   \n",
              "83257                                       0   \n",
              "4204                                        0   \n",
              "...                                       ...   \n",
              "6265                                        0   \n",
              "54886                                       0   \n",
              "76820                                       0   \n",
              "860                                         0   \n",
              "15795                                       0   \n",
              "\n",
              "       primary_diagnosis_Musculoskeletal Issues  primary_diagnosis_Other  \\\n",
              "15992                                         0                        1   \n",
              "10606                                         0                        1   \n",
              "64779                                         0                        1   \n",
              "83257                                         0                        1   \n",
              "4204                                          0                        1   \n",
              "...                                         ...                      ...   \n",
              "6265                                          0                        0   \n",
              "54886                                         0                        1   \n",
              "76820                                         1                        0   \n",
              "860                                           0                        0   \n",
              "15795                                         0                        1   \n",
              "\n",
              "       primary_diagnosis_Respiratory Issues  max_glu_serum_>200  \\\n",
              "15992                                     0                   0   \n",
              "10606                                     0                   0   \n",
              "64779                                     0                   0   \n",
              "83257                                     0                   0   \n",
              "4204                                      0                   0   \n",
              "...                                     ...                 ...   \n",
              "6265                                      1                   0   \n",
              "54886                                     0                   0   \n",
              "76820                                     0                   0   \n",
              "860                                       1                   0   \n",
              "15795                                     0                   0   \n",
              "\n",
              "       max_glu_serum_>300  max_glu_serum_None  max_glu_serum_Norm  \\\n",
              "15992                   0                   1                   0   \n",
              "10606                   0                   1                   0   \n",
              "64779                   0                   1                   0   \n",
              "83257                   0                   1                   0   \n",
              "4204                    0                   1                   0   \n",
              "...                   ...                 ...                 ...   \n",
              "6265                    0                   1                   0   \n",
              "54886                   0                   1                   0   \n",
              "76820                   0                   1                   0   \n",
              "860                     0                   1                   0   \n",
              "15795                   0                   1                   0   \n",
              "\n",
              "       A1Cresult_>7  A1Cresult_>8  A1Cresult_None  A1Cresult_Norm  \\\n",
              "15992             0             0               1               0   \n",
              "10606             0             0               1               0   \n",
              "64779             0             0               1               0   \n",
              "83257             0             0               1               0   \n",
              "4204              0             0               1               0   \n",
              "...             ...           ...             ...             ...   \n",
              "6265              0             0               1               0   \n",
              "54886             0             0               1               0   \n",
              "76820             0             0               1               0   \n",
              "860               0             0               1               0   \n",
              "15795             0             0               1               0   \n",
              "\n",
              "       insulin_Down  insulin_No  insulin_Steady  insulin_Up  change_Ch  \\\n",
              "15992             0           1               0           0          1   \n",
              "10606             0           1               0           0          0   \n",
              "64779             0           1               0           0          0   \n",
              "83257             1           0               0           0          1   \n",
              "4204              0           1               0           0          0   \n",
              "...             ...         ...             ...         ...        ...   \n",
              "6265              0           1               0           0          0   \n",
              "54886             0           0               0           1          1   \n",
              "76820             1           0               0           0          1   \n",
              "860               1           0               0           0          1   \n",
              "15795             0           0               1           0          1   \n",
              "\n",
              "       change_No  diabetesMed_No  diabetesMed_Yes  \n",
              "15992          0               0                1  \n",
              "10606          1               1                0  \n",
              "64779          1               1                0  \n",
              "83257          0               0                1  \n",
              "4204           1               0                1  \n",
              "...          ...             ...              ...  \n",
              "6265           1               1                0  \n",
              "54886          0               0                1  \n",
              "76820          0               0                1  \n",
              "860            0               0                1  \n",
              "15795          0               0                1  \n",
              "\n",
              "[71236 rows x 54 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5b26d894-a741-4091-8d4f-872da5599fa9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>time_in_hospital</th>\n",
              "      <th>num_lab_procedures</th>\n",
              "      <th>num_procedures</th>\n",
              "      <th>num_medications</th>\n",
              "      <th>number_diagnoses</th>\n",
              "      <th>medicare</th>\n",
              "      <th>medicaid</th>\n",
              "      <th>had_emergency</th>\n",
              "      <th>had_inpatient_days</th>\n",
              "      <th>had_outpatient_days</th>\n",
              "      <th>race_AfricanAmerican</th>\n",
              "      <th>race_Asian</th>\n",
              "      <th>race_Caucasian</th>\n",
              "      <th>race_Hispanic</th>\n",
              "      <th>race_Other</th>\n",
              "      <th>race_Unknown</th>\n",
              "      <th>gender_Female</th>\n",
              "      <th>gender_Male</th>\n",
              "      <th>gender_Unknown/Invalid</th>\n",
              "      <th>age_30 years or younger</th>\n",
              "      <th>age_30-60 years</th>\n",
              "      <th>age_Over 60 years</th>\n",
              "      <th>discharge_disposition_id_Discharged to Home</th>\n",
              "      <th>discharge_disposition_id_Other</th>\n",
              "      <th>admission_source_id_Emergency</th>\n",
              "      <th>...</th>\n",
              "      <th>medical_specialty_Family/GeneralPractice</th>\n",
              "      <th>medical_specialty_InternalMedicine</th>\n",
              "      <th>medical_specialty_Missing</th>\n",
              "      <th>medical_specialty_Other</th>\n",
              "      <th>primary_diagnosis_Diabetes</th>\n",
              "      <th>primary_diagnosis_Genitourinary Issues</th>\n",
              "      <th>primary_diagnosis_Musculoskeletal Issues</th>\n",
              "      <th>primary_diagnosis_Other</th>\n",
              "      <th>primary_diagnosis_Respiratory Issues</th>\n",
              "      <th>max_glu_serum_&gt;200</th>\n",
              "      <th>max_glu_serum_&gt;300</th>\n",
              "      <th>max_glu_serum_None</th>\n",
              "      <th>max_glu_serum_Norm</th>\n",
              "      <th>A1Cresult_&gt;7</th>\n",
              "      <th>A1Cresult_&gt;8</th>\n",
              "      <th>A1Cresult_None</th>\n",
              "      <th>A1Cresult_Norm</th>\n",
              "      <th>insulin_Down</th>\n",
              "      <th>insulin_No</th>\n",
              "      <th>insulin_Steady</th>\n",
              "      <th>insulin_Up</th>\n",
              "      <th>change_Ch</th>\n",
              "      <th>change_No</th>\n",
              "      <th>diabetesMed_No</th>\n",
              "      <th>diabetesMed_Yes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>15992</th>\n",
              "      <td>10</td>\n",
              "      <td>45</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10606</th>\n",
              "      <td>4</td>\n",
              "      <td>39</td>\n",
              "      <td>0</td>\n",
              "      <td>11</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64779</th>\n",
              "      <td>1</td>\n",
              "      <td>37</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83257</th>\n",
              "      <td>8</td>\n",
              "      <td>45</td>\n",
              "      <td>0</td>\n",
              "      <td>24</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4204</th>\n",
              "      <td>12</td>\n",
              "      <td>51</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6265</th>\n",
              "      <td>2</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54886</th>\n",
              "      <td>5</td>\n",
              "      <td>63</td>\n",
              "      <td>2</td>\n",
              "      <td>23</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76820</th>\n",
              "      <td>3</td>\n",
              "      <td>55</td>\n",
              "      <td>1</td>\n",
              "      <td>33</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>860</th>\n",
              "      <td>12</td>\n",
              "      <td>77</td>\n",
              "      <td>2</td>\n",
              "      <td>21</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15795</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>17</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>71236 rows Ã— 54 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5b26d894-a741-4091-8d4f-872da5599fa9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5b26d894-a741-4091-8d4f-872da5599fa9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5b26d894-a741-4091-8d4f-872da5599fa9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b25856f7-99c6-430a-8460-3c99b46746a5\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b25856f7-99c6-430a-8460-3c99b46746a5')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b25856f7-99c6-430a-8460-3c99b46746a5 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "id": "Vw31OVzgPd55",
        "outputId": "c5282202-f4ba-414c-911c-001c13674454"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       time_in_hospital  num_lab_procedures  num_procedures  num_medications  \\\n",
              "35956                11                  68               0               20   \n",
              "60927                 1                  20               0                7   \n",
              "79920                 4                  21               3               23   \n",
              "50078                12                  28               0               19   \n",
              "44080                 1                  21               0                6   \n",
              "...                 ...                 ...             ...              ...   \n",
              "19901                 1                  45               1                5   \n",
              "9561                  4                  58               0               10   \n",
              "47211                 2                  68               6               16   \n",
              "25232                 6                  61               2                7   \n",
              "22212                12                  76               6               25   \n",
              "\n",
              "       number_diagnoses  medicare  medicaid  had_emergency  \\\n",
              "35956                 5         0         0              0   \n",
              "60927                 8         0         0              0   \n",
              "79920                 7         0         0              0   \n",
              "50078                 7         0         0              0   \n",
              "44080                 7         0         0              0   \n",
              "...                 ...       ...       ...            ...   \n",
              "19901                 4         0         0              0   \n",
              "9561                  9         0         0              0   \n",
              "47211                 9         1         0              0   \n",
              "25232                 5         1         0              0   \n",
              "22212                 9         0         0              0   \n",
              "\n",
              "       had_inpatient_days  had_outpatient_days  race_AfricanAmerican  \\\n",
              "35956                   0                    0                     0   \n",
              "60927                   0                    0                     0   \n",
              "79920                   1                    1                     0   \n",
              "50078                   1                    0                     0   \n",
              "44080                   0                    0                     1   \n",
              "...                   ...                  ...                   ...   \n",
              "19901                   0                    0                     0   \n",
              "9561                    1                    0                     0   \n",
              "47211                   0                    0                     0   \n",
              "25232                   0                    0                     0   \n",
              "22212                   1                    0                     1   \n",
              "\n",
              "       race_Asian  race_Caucasian  race_Hispanic  race_Other  race_Unknown  \\\n",
              "35956           0               1              0           0             0   \n",
              "60927           0               1              0           0             0   \n",
              "79920           0               1              0           0             0   \n",
              "50078           0               1              0           0             0   \n",
              "44080           0               0              0           0             0   \n",
              "...           ...             ...            ...         ...           ...   \n",
              "19901           0               0              1           0             0   \n",
              "9561            0               1              0           0             0   \n",
              "47211           0               1              0           0             0   \n",
              "25232           0               1              0           0             0   \n",
              "22212           0               0              0           0             0   \n",
              "\n",
              "       gender_Female  gender_Male  gender_Unknown/Invalid  \\\n",
              "35956              1            0                       0   \n",
              "60927              0            1                       0   \n",
              "79920              1            0                       0   \n",
              "50078              0            1                       0   \n",
              "44080              1            0                       0   \n",
              "...              ...          ...                     ...   \n",
              "19901              1            0                       0   \n",
              "9561               1            0                       0   \n",
              "47211              0            1                       0   \n",
              "25232              0            1                       0   \n",
              "22212              0            1                       0   \n",
              "\n",
              "       age_30 years or younger  age_30-60 years  age_Over 60 years  \\\n",
              "35956                        0                0                  1   \n",
              "60927                        0                1                  0   \n",
              "79920                        0                0                  1   \n",
              "50078                        0                1                  0   \n",
              "44080                        0                0                  1   \n",
              "...                        ...              ...                ...   \n",
              "19901                        0                1                  0   \n",
              "9561                         0                0                  1   \n",
              "47211                        0                1                  0   \n",
              "25232                        0                0                  1   \n",
              "22212                        0                0                  1   \n",
              "\n",
              "       discharge_disposition_id_Discharged to Home  \\\n",
              "35956                                            1   \n",
              "60927                                            1   \n",
              "79920                                            0   \n",
              "50078                                            1   \n",
              "44080                                            0   \n",
              "...                                            ...   \n",
              "19901                                            1   \n",
              "9561                                             1   \n",
              "47211                                            1   \n",
              "25232                                            1   \n",
              "22212                                            0   \n",
              "\n",
              "       discharge_disposition_id_Other  admission_source_id_Emergency  ...  \\\n",
              "35956                               0                              0  ...   \n",
              "60927                               0                              0  ...   \n",
              "79920                               1                              0  ...   \n",
              "50078                               0                              0  ...   \n",
              "44080                               1                              1  ...   \n",
              "...                               ...                            ...  ...   \n",
              "19901                               0                              1  ...   \n",
              "9561                                0                              1  ...   \n",
              "47211                               0                              1  ...   \n",
              "25232                               0                              0  ...   \n",
              "22212                               1                              1  ...   \n",
              "\n",
              "       medical_specialty_Family/GeneralPractice  \\\n",
              "35956                                         0   \n",
              "60927                                         0   \n",
              "79920                                         0   \n",
              "50078                                         0   \n",
              "44080                                         0   \n",
              "...                                         ...   \n",
              "19901                                         0   \n",
              "9561                                          0   \n",
              "47211                                         0   \n",
              "25232                                         1   \n",
              "22212                                         0   \n",
              "\n",
              "       medical_specialty_InternalMedicine  medical_specialty_Missing  \\\n",
              "35956                                   1                          0   \n",
              "60927                                   0                          1   \n",
              "79920                                   0                          1   \n",
              "50078                                   0                          0   \n",
              "44080                                   0                          1   \n",
              "...                                   ...                        ...   \n",
              "19901                                   0                          0   \n",
              "9561                                    0                          0   \n",
              "47211                                   0                          0   \n",
              "25232                                   0                          0   \n",
              "22212                                   0                          1   \n",
              "\n",
              "       medical_specialty_Other  primary_diagnosis_Diabetes  \\\n",
              "35956                        0                           1   \n",
              "60927                        0                           0   \n",
              "79920                        0                           0   \n",
              "50078                        1                           0   \n",
              "44080                        0                           0   \n",
              "...                        ...                         ...   \n",
              "19901                        1                           0   \n",
              "9561                         0                           0   \n",
              "47211                        0                           0   \n",
              "25232                        0                           0   \n",
              "22212                        0                           0   \n",
              "\n",
              "       primary_diagnosis_Genitourinary Issues  \\\n",
              "35956                                       0   \n",
              "60927                                       0   \n",
              "79920                                       0   \n",
              "50078                                       0   \n",
              "44080                                       0   \n",
              "...                                       ...   \n",
              "19901                                       0   \n",
              "9561                                        0   \n",
              "47211                                       0   \n",
              "25232                                       0   \n",
              "22212                                       0   \n",
              "\n",
              "       primary_diagnosis_Musculoskeletal Issues  primary_diagnosis_Other  \\\n",
              "35956                                         0                        0   \n",
              "60927                                         0                        1   \n",
              "79920                                         1                        0   \n",
              "50078                                         0                        0   \n",
              "44080                                         0                        1   \n",
              "...                                         ...                      ...   \n",
              "19901                                         0                        1   \n",
              "9561                                          0                        1   \n",
              "47211                                         0                        1   \n",
              "25232                                         0                        0   \n",
              "22212                                         0                        1   \n",
              "\n",
              "       primary_diagnosis_Respiratory Issues  max_glu_serum_>200  \\\n",
              "35956                                     0                   0   \n",
              "60927                                     0                   0   \n",
              "79920                                     0                   0   \n",
              "50078                                     1                   0   \n",
              "44080                                     0                   0   \n",
              "...                                     ...                 ...   \n",
              "19901                                     0                   0   \n",
              "9561                                      0                   0   \n",
              "47211                                     0                   0   \n",
              "25232                                     1                   0   \n",
              "22212                                     0                   0   \n",
              "\n",
              "       max_glu_serum_>300  max_glu_serum_None  max_glu_serum_Norm  \\\n",
              "35956                   0                   1                   0   \n",
              "60927                   0                   1                   0   \n",
              "79920                   0                   1                   0   \n",
              "50078                   0                   1                   0   \n",
              "44080                   0                   1                   0   \n",
              "...                   ...                 ...                 ...   \n",
              "19901                   0                   1                   0   \n",
              "9561                    0                   1                   0   \n",
              "47211                   0                   1                   0   \n",
              "25232                   0                   1                   0   \n",
              "22212                   0                   1                   0   \n",
              "\n",
              "       A1Cresult_>7  A1Cresult_>8  A1Cresult_None  A1Cresult_Norm  \\\n",
              "35956             0             0               1               0   \n",
              "60927             0             0               1               0   \n",
              "79920             0             0               1               0   \n",
              "50078             0             0               1               0   \n",
              "44080             0             0               1               0   \n",
              "...             ...           ...             ...             ...   \n",
              "19901             0             0               1               0   \n",
              "9561              0             0               1               0   \n",
              "47211             0             0               1               0   \n",
              "25232             0             0               1               0   \n",
              "22212             0             0               1               0   \n",
              "\n",
              "       insulin_Down  insulin_No  insulin_Steady  insulin_Up  change_Ch  \\\n",
              "35956             0           0               1           0          0   \n",
              "60927             0           1               0           0          0   \n",
              "79920             0           1               0           0          0   \n",
              "50078             0           1               0           0          0   \n",
              "44080             0           1               0           0          0   \n",
              "...             ...         ...             ...         ...        ...   \n",
              "19901             0           1               0           0          0   \n",
              "9561              0           0               1           0          0   \n",
              "47211             0           1               0           0          0   \n",
              "25232             0           1               0           0          0   \n",
              "22212             0           1               0           0          0   \n",
              "\n",
              "       change_No  diabetesMed_No  diabetesMed_Yes  \n",
              "35956          1               0                1  \n",
              "60927          1               0                1  \n",
              "79920          1               0                1  \n",
              "50078          1               0                1  \n",
              "44080          1               0                1  \n",
              "...          ...             ...              ...  \n",
              "19901          1               1                0  \n",
              "9561           1               0                1  \n",
              "47211          1               1                0  \n",
              "25232          1               1                0  \n",
              "22212          1               1                0  \n",
              "\n",
              "[30530 rows x 54 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-13509771-3d33-4529-8451-090019dc2405\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>time_in_hospital</th>\n",
              "      <th>num_lab_procedures</th>\n",
              "      <th>num_procedures</th>\n",
              "      <th>num_medications</th>\n",
              "      <th>number_diagnoses</th>\n",
              "      <th>medicare</th>\n",
              "      <th>medicaid</th>\n",
              "      <th>had_emergency</th>\n",
              "      <th>had_inpatient_days</th>\n",
              "      <th>had_outpatient_days</th>\n",
              "      <th>race_AfricanAmerican</th>\n",
              "      <th>race_Asian</th>\n",
              "      <th>race_Caucasian</th>\n",
              "      <th>race_Hispanic</th>\n",
              "      <th>race_Other</th>\n",
              "      <th>race_Unknown</th>\n",
              "      <th>gender_Female</th>\n",
              "      <th>gender_Male</th>\n",
              "      <th>gender_Unknown/Invalid</th>\n",
              "      <th>age_30 years or younger</th>\n",
              "      <th>age_30-60 years</th>\n",
              "      <th>age_Over 60 years</th>\n",
              "      <th>discharge_disposition_id_Discharged to Home</th>\n",
              "      <th>discharge_disposition_id_Other</th>\n",
              "      <th>admission_source_id_Emergency</th>\n",
              "      <th>...</th>\n",
              "      <th>medical_specialty_Family/GeneralPractice</th>\n",
              "      <th>medical_specialty_InternalMedicine</th>\n",
              "      <th>medical_specialty_Missing</th>\n",
              "      <th>medical_specialty_Other</th>\n",
              "      <th>primary_diagnosis_Diabetes</th>\n",
              "      <th>primary_diagnosis_Genitourinary Issues</th>\n",
              "      <th>primary_diagnosis_Musculoskeletal Issues</th>\n",
              "      <th>primary_diagnosis_Other</th>\n",
              "      <th>primary_diagnosis_Respiratory Issues</th>\n",
              "      <th>max_glu_serum_&gt;200</th>\n",
              "      <th>max_glu_serum_&gt;300</th>\n",
              "      <th>max_glu_serum_None</th>\n",
              "      <th>max_glu_serum_Norm</th>\n",
              "      <th>A1Cresult_&gt;7</th>\n",
              "      <th>A1Cresult_&gt;8</th>\n",
              "      <th>A1Cresult_None</th>\n",
              "      <th>A1Cresult_Norm</th>\n",
              "      <th>insulin_Down</th>\n",
              "      <th>insulin_No</th>\n",
              "      <th>insulin_Steady</th>\n",
              "      <th>insulin_Up</th>\n",
              "      <th>change_Ch</th>\n",
              "      <th>change_No</th>\n",
              "      <th>diabetesMed_No</th>\n",
              "      <th>diabetesMed_Yes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>35956</th>\n",
              "      <td>11</td>\n",
              "      <td>68</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60927</th>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79920</th>\n",
              "      <td>4</td>\n",
              "      <td>21</td>\n",
              "      <td>3</td>\n",
              "      <td>23</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50078</th>\n",
              "      <td>12</td>\n",
              "      <td>28</td>\n",
              "      <td>0</td>\n",
              "      <td>19</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44080</th>\n",
              "      <td>1</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19901</th>\n",
              "      <td>1</td>\n",
              "      <td>45</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9561</th>\n",
              "      <td>4</td>\n",
              "      <td>58</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47211</th>\n",
              "      <td>2</td>\n",
              "      <td>68</td>\n",
              "      <td>6</td>\n",
              "      <td>16</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25232</th>\n",
              "      <td>6</td>\n",
              "      <td>61</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22212</th>\n",
              "      <td>12</td>\n",
              "      <td>76</td>\n",
              "      <td>6</td>\n",
              "      <td>25</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>30530 rows Ã— 54 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-13509771-3d33-4529-8451-090019dc2405')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-13509771-3d33-4529-8451-090019dc2405 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-13509771-3d33-4529-8451-090019dc2405');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-94858b48-1c14-471a-9b41-b6aebdfbfe45\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-94858b48-1c14-471a-9b41-b6aebdfbfe45')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-94858b48-1c14-471a-9b41-b6aebdfbfe45 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-5. Modeling ë° Evaluation í•¨ìˆ˜ ì •ì˜\n",
        "ëª¨ë¸ì„ ìƒì„±í•˜ë©´ ì„±ëŠ¥ì— ëŒ€í•œ í‰ê°€ë¥¼ ìœ„í•´ evaluation í•¨ìˆ˜ë¥¼ ì •ì˜í•˜ê³  ì‹œì‘í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤.\n",
        "ì—¬ê¸°ì—ì„œëŠ” ì¼ë°˜ì ìœ¼ë¡œ í‰ê°€ì²™ë„ì—ì„œ ë§ì´ ì‚¬ìš©ë˜ëŠ” Accuracy, Precision, Recall, F1 scoreë¥¼ êµ¬í•˜ëŠ” evaluation í•¨ìˆ˜ë¥¼ ì‘ì„±í•˜ì—¬ ì‚¬ìš©í•˜ë„ë¡ í•˜ì.\n",
        "ì´ í•¨ìˆ˜ëŠ” Confusion Matrixë¥¼ Heatmapìœ¼ë¡œ ê·¸ë¦¬ëŠ” ê²ƒë„ í¬í•¨í•œë‹¤.\n"
      ],
      "metadata": {
        "id": "fjplZBoIPgs6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-6. Evaluation í•¨ìˆ˜ ì •ì˜"
      ],
      "metadata": {
        "id": "wKAKebMFPrRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_class_mdl(fitted_model, X_train, X_test, y_train, y_test, plot=True, pct=True, thresh=0.5):\n",
        "    y_train_pred = fitted_model.predict(X_train).squeeze()\n",
        "    if len(np.unique(y_train_pred)) > 2:\n",
        "        y_train_pred = np.where(y_train_pred > thresh, 1, 0)\n",
        "        y_test_prob = fitted_model.predict(X_test).squeeze()\n",
        "        y_test_pred = np.where(y_test_prob > thresh, 1, 0)\n",
        "    else:\n",
        "        y_test_prob = fitted_model.predict_proba(X_test)[:,1]\n",
        "        y_test_pred = np.where(y_test_prob > thresh, 1, 0)\n",
        "    roc_auc_te = metrics.roc_auc_score(y_test, y_test_prob)\n",
        "\n",
        "    cf_matrix = metrics.confusion_matrix(y_test, y_test_pred)\n",
        "    tn, fp, fn, tp = cf_matrix.ravel()\n",
        "    acc_tr = metrics.accuracy_score(y_train, y_train_pred)\n",
        "    acc_te = metrics.accuracy_score(y_test, y_test_pred)\n",
        "    pre_te = metrics.precision_score(y_test, y_test_pred)\n",
        "    rec_te = metrics.recall_score(y_test, y_test_pred)\n",
        "    f1_te = metrics.f1_score(y_test, y_test_pred)\n",
        "    mcc_te = metrics.matthews_corrcoef(y_test, y_test_pred)\n",
        "\n",
        "    if plot:\n",
        "        print(f\"Accuracy_train:  {acc_tr:.4f}\\t\\tAccuracy_test:   {acc_te:.4f}\")\n",
        "        print(f\"Precision_test:  {pre_te:.4f}\\t\\tRecall_test:     {rec_te:.4f}\")\n",
        "        print(f\"ROC-AUC_test:    {roc_auc_te:.4f}\\t\\tF1_test:         {f1_te:.4f}\\t\\tMCC_test: {mcc_te:.4f}\")\n",
        "        plt.figure(figsize=(6, 5))\n",
        "        if pct:\n",
        "            ax = sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True,\\\n",
        "                        fmt='.2%', cmap='Blues', annot_kws={'size':16})\n",
        "        else:\n",
        "            ax = sns.heatmap(cf_matrix, annot=True,\\\n",
        "                        fmt='d',cmap='Blues', annot_kws={'size':16})\n",
        "        ax.set_xlabel('Predicted', fontsize=12)\n",
        "        ax.set_ylabel('Observed', fontsize=12)\n",
        "        plt.show()\n",
        "\n",
        "        return y_train_pred, y_test_prob, y_test_pred\n",
        "    else:\n",
        "        t = cf_matrix.sum()\n",
        "        metrics_dict = {'accuracy_train':acc_tr , 'accuracy_test':acc_te, 'precision':pre_te, 'recall':rec_te,\\\n",
        "                      'roc_auc':roc_auc_te,  'f1':f1_te, 'mcc': mcc_te, 'tn%':tn/t, 'fp%':fp/t, 'fn%':fn/t, 'tp%':tp/t }\n",
        "        return metrics_dict"
      ],
      "metadata": {
        "id": "pHTfpkhEPjfs"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ë¬¼ë¡ ì…ë‹ˆë‹¤. `evaluate_class_mdl` í•¨ìˆ˜ë¥¼ line-by-lineìœ¼ë¡œ ìì„¸í•˜ê²Œ ë¶„ì„í•˜ê² ìŠµë‹ˆë‹¤.\n",
        "\n",
        "1. **í•¨ìˆ˜ ì •ì˜**:\n",
        "```python\n",
        "def evaluate_class_mdl(fitted_model, X_train, X_test, y_train, y_test, plot=True, pct=True, thresh=0.5):\n",
        "```\n",
        "- `evaluate_class_mdl`ë¼ëŠ” ì´ë¦„ì˜ í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
        "- ì…ë ¥ ë³€ìˆ˜ë¡œ í•™ìŠµëœ ëª¨ë¸, í›ˆë ¨ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„°, ê·¸ë¦¬ê³  ì„¸ ê°€ì§€ ì˜µì…˜(ì‹œê°í™” ì—¬ë¶€, ë°±ë¶„ìœ¨ í‘œì‹œ ì—¬ë¶€, ì„ê³„ê°’)ì„ ë°›ìŠµë‹ˆë‹¤.\n",
        "\n",
        "2. **í›ˆë ¨ ë°ì´í„° ì˜ˆì¸¡**:\n",
        "```python\n",
        "y_train_pred = fitted_model.predict(X_train).squeeze()\n",
        "```\n",
        "- í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ê°’ì„ êµ¬í•˜ê³ , ì´ ê°’ì„ `y_train_pred`ì— ì €ì¥í•©ë‹ˆë‹¤.\n",
        "\n",
        "3. **ì˜ˆì¸¡ê°’ ìœ í˜• í™•ì¸ ë° ë³€í™˜**:\n",
        "```python\n",
        "if len(np.unique(y_train_pred)) > 2:\n",
        "    y_train_pred = np.where(y_train_pred > thresh, 1, 0)\n",
        "    y_test_prob = fitted_model.predict(X_test).squeeze()\n",
        "    y_test_pred = np.where(y_test_prob > thresh, 1, 0)\n",
        "```\n",
        "- ë§Œì•½ í›ˆë ¨ ë°ì´í„°ì˜ ì˜ˆì¸¡ê°’ì´ ì´ì§„ ê°’(0 ë˜ëŠ” 1)ì´ ì•„ë‹ˆë¼ë©´, ì£¼ì–´ì§„ ì„ê³„ê°’(`thresh`)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì´ì§„ ì˜ˆì¸¡ê°’ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
        "- í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•´ì„œë„ ë§ˆì°¬ê°€ì§€ë¡œ í™•ë¥  ê°’ì„ êµ¬í•˜ê³ , ì„ê³„ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ì´ì§„ ì˜ˆì¸¡ê°’ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "\n",
        "4. **ì´ì§„ ì˜ˆì¸¡ì˜ ê²½ìš°**:\n",
        "```python\n",
        "else:   \n",
        "    y_test_prob = fitted_model.predict_proba(X_test)[:,1]\n",
        "    y_test_pred = np.where(y_test_prob > thresh, 1, 0)\n",
        "```\n",
        "- ë§Œì•½ í›ˆë ¨ ë°ì´í„°ì˜ ì˜ˆì¸¡ê°’ì´ ì´ë¯¸ ì´ì§„ ê°’ì´ë¼ë©´, í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•´ì„œ í™•ë¥  ê°’ì„ êµ¬í•˜ê³ , ì„ê³„ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ì´ì§„ ì˜ˆì¸¡ê°’ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "\n",
        "5. **ROC-AUC ê³„ì‚°**:\n",
        "```python\n",
        "roc_auc_te = metrics.roc_auc_score(y_test, y_test_prob)\n",
        "```\n",
        "- í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ì‹¤ì œê°’ê³¼ í™•ë¥  ì˜ˆì¸¡ê°’ì„ ì‚¬ìš©í•˜ì—¬ ROC-AUC ê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
        "\n",
        "6. **í˜¼ë™ í–‰ë ¬ ìƒì„±**:\n",
        "```python\n",
        "cf_matrix = metrics.confusion_matrix(y_test, y_test_pred)\n",
        "```\n",
        "- í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ì‹¤ì œê°’ê³¼ ì´ì§„ ì˜ˆì¸¡ê°’ì„ ì‚¬ìš©í•˜ì—¬ í˜¼ë™ í–‰ë ¬ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "\n",
        "7. **í˜¼ë™ í–‰ë ¬ ê°’ ì¶”ì¶œ**:\n",
        "```python\n",
        "tn, fp, fn, tp = cf_matrix.ravel()\n",
        "```\n",
        "- í˜¼ë™ í–‰ë ¬ì—ì„œ True Negative, False Positive, False Negative, True Positive ê°’ì„ ê°ê° ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
        "\n",
        "8. **ë‹¤ì–‘í•œ ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°**:\n",
        "```python\n",
        "acc_tr = metrics.accuracy_score(y_train, y_train_pred)\n",
        "acc_te = metrics.accuracy_score(y_test, y_test_pred)\n",
        "pre_te = metrics.precision_score(y_test, y_test_pred)\n",
        "rec_te = metrics.recall_score(y_test, y_test_pred)\n",
        "f1_te = metrics.f1_score(y_test, y_test_pred)\n",
        "mcc_te = metrics.matthews_corrcoef(y_test, y_test_pred)\n",
        "```\n",
        "- í›ˆë ¨ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ì •í™•ë„, í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ì •ë°€ë„, ì¬í˜„ìœ¨, F1 ìŠ¤ì½”ì–´, ë§¤íŠœ ìƒê´€ ê³„ìˆ˜ì™€ ê°™ì€ ì„±ëŠ¥ ì§€í‘œë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
        "\n",
        "9. **ê²°ê³¼ ì¶œë ¥ ë° ì‹œê°í™”**:\n",
        "```python\n",
        "if plot:\n",
        "    print(f\"Accuracy_train:  {acc_tr:.4f}\\t\\tAccuracy_test:   {acc_te:.4f}\")\n",
        "    ...\n",
        "    plt.show()\n",
        "    return y_train_pred, y_test_prob, y_test_pred\n",
        "```\n",
        "- `plot=True`ì¸ ê²½ìš°, ì„±ëŠ¥ ì§€í‘œë¥¼ ì¶œë ¥í•˜ê³  í˜¼ë™ í–‰ë ¬ì„ ì‹œê°í™”í•˜ì—¬ í‘œì‹œí•©ë‹ˆë‹¤.\n",
        "- ì‹œê°í™” ê²°ê³¼ë¥¼ í‘œì‹œí•œ í›„, í›ˆë ¨ ë°ì´í„° ì˜ˆì¸¡ê°’, í…ŒìŠ¤íŠ¸ ë°ì´í„° í™•ë¥ ê°’, í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ê°’ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
        "\n",
        "10. **ì„±ëŠ¥ ì§€í‘œ ë°˜í™˜**:\n",
        "```python\n",
        "else:\n",
        "    ...\n",
        "    return metrics_dict\n",
        "```\n",
        "- `plot=False`ì¸ ê²½ìš°, ê³„ì‚°ëœ ì„±ëŠ¥ ì§€í‘œë“¤ì„ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
        "\n",
        "ì´ë ‡ê²Œ `evaluate_class_mdl` í•¨ìˆ˜ëŠ” ì£¼ì–´ì§„ í•™ìŠµëœ ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì„±ëŠ¥ì„ í‰ê°€í•˜ê³ , ê²°ê³¼ë¥¼ ì¶œë ¥í•˜ê±°ë‚˜ ë°˜í™˜í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "6whfutXBQiHU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`.squeeze()`ëŠ” NumPy ë°°ì—´ì˜ ë©”ì„œë“œë¡œ, ì°¨ì› ì¤‘ í¬ê¸°ê°€ 1ì¸ ì°¨ì›ì„ ì œê±°í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.\n",
        "\n",
        "ì˜ˆë¥¼ ë“¤ì–´, ë§Œì•½ ë°°ì—´ì˜ í˜•íƒœ(shape)ê°€ `(n, 1)`ì´ë¼ë©´, `.squeeze()`ë¥¼ ì‚¬ìš©í•˜ë©´ ë°°ì—´ì˜ í˜•íƒœëŠ” `(n,)`ì´ ë©ë‹ˆë‹¤.\n",
        "\n",
        "`fitted_model.predict(X_train)`ì˜ ê²°ê³¼ëŠ” ë•Œë¡œëŠ” 2ì°¨ì› ë°°ì—´ë¡œ ë°˜í™˜ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, `(n, 1)`ì™€ ê°™ì€ í˜•íƒœë¡œ ë§ì´ì£ . `.squeeze()`ë¥¼ ì‚¬ìš©í•¨ìœ¼ë¡œì¨ ê²°ê³¼ë¥¼ 1ì°¨ì› ë°°ì—´ë¡œ ë³€í™˜í•˜ì—¬ ì—°ì‚°ì„ ê°„ì†Œí™”í•˜ê±°ë‚˜, ë‹¤ë¥¸ í•¨ìˆ˜ì™€ì˜ í˜¸í™˜ì„±ì„ ë†’ì´ê¸° ìœ„í•´ ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "ê°„ë‹¨í•œ ì˜ˆì œë¡œ ì„¤ëª…í•˜ê² ìŠµë‹ˆë‹¤:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# 2ì°¨ì› ë°°ì—´ ìƒì„±\n",
        "arr = np.array([[1], [2], [3], [4]])\n",
        "print(arr.shape)  # (4, 1)\n",
        "\n",
        "# .squeeze()ë¥¼ ì‚¬ìš©í•˜ì—¬ 1ì°¨ì› ë°°ì—´ë¡œ ë³€í™˜\n",
        "squeezed_arr = arr.squeeze()\n",
        "print(squeezed_arr.shape)  # (4,)\n",
        "```\n",
        "\n",
        "ë”°ë¼ì„œ, `y_train_pred = fitted_model.predict(X_train).squeeze()`ì—ì„œ `.squeeze()`ëŠ” ë°˜í™˜ëœ ì˜ˆì¸¡ê°’ ë°°ì—´ì„ 1ì°¨ì› ë°°ì—´ë¡œ ë³€í™˜í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "0lUaFrpMUGgp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-7. Base ëª¨ë¸ í•™ìŠµ"
      ],
      "metadata": {
        "id": "SD_qcCBUP7O-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "clf = lgb.LGBMClassifier(random_state=rand, n_jobs=-1)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "_ =  evaluate_class_mdl(clf, X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 690
        },
        "id": "TykimZ-8PjdQ",
        "outputId": "d2092c68-9ac4-43f7-93b8-a88028d70e30"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.707875 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "Accuracy_train:  0.8888\t\tAccuracy_test:   0.8877\n",
            "Precision_test:  0.0000\t\tRecall_test:     0.0000\n",
            "ROC-AUC_test:    0.6469\t\tF1_test:         0.0000\t\tMCC_test: -0.0029\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ìˆ«ìë§Œ ë´¤ì„ ë•ŒëŠ” ì •í™•ë„ëŠ” ë§¤ìš° ë†’ì€ ê²ƒìœ¼ë¡œ ë³´ì´ì§€ë§Œ,\n",
        "Precisionê³¼ Recallì€ ë§¤ìš° ë‚®ì•„ ì „ë°˜ì ì¸ ì„±ëŠ¥ì€ ë‚˜ì˜ë‹¤."
      ],
      "metadata": {
        "id": "w35FUmifQGcZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1-7-1. Class Weights"
      ],
      "metadata": {
        "id": "GRd4IjTVQIWS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "ëª¨ë¸ì€ ê¸°ë³¸ì ìœ¼ë¡œ Positive í´ë˜ìŠ¤ë¥¼ ì˜ˆì¸¡í•  ë•Œ ë³´ìˆ˜ì  ì ‘ê·¼ ë°©ë²•ìœ¼ë¡œ ëª¨ë¸ì„ ìƒì„±í•˜ê¸° ë–„ë¬¸ì— ë„ˆë¬´ ë§ì€ ìœ„í—˜ì„ ê°ìˆ˜í•˜ì§€ ì•ŠëŠ”ë‹¤.\n",
        "í•´ê²°ì±…ì€ Positive í´ë˜ìŠ¤ì— ë” ë§ì€ ë¹„ì¤‘ì„ ë‘ë„ë¡ ê°•ì œí•˜ëŠ” Hyperparameterë¥¼ í™œìš©í•˜ëŠ” ê²ƒì´ë‹¤.\n",
        "í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì€ ì—¬ëŸ¬ ê°€ì§€ê°€ ìˆì§€ë§Œ, ê·¸ ì¤‘ í•˜ë‚˜ì¸ `scale_pos_weight`ë¥¼ í™œìš©í•´ë³´ì.\n",
        "\n",
        "ë³´í†µ ë§ì´ ì‚¬ìš©í•˜ëŠ” ê°’ì€ ì•„ë˜ ìˆ˜ì‹ì„ í†µí•´ ê³„ì‚°í•œë‹¤.\n",
        "\n",
        "$\\large{\\frac{\\text{number of negative samples}}{\\text{number of positive samples}}}$"
      ],
      "metadata": {
        "id": "pZ0sK5FHQJzM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "ëª¨ë¸ì´ í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¬¸ì œë¥¼ ì²˜ë¦¬í•  ë•Œ, ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œëŠ” Positive í´ë˜ìŠ¤ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë° ë³´ìˆ˜ì ìœ¼ë¡œ ë™ì‘í•˜ê²Œ ë©ë‹ˆë‹¤. ì´ëŠ” Negative í´ë˜ìŠ¤ì˜ ë°ì´í„°ê°€ ë§ì„ ê²½ìš°, ë‹¨ìˆœíˆ ëŒ€ë¶€ë¶„ì˜ ì˜ˆì¸¡ì„ Negativeë¡œ í•˜ë©´ ì •í™•ë„ê°€ ë†’ê²Œ ë‚˜ì˜¤ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ëŸ° ì ‘ê·¼ ë°©ì‹ì€ Positive í´ë˜ìŠ¤ë¥¼ ì˜ëª» ë¶„ë¥˜í•˜ëŠ” ê²½ìš°ê°€ ë§ì•„ì§ˆ ìˆ˜ ìˆìœ¼ë©°, íŠ¹íˆ Positive í´ë˜ìŠ¤ì˜ ì˜ˆì¸¡ì´ ì¤‘ìš”í•œ ìƒí™©ì—ì„œëŠ” ì´ëŸ° ì ‘ê·¼ ë°©ì‹ì´ ë¬¸ì œê°€ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ í´ë˜ìŠ¤ì˜ ë¶ˆê· í˜•ì„ í•´ì†Œí•˜ëŠ” ë‹¤ì–‘í•œ ë°©ë²•ì´ ì œì•ˆë˜ì—ˆìŠµë‹ˆë‹¤. ê·¸ ì¤‘ í•˜ë‚˜ëŠ” í´ë˜ìŠ¤ì— ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì—¬ê¸°ì„œ ì–¸ê¸‰ëœ `scale_pos_weight`ëŠ” ê·¸ëŸ¬í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¤‘ í•˜ë‚˜ë¡œ, ì£¼ë¡œ XGBoostì™€ ê°™ì€ ì•Œê³ ë¦¬ì¦˜ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
        "\n",
        "`scale_pos_weight`ëŠ” Positive í´ë˜ìŠ¤ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì ˆí•˜ëŠ” ë° ì‚¬ìš©ë˜ë©°, ê°’ì´ í´ìˆ˜ë¡ ëª¨ë¸ì€ Positive í´ë˜ìŠ¤ì— ë” í° ì¤‘ìš”ë„ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤. ì´ í•˜ì´í¼íŒŒë¼ë¯¸í„°ì˜ ê°’ì„ ì„¤ì •í•˜ëŠ” ì¼ë°˜ì ì¸ ë°©ë²•ì€ Negative ìƒ˜í”Œ ìˆ˜ë¥¼ Positive ìƒ˜í”Œ ìˆ˜ë¡œ ë‚˜ëˆˆ ê°’ìœ¼ë¡œ ê³„ì‚°í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ë ‡ê²Œ ê³„ì‚°ëœ ê°’ì€ Positive í´ë˜ìŠ¤ì˜ ì˜ˆì¸¡ì— ë” í° ì¤‘ìš”ë„ë¥¼ ë¶€ì—¬í•˜ë„ë¡ ëª¨ë¸ì„ ìœ ë„í•©ë‹ˆë‹¤.\n",
        "\n",
        "ì˜ˆë¥¼ ë“¤ì–´, Negative í´ë˜ìŠ¤ ìƒ˜í”Œì´ 1000ê°œ, Positive í´ë˜ìŠ¤ ìƒ˜í”Œì´ 100ê°œì¸ ê²½ìš° `scale_pos_weight`ëŠ” \\( \\frac{1000}{100} = 10 \\)ì´ ë©ë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì— Positive í´ë˜ìŠ¤ë¥¼ 10ë°° ë” ì¤‘ìš”í•˜ê²Œ ì—¬ê¸°ë„ë¡ ì•Œë ¤ì£¼ëŠ” ê²ƒì…ë‹ˆë‹¤.\n",
        "\n",
        "ì´ë ‡ê²Œ í´ë˜ìŠ¤ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì ˆí•¨ìœ¼ë¡œì¨ ëª¨ë¸ì´ ë” ê· í˜• ì¡íŒ ì˜ˆì¸¡ì„ í•˜ë„ë¡ ë„ì™€ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "6oXYNtY7Qvjo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def_scale_pos_weight = len(y[y==0]) / len(y[y==1])\n",
        "print(f\"default scale pos weight: {def_scale_pos_weight:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iG0SuyDaPjaj",
        "outputId": "9cd6d8ae-c69c-4fba-d286-828a747b6654"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default scale pos weight: 7.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ì´ ê°’ì€ ì‹¤ì œë¡œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ì§€ë§Œ ì—¬ì „íˆ ìƒë‹¹í•œ ìˆ˜ì¤€ì˜ ì˜¤íƒì§€ê°€ ì¡´ì¬í•œë‹¤."
      ],
      "metadata": {
        "id": "2N99T7jjQbx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = lgb.LGBMClassifier(random_state=rand, n_jobs=-1, scale_pos_weight=def_scale_pos_weight)\n",
        "clf.fit(X_train, y_train)\n",
        "_ =  evaluate_class_mdl(clf, X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 690
        },
        "id": "FcHe7XBBPjXk",
        "outputId": "bdd29a74-0ae9-45e0-ac28-dd412c657ea1"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.194974 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "Accuracy_train:  0.6381\t\tAccuracy_test:   0.6086\n",
            "Precision_test:  0.1638\t\tRecall_test:     0.6064\n",
            "ROC-AUC_test:    0.6458\t\tF1_test:         0.2580\t\tMCC_test: 0.1379\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ê·¸ë¦¬ê³  ë‘ ë°°ë¡œ ë§Œë“¤ë©´ ì˜¤íƒì§€ê°€ ë‘ ë°°ê°€ ëœë‹¤."
      ],
      "metadata": {
        "id": "CMf59AS4RuE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = lgb.LGBMClassifier(random_state=rand, n_jobs=-1, scale_pos_weight=def_scale_pos_weight*2)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "_ = evaluate_class_mdl(clf, X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 690
        },
        "id": "lPb3lbWBPjVd",
        "outputId": "86ccb55b-11cc-4559-8e9b-bbec8879acbf"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.230456 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "Accuracy_train:  0.3354\t\tAccuracy_test:   0.3157\n",
            "Precision_test:  0.1293\t\tRecall_test:     0.8893\n",
            "ROC-AUC_test:    0.6438\t\tF1_test:         0.2258\t\tMCC_test: 0.0997\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ê·¸ë¦¬ê³  ì ˆë°˜ìœ¼ë¡œ ì¤„ì´ë©´ ë°˜ëŒ€ë¡œ ëª¨ë¸ì´ ë” ë³´ìˆ˜ì ìœ¼ë¡œ ë°”ë€ë‹¤."
      ],
      "metadata": {
        "id": "I5EpQq2xR2y0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = lgb.LGBMClassifier(random_state=rand, n_jobs=-1, scale_pos_weight=def_scale_pos_weight/2)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "_ = evaluate_class_mdl(clf, X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 690
        },
        "id": "4YqihI3oPjSx",
        "outputId": "0da1a76e-aef7-4546-efae-5309bbbf1c88"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018121 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "Accuracy_train:  0.8771\t\tAccuracy_test:   0.8617\n",
            "Precision_test:  0.2232\t\tRecall_test:     0.0937\n",
            "ROC-AUC_test:    0.6485\t\tF1_test:         0.1320\t\tMCC_test: 0.0782\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1-7-2. Hyperparameter tuning"
      ],
      "metadata": {
        "id": "lY4P8DBaR662"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ë³´ìˆ˜ì  ì ‘ê·¼ ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ì€ íŠ¹ì§• ë•Œë¬¸ì— ì‹¤ì œ ì ìš©ì´ ë„ˆë¬´ ì–´ë µë‹¤.\n",
        "\n",
        "- True Positive(ì •íƒ)ë§Œ Maximize\n",
        "- False Positive(ë¯¸íƒ) ìµœì†Œí™”\n",
        "- ë³´í†µì€ Recallì´ë‚˜ Precisionì„ ë” ì„ í˜¸\n",
        "- ë¯¸íƒë³´ë‹¤ ì˜¤íƒì´ í›¨ì”¬ ë” ë§ìŒ"
      ],
      "metadata": {
        "id": "5RjhZvnqR77N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "í˜„ì¬ ëª¨ë¸ì˜ ê²°ê³¼ë¥¼ ë³´ë©´, ê±°ì˜ ëª¨ë“  ê²ƒì´ Positive í´ë˜ìŠ¤ì— ì†í•  ê²ƒìœ¼ë¡œ ì˜ˆìƒë˜ëŠ” ë¬¸ì œê°€ ìˆë‹¤."
      ],
      "metadata": {
        "id": "Lh3VEe9tR9PK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ëª‡ ê°€ì§€ ì œì•½ ì¡°ê±´ì„ ì •ì˜í•´ì•¼ í•œë‹¤.\n",
        "ì˜ˆë¥¼ ë“¤ì–´, ì¬ì…ì›í•˜ì§€ ì•Šì€ ëª¨ë“  ë°ì´í„°ì„ í¬í•¨í•  ìˆ˜ ìˆë„ë¡ ì ì–´ë„ ì¶©ë¶„í•œ true negative(ì •ìŒì„±)ì´ ìˆì–´ì•¼í•œë‹¤."
      ],
      "metadata": {
        "id": "EslePTjiR-ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "min_tn = X_orig[X_orig['readmitted']=='NO'].shape[0] / X_orig.shape[0]\n",
        "print(f\"Patients % that never readmitted (Minimum % of True Negatives): {min_tn:.2%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6-WmyHXPjQE",
        "outputId": "7c06be7a-2878-44b8-b49a-86aade303efa"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Patients % that never readmitted (Minimum % of True Negatives): 53.91%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ë˜í•œ ìš°ë¦¬ëŠ” 30ì¼ì´ ì§€ë‚˜ì„œ ì¬ì…ì›í•œ í™˜ìë„ False Positiveì— í¬í•¨ë˜ë„ë¡ ë¹„ìœ¨ì„ ì¡°ì •í•´ì•¼ í•œë‹¤."
      ],
      "metadata": {
        "id": "0qhI2GPZSSHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_fp = X_orig[X_orig['readmitted']=='>30'].shape[0] / X_orig.shape[0]\n",
        "print(f\"Patients % that were readmitted over 30 days later (Maximum % of False Positives): {max_fp:.2%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdkvBEwNPjNN",
        "outputId": "cb5bdc8d-f96f-4d2f-e996-a706dd1a4157"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Patients % that were readmitted over 30 days later (Maximum % of False Positives): 34.93%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define optimization fuction**"
      ],
      "metadata": {
        "id": "gxBsEvH-SfPW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "í•¨ìˆ˜ì—ì„œ `scale_pos_weight`ë¥¼ ì‚¬ìš©í•œ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì™¸ì—ë„ `max_depth`(íŠ¸ë¦¬ ê¹Šì´ë¥¼ ì œí•œí•˜ê¸° ìœ„í•´) ë° `reg_lambda` ë° `reg_alpha`ë¥¼ ì‚¬ìš©í•œ L1/L2 ì •ê·œí™”ì— ëŒ€í•œ ìµœìƒì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ìŠµë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ëª¨ë¸ì´ ì¼ë°˜í™”ë˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤. ëª©ì  í•¨ìˆ˜ëŠ” ë‘ ì œì•½ ì¡°ê±´ì´ ì¶©ì¡±ë˜ì§€ ì•ŠëŠ” ê²½ìš°ë¥¼ ì œì™¸í•˜ê³  `recall`ì„ ì¶œë ¥í•©ë‹ˆë‹¤. ì´ ê²½ìš° 0ì„ ë°˜í™˜í•©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "AcrAqZ8bSgaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize_lgb(trial):\n",
        "    params = {\n",
        "        'max_depth': trial.suggest_int('max_depth', 2, 11),\n",
        "        'scale_pos_weight': trial.suggest_float('scale_pos_weight', def_scale_pos_weight/2, def_scale_pos_weight*2),\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True)\n",
        "    }\n",
        "    if params['max_depth'] == 11:\n",
        "        params['max_depth'] = -1\n",
        "\n",
        "    clf = lgb.LGBMClassifier(random_state=rand, n_jobs=-1, **params)\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    metrics_dict = evaluate_class_mdl(clf, X_train, X_test, y_train, y_test, plot=False)\n",
        "\n",
        "    if (metrics_dict['tn%'] < min_tn) or (metrics_dict['fp%'] > max_fp):\n",
        "        return 0\n",
        "\n",
        "    return metrics_dict['recall']"
      ],
      "metadata": {
        "id": "aeQDZkQ6PjLC"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. **í•¨ìˆ˜ ì •ì˜**:\n",
        "```python\n",
        "def optimize_lgb(trial):\n",
        "```\n",
        "- `optimize_lgb`ë¼ëŠ” ì´ë¦„ì˜ í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
        "- `trial`ì´ë¼ëŠ” ì¸ìë¥¼ ë°›ìŠµë‹ˆë‹¤. ì´ ì¸ìëŠ” Optuna ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”ë¥¼ ìœ„í•œ ê° ì‹œë„ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n",
        "\n",
        "2. **í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •**:\n",
        "```python\n",
        "params = {\n",
        "    'max_depth': trial.suggest_int('max_depth', 2, 11),\n",
        "    'scale_pos_weight': trial.suggest_float('scale_pos_weight', def_scale_pos_weight/2, def_scale_pos_weight*2),\n",
        "    'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
        "    'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True)\n",
        "}\n",
        "```\n",
        "- `params` ë”•ì…”ë„ˆë¦¬ ì•ˆì— LightGBM ëª¨ë¸ì˜ ì—¬ëŸ¬ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\n",
        "- `trial.suggest_...` ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ê° í•˜ì´í¼íŒŒë¼ë¯¸í„°ì˜ ë²”ìœ„ì™€ íƒ€ì…ì„ ì§€ì •í•©ë‹ˆë‹¤.\n",
        "\n",
        "3. **max_depth ê°’ ìˆ˜ì •**:\n",
        "```python\n",
        "if params['max_depth'] == 11:\n",
        "    params['max_depth'] = -1\n",
        "```\n",
        "- ë§Œì•½ `max_depth` ê°’ì´ 11ì´ë¼ë©´, ê·¸ ê°’ì„ -1ë¡œ ë³€ê²½í•©ë‹ˆë‹¤. LightGBMì—ì„œ `max_depth` ê°’ì´ -1ì¸ ê²½ìš°, ì œí•œ ì—†ì´ ê¹Šê²Œ íŠ¸ë¦¬ë¥¼ ì„±ì¥ì‹œí‚¤ë¼ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤.\n",
        "\n",
        "4. **ëª¨ë¸ í•™ìŠµ**:\n",
        "```python\n",
        "clf = lgb.LGBMClassifier(random_state=rand, n_jobs=-1, **params)\n",
        "clf.fit(X_train, y_train)\n",
        "```\n",
        "- ì„¤ì •ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ LightGBM ë¶„ë¥˜ê¸°ë¥¼ ì´ˆê¸°í™”í•˜ê³  í›ˆë ¨ ë°ì´í„°ë¡œ í•™ìŠµì‹œí‚µë‹ˆë‹¤.\n",
        "\n",
        "5. **ëª¨ë¸ ì„±ëŠ¥ í‰ê°€**:\n",
        "```python\n",
        "metrics_dict = evaluate_class_mdl(clf, X_train, X_test, y_train, y_test, plot=False)\n",
        "```\n",
        "- ì•ì„œ ì£¼ì–´ì§„ `evaluate_class_mdl` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµëœ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê³ , ê·¸ ê²°ê³¼ë¥¼ `metrics_dict`ì— ì €ì¥í•©ë‹ˆë‹¤.\n",
        "\n",
        "6. **ì¡°ê±´ í™•ì¸ ë° ë°˜í™˜**:\n",
        "```python\n",
        "if (metrics_dict['tn%'] < min_tn) or (metrics_dict['fp%'] > max_fp):\n",
        "    return 0\n",
        "```\n",
        "- ë§Œì•½ True Negativeì˜ ë¹„ìœ¨ì´ `min_tn`ë³´ë‹¤ ì‘ê±°ë‚˜, False Positiveì˜ ë¹„ìœ¨ì´ `max_fp`ë³´ë‹¤ í¬ë‹¤ë©´, 0ì„ ë°˜í™˜í•©ë‹ˆë‹¤. ì´ëŠ” í•´ë‹¹ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •ì—ì„œ ì›í•˜ëŠ” ì„±ëŠ¥ì„ ì–»ì§€ ëª»í–ˆìŒì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n",
        "\n",
        "7. **ê²°ê³¼ ë°˜í™˜**:\n",
        "```python\n",
        "return metrics_dict['recall']\n",
        "```\n",
        "- ìµœì¢…ì ìœ¼ë¡œ, `recall` ê°’ì„ ë°˜í™˜í•©ë‹ˆë‹¤. ì´ ê°’ì€ Optuna ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ìµœì í™”í•˜ëŠ” ë° ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "\n",
        "í•¨ìˆ˜ì˜ ì£¼ìš” ëª©ì ì€ ì£¼ì–´ì§„ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •ì—ì„œì˜ ëª¨ë¸ ì„±ëŠ¥ì„ í‰ê°€í•˜ê³ , ê·¸ ì„±ëŠ¥ì„ ê¸°ë°˜ìœ¼ë¡œ ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ëŠ” ê²ƒì…ë‹ˆë‹¤. Optunaì™€ ê°™ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ì´ í•¨ìˆ˜ë¥¼ ì—¬ëŸ¬ ë²ˆ í˜¸ì¶œí•˜ë©° ë‹¤ì–‘í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•©ì„ ì‹œë„í•˜ì—¬ ìµœì ì˜ ê°’ì„ ì°¾ìŠµë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "rYgL3YFNTqrB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run optimization trials**"
      ],
      "metadata": {
        "id": "JlV7dYqSTH0C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`optuna`ë¡œ 100ë²ˆì˜ ì‹œë„ë¥¼ ì‹¤í–‰í•˜ëŠ” ë° ëª‡ ë¶„ ì •ë„ ê±¸ë¦°ë‹¤."
      ],
      "metadata": {
        "id": "AeNFMZEZTKAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "opt_study = optuna.create_study(direction='maximize')\n",
        "opt_study.optimize(optimize_lgb, n_trials=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEXBFCzEPjIR",
        "outputId": "5dc193e9-6190-4a51-c7dd-bec692e14b4a"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:05,379] A new study created in memory with name: no-name-65fc8817-6724-47c1-9d2a-c887397b7fae\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.775424 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:08,195] Trial 0 finished with value: 0.0 and parameters: {'max_depth': 7, 'scale_pos_weight': 15.121805714925035, 'reg_lambda': 6.718374415691349e-08, 'reg_alpha': 3.584673017187385e-05}. Best is trial 0 with value: 0.0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005327 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:08,839] Trial 1 finished with value: 0.5527007299270073 and parameters: {'max_depth': 8, 'scale_pos_weight': 7.285324078943653, 'reg_lambda': 0.2537414656103463, 'reg_alpha': 3.894389036781523e-08}. Best is trial 1 with value: 0.5527007299270073.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005579 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:09,449] Trial 2 finished with value: 0.0 and parameters: {'max_depth': 9, 'scale_pos_weight': 13.068285593462033, 'reg_lambda': 0.0009014336482409883, 'reg_alpha': 1.7633980509296692}. Best is trial 1 with value: 0.5527007299270073.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005803 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:10,057] Trial 3 finished with value: 0.0 and parameters: {'max_depth': 8, 'scale_pos_weight': 13.33316888664271, 'reg_lambda': 6.350742461788189e-05, 'reg_alpha': 0.0008573226392481219}. Best is trial 1 with value: 0.5527007299270073.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005325 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:10,699] Trial 4 finished with value: 0.0 and parameters: {'max_depth': 10, 'scale_pos_weight': 9.944959740045801, 'reg_lambda': 9.502061550462396e-05, 'reg_alpha': 6.580814546308679}. Best is trial 1 with value: 0.5527007299270073.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007007 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:13,257] Trial 5 finished with value: 0.49664233576642336 and parameters: {'max_depth': 5, 'scale_pos_weight': 6.670155697044471, 'reg_lambda': 9.459018499810325, 'reg_alpha': 0.1458054377559234}. Best is trial 1 with value: 0.5527007299270073.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007150 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:14,134] Trial 6 finished with value: 0.0 and parameters: {'max_depth': 3, 'scale_pos_weight': 15.649387498443033, 'reg_lambda': 0.007425256981131767, 'reg_alpha': 3.733802702415331}. Best is trial 1 with value: 0.5527007299270073.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.226336 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:15,223] Trial 7 finished with value: 0.0 and parameters: {'max_depth': 7, 'scale_pos_weight': 14.349023680218632, 'reg_lambda': 0.0008347291761900468, 'reg_alpha': 7.347510426851715e-07}. Best is trial 1 with value: 0.5527007299270073.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005742 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:16,319] Trial 8 finished with value: 0.0 and parameters: {'max_depth': 4, 'scale_pos_weight': 15.478299387010061, 'reg_lambda': 0.0005009862477421742, 'reg_alpha': 0.07185091264594592}. Best is trial 1 with value: 0.5527007299270073.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.555636 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:18,199] Trial 9 finished with value: 0.0 and parameters: {'max_depth': 5, 'scale_pos_weight': 9.305589006188278, 'reg_lambda': 5.723253915589503e-05, 'reg_alpha': 0.24013529844052278}. Best is trial 1 with value: 0.5527007299270073.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005900 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:18,960] Trial 10 finished with value: 0.12058394160583942 and parameters: {'max_depth': 11, 'scale_pos_weight': 4.209490206261985, 'reg_lambda': 4.8992511070006035, 'reg_alpha': 1.0587759705473821e-08}. Best is trial 1 with value: 0.5527007299270073.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007760 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:19,728] Trial 11 finished with value: 0.4916788321167883 and parameters: {'max_depth': 5, 'scale_pos_weight': 6.656379827459399, 'reg_lambda': 7.063822717434679, 'reg_alpha': 0.015940383369689247}. Best is trial 1 with value: 0.5527007299270073.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007015 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:21,282] Trial 12 finished with value: 0.5708029197080292 and parameters: {'max_depth': 6, 'scale_pos_weight': 7.445341876549438, 'reg_lambda': 0.2954115430679669, 'reg_alpha': 0.0013860634949149628}. Best is trial 12 with value: 0.5708029197080292.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.108749 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:21,994] Trial 13 finished with value: 0.0 and parameters: {'max_depth': 2, 'scale_pos_weight': 8.144461797956358, 'reg_lambda': 0.18825844553923904, 'reg_alpha': 4.399332836294035e-05}. Best is trial 12 with value: 0.5708029197080292.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005104 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:22,606] Trial 14 finished with value: 0.5868613138686132 and parameters: {'max_depth': 9, 'scale_pos_weight': 7.595879249275127, 'reg_lambda': 0.08899201678601817, 'reg_alpha': 0.0019180091626405076}. Best is trial 14 with value: 0.5868613138686132.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005276 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:23,261] Trial 15 finished with value: 0.0 and parameters: {'max_depth': 9, 'scale_pos_weight': 11.643686086007147, 'reg_lambda': 0.07463769688987422, 'reg_alpha': 0.002001494905706914}. Best is trial 14 with value: 0.5868613138686132.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005518 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:23,890] Trial 16 finished with value: 0.2773722627737226 and parameters: {'max_depth': 6, 'scale_pos_weight': 5.125531244383088, 'reg_lambda': 0.024187808062662613, 'reg_alpha': 0.003987840686630677}. Best is trial 14 with value: 0.5868613138686132.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005489 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:24,765] Trial 17 finished with value: 0.0 and parameters: {'max_depth': 11, 'scale_pos_weight': 8.45152334348036, 'reg_lambda': 0.6340422130858446, 'reg_alpha': 0.00020738788953331765}. Best is trial 14 with value: 0.5868613138686132.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005426 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:26,132] Trial 18 finished with value: 0.0 and parameters: {'max_depth': 9, 'scale_pos_weight': 10.88532542388841, 'reg_lambda': 0.013270049481035497, 'reg_alpha': 0.00810373252559796}. Best is trial 14 with value: 0.5868613138686132.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005260 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:27,245] Trial 19 finished with value: 0.3605839416058394 and parameters: {'max_depth': 6, 'scale_pos_weight': 5.7251145079611625, 'reg_lambda': 1.2053442304318456, 'reg_alpha': 0.0002674974279405276}. Best is trial 14 with value: 0.5868613138686132.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005102 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:28,317] Trial 20 finished with value: 0.0 and parameters: {'max_depth': 8, 'scale_pos_weight': 8.389758857590017, 'reg_lambda': 0.7066200479195752, 'reg_alpha': 7.815822457092896e-06}. Best is trial 14 with value: 0.5868613138686132.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006622 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:29,207] Trial 21 finished with value: 0.5013138686131386 and parameters: {'max_depth': 8, 'scale_pos_weight': 6.780549867326911, 'reg_lambda': 0.12651043746302318, 'reg_alpha': 1.2567721767867842e-06}. Best is trial 14 with value: 0.5868613138686132.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061278 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:30,462] Trial 22 finished with value: 0.5935766423357665 and parameters: {'max_depth': 10, 'scale_pos_weight': 7.691212531642549, 'reg_lambda': 0.4346572438786779, 'reg_alpha': 0.000583558670160461}. Best is trial 22 with value: 0.5935766423357665.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006179 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:31,512] Trial 23 finished with value: 0.5953284671532847 and parameters: {'max_depth': 10, 'scale_pos_weight': 7.687551972379928, 'reg_lambda': 0.9291342559835776, 'reg_alpha': 0.0012332568838189692}. Best is trial 23 with value: 0.5953284671532847.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.132509 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:36,872] Trial 24 finished with value: 0.0 and parameters: {'max_depth': 10, 'scale_pos_weight': 9.25048432379706, 'reg_lambda': 1.6166307199046988, 'reg_alpha': 0.012772043134607131}. Best is trial 23 with value: 0.5953284671532847.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006092 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:39,249] Trial 25 finished with value: 0.6108029197080292 and parameters: {'max_depth': 10, 'scale_pos_weight': 7.879395574884191, 'reg_lambda': 0.061015912579973706, 'reg_alpha': 0.0003427384767486089}. Best is trial 25 with value: 0.6108029197080292.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011973 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:43,673] Trial 26 finished with value: 0.385985401459854 and parameters: {'max_depth': 10, 'scale_pos_weight': 5.941348414856366, 'reg_lambda': 2.0329558884776313, 'reg_alpha': 0.00026783243418783073}. Best is trial 25 with value: 0.6108029197080292.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039659 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:45,926] Trial 27 finished with value: 0.0 and parameters: {'max_depth': 11, 'scale_pos_weight': 8.682000903984507, 'reg_lambda': 0.039929424955663456, 'reg_alpha': 8.368495725309467e-05}. Best is trial 25 with value: 0.6108029197080292.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005739 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:46,799] Trial 28 finished with value: 0.0 and parameters: {'max_depth': 10, 'scale_pos_weight': 9.852258480082817, 'reg_lambda': 0.004750393099177101, 'reg_alpha': 0.0004970329185434069}. Best is trial 25 with value: 0.6108029197080292.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007161 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:48,245] Trial 29 finished with value: 0.5944525547445255 and parameters: {'max_depth': 11, 'scale_pos_weight': 7.759917231579983, 'reg_lambda': 2.5892195538142087, 'reg_alpha': 2.6629508074998857e-05}. Best is trial 25 with value: 0.6108029197080292.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006056 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:48,879] Trial 30 finished with value: 0.0 and parameters: {'max_depth': 11, 'scale_pos_weight': 8.976087711656465, 'reg_lambda': 2.426511510642956, 'reg_alpha': 3.480275567982472e-05}. Best is trial 25 with value: 0.6108029197080292.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005543 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:49,529] Trial 31 finished with value: 0.6014598540145986 and parameters: {'max_depth': 10, 'scale_pos_weight': 7.878507969274607, 'reg_lambda': 0.6320124931389447, 'reg_alpha': 1.2113850462006202e-05}. Best is trial 25 with value: 0.6108029197080292.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006278 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:50,213] Trial 32 finished with value: 0.6081751824817518 and parameters: {'max_depth': 11, 'scale_pos_weight': 7.920541364055442, 'reg_lambda': 2.1319952795975703, 'reg_alpha': 1.4228585453067834e-05}. Best is trial 25 with value: 0.6108029197080292.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005808 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:50,884] Trial 33 finished with value: 0.5293430656934307 and parameters: {'max_depth': 9, 'scale_pos_weight': 7.052896993033975, 'reg_lambda': 0.36402627806376886, 'reg_alpha': 5.546521816571866e-06}. Best is trial 25 with value: 0.6108029197080292.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005449 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:51,532] Trial 34 finished with value: 0.6090510948905109 and parameters: {'max_depth': 10, 'scale_pos_weight': 8.035951023656683, 'reg_lambda': 0.05037644972813387, 'reg_alpha': 0.00010849973367353079}. Best is trial 25 with value: 0.6108029197080292.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005840 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:52,213] Trial 35 finished with value: 0.0 and parameters: {'max_depth': 7, 'scale_pos_weight': 8.392313678444472, 'reg_lambda': 0.04242756795996777, 'reg_alpha': 0.0001236583425873287}. Best is trial 25 with value: 0.6108029197080292.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005475 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:52,853] Trial 36 finished with value: 0.0 and parameters: {'max_depth': 9, 'scale_pos_weight': 9.43212767908369, 'reg_lambda': 0.004082746280186121, 'reg_alpha': 1.2520561799238527e-05}. Best is trial 25 with value: 0.6108029197080292.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006133 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:53,516] Trial 37 finished with value: 0.0 and parameters: {'max_depth': 8, 'scale_pos_weight': 10.55900511542066, 'reg_lambda': 0.16495447876404556, 'reg_alpha': 2.742197349591006e-06}. Best is trial 25 with value: 0.6108029197080292.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006379 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:56,567] Trial 38 finished with value: 0.4216058394160584 and parameters: {'max_depth': 10, 'scale_pos_weight': 6.208884839484374, 'reg_lambda': 0.021489772553523863, 'reg_alpha': 5.80866621105285e-05}. Best is trial 25 with value: 0.6108029197080292.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007726 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:57,610] Trial 39 finished with value: 0.5246715328467153 and parameters: {'max_depth': 11, 'scale_pos_weight': 6.9357934470081855, 'reg_lambda': 9.80766975669001, 'reg_alpha': 1.5833484256999063e-07}. Best is trial 25 with value: 0.6108029197080292.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006814 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:58,698] Trial 40 finished with value: 0.0 and parameters: {'max_depth': 8, 'scale_pos_weight': 8.775744025823586, 'reg_lambda': 0.0750223326881274, 'reg_alpha': 1.3880472832072269e-05}. Best is trial 25 with value: 0.6108029197080292.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011308 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:30:59,791] Trial 41 finished with value: 0.6154744525547445 and parameters: {'max_depth': 10, 'scale_pos_weight': 7.926867998628939, 'reg_lambda': 0.8284413271486493, 'reg_alpha': 0.00010748920048463551}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007066 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:00,552] Trial 42 finished with value: 0.0 and parameters: {'max_depth': 10, 'scale_pos_weight': 8.052292145285698, 'reg_lambda': 0.3166062780177792, 'reg_alpha': 9.167612973654155e-05}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007229 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:01,300] Trial 43 finished with value: 0.5594160583941605 and parameters: {'max_depth': 11, 'scale_pos_weight': 7.305704005181168, 'reg_lambda': 4.017530257338179, 'reg_alpha': 1.9173828281910887e-05}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007032 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:02,475] Trial 44 finished with value: 0.0 and parameters: {'max_depth': 9, 'scale_pos_weight': 8.021743140877806, 'reg_lambda': 1.187244851050583, 'reg_alpha': 3.5666927550167333e-06}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007048 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:05,112] Trial 45 finished with value: 0.4548905109489051 and parameters: {'max_depth': 11, 'scale_pos_weight': 6.427817022018748, 'reg_lambda': 4.1377017037091735, 'reg_alpha': 5.431896305763212e-05}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010148 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:05,744] Trial 46 finished with value: 0.5275912408759124 and parameters: {'max_depth': 10, 'scale_pos_weight': 7.034096275972473, 'reg_lambda': 0.15000807689582135, 'reg_alpha': 0.00015306981681839838}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006952 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:06,363] Trial 47 finished with value: 0.0 and parameters: {'max_depth': 10, 'scale_pos_weight': 9.082937691687734, 'reg_lambda': 0.5725697783866187, 'reg_alpha': 1.0046081828680355e-05}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006083 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:07,064] Trial 48 finished with value: 0.5672992700729927 and parameters: {'max_depth': 7, 'scale_pos_weight': 7.290725679233404, 'reg_lambda': 9.9175755896143, 'reg_alpha': 0.0005547367147994663}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006352 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:07,739] Trial 49 finished with value: 0.0 and parameters: {'max_depth': 9, 'scale_pos_weight': 8.24775844709045, 'reg_lambda': 0.05719636037789557, 'reg_alpha': 1.2831057290023223e-06}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006140 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:08,352] Trial 50 finished with value: 0.0 and parameters: {'max_depth': 4, 'scale_pos_weight': 9.478301620530884, 'reg_lambda': 0.0020412311078588322, 'reg_alpha': 2.526153030203372e-05}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006835 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:09,084] Trial 51 finished with value: 0.5915328467153285 and parameters: {'max_depth': 10, 'scale_pos_weight': 7.651686369456091, 'reg_lambda': 0.9371874893532784, 'reg_alpha': 0.0010185393010008535}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006175 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:09,756] Trial 52 finished with value: 0.604087591240876 and parameters: {'max_depth': 11, 'scale_pos_weight': 7.901791321478852, 'reg_lambda': 0.20654298926942996, 'reg_alpha': 0.00030735155089074164}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005913 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:10,389] Trial 53 finished with value: 0.4762043795620438 and parameters: {'max_depth': 11, 'scale_pos_weight': 6.60706884260888, 'reg_lambda': 0.2646127762077629, 'reg_alpha': 0.00018272852694786414}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006238 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:11,059] Trial 54 finished with value: 0.0 and parameters: {'max_depth': 11, 'scale_pos_weight': 8.398355627413132, 'reg_lambda': 0.01219325002861966, 'reg_alpha': 0.00032635745493888365}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006601 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:11,723] Trial 55 finished with value: 0.0 and parameters: {'max_depth': 9, 'scale_pos_weight': 8.758007742311822, 'reg_lambda': 0.10060276715426268, 'reg_alpha': 4.120615901714219e-05}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005860 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:12,387] Trial 56 finished with value: 0.0 and parameters: {'max_depth': 11, 'scale_pos_weight': 7.980855825115882, 'reg_lambda': 0.2464730676628332, 'reg_alpha': 9.292007726985918e-05}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006641 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:13,344] Trial 57 finished with value: 0.5687591240875912 and parameters: {'max_depth': 10, 'scale_pos_weight': 7.4494729536595585, 'reg_lambda': 4.06516527740153, 'reg_alpha': 0.002508233307933831}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006019 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:14,588] Trial 58 finished with value: 0.5632116788321168 and parameters: {'max_depth': 2, 'scale_pos_weight': 7.144842930336576, 'reg_lambda': 0.5502774295839646, 'reg_alpha': 0.0008236900453238108}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011078 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:17,266] Trial 59 finished with value: 0.33985401459854014 and parameters: {'max_depth': 10, 'scale_pos_weight': 5.619298531296808, 'reg_lambda': 0.03349037743863412, 'reg_alpha': 0.0002871030361862076}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007647 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:18,034] Trial 60 finished with value: 0.4691970802919708 and parameters: {'max_depth': 9, 'scale_pos_weight': 6.531626205316045, 'reg_lambda': 1.6417328355315755, 'reg_alpha': 1.9104334332541163e-05}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006468 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:18,819] Trial 61 finished with value: 0.5982481751824817 and parameters: {'max_depth': 10, 'scale_pos_weight': 7.824631769566554, 'reg_lambda': 0.8175700858825345, 'reg_alpha': 0.0012170915768700108}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007870 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:20,084] Trial 62 finished with value: 0.6014598540145986 and parameters: {'max_depth': 10, 'scale_pos_weight': 7.867999273862871, 'reg_lambda': 0.13319073912231114, 'reg_alpha': 0.003997181585792564}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006372 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:20,847] Trial 63 finished with value: 0.0 and parameters: {'max_depth': 11, 'scale_pos_weight': 8.653678383948467, 'reg_lambda': 0.09611726127653285, 'reg_alpha': 0.0046547588058868895}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006449 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:21,566] Trial 64 finished with value: 0.0 and parameters: {'max_depth': 11, 'scale_pos_weight': 8.168391452185482, 'reg_lambda': 0.19701316144727335, 'reg_alpha': 0.00013534314640187053}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031954 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:22,395] Trial 65 finished with value: 0.5605839416058395 and parameters: {'max_depth': 10, 'scale_pos_weight': 7.3271151087457875, 'reg_lambda': 0.018786434220302028, 'reg_alpha': 0.0003868890167129254}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008045 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:23,506] Trial 66 finished with value: 0.0 and parameters: {'max_depth': 8, 'scale_pos_weight': 9.145156481984149, 'reg_lambda': 0.39069709409748504, 'reg_alpha': 0.023410120649334613}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007407 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:24,260] Trial 67 finished with value: 0.5086131386861313 and parameters: {'max_depth': 11, 'scale_pos_weight': 6.871516434261479, 'reg_lambda': 0.054559201928517466, 'reg_alpha': 3.443319312793974e-05}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008145 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:25,186] Trial 68 finished with value: 0.6008759124087591 and parameters: {'max_depth': 10, 'scale_pos_weight': 7.717638950074808, 'reg_lambda': 1.8736586439700451, 'reg_alpha': 6.837879073468386e-05}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006752 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:25,834] Trial 69 finished with value: 0.0 and parameters: {'max_depth': 9, 'scale_pos_weight': 8.54883170674196, 'reg_lambda': 0.11328191615429135, 'reg_alpha': 0.0006385767939315269}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005593 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:26,502] Trial 70 finished with value: 0.0 and parameters: {'max_depth': 10, 'scale_pos_weight': 9.616990788324213, 'reg_lambda': 0.5772346323233838, 'reg_alpha': 0.0025463766297841935}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006439 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:27,182] Trial 71 finished with value: 0.6055474452554744 and parameters: {'max_depth': 10, 'scale_pos_weight': 7.795480028078215, 'reg_lambda': 2.62753175513866, 'reg_alpha': 7.351383269272804e-05}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005881 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:27,853] Trial 72 finished with value: 0.0 and parameters: {'max_depth': 10, 'scale_pos_weight': 8.066378055337982, 'reg_lambda': 2.6605945997262896, 'reg_alpha': 0.00016717992165914038}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006000 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:28,545] Trial 73 finished with value: 0.0 and parameters: {'max_depth': 11, 'scale_pos_weight': 8.964855095343657, 'reg_lambda': 5.617292350499451, 'reg_alpha': 7.105574090774774e-06}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006169 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:30,435] Trial 74 finished with value: 0.5620437956204379 and parameters: {'max_depth': 9, 'scale_pos_weight': 7.460479651672464, 'reg_lambda': 1.267417027574727, 'reg_alpha': 6.323028580447011e-05}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006086 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:31,310] Trial 75 finished with value: 0.0 and parameters: {'max_depth': 11, 'scale_pos_weight': 8.433310044549069, 'reg_lambda': 0.23380293091417786, 'reg_alpha': 1.9308843762136446e-05}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005569 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:31,964] Trial 76 finished with value: 0.6052554744525548 and parameters: {'max_depth': 9, 'scale_pos_weight': 7.839164858229969, 'reg_lambda': 0.7990193520708064, 'reg_alpha': 0.00021655758070583282}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005502 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:32,647] Trial 77 finished with value: 0.5194160583941606 and parameters: {'max_depth': 9, 'scale_pos_weight': 6.9276410860606905, 'reg_lambda': 0.9801414150945221, 'reg_alpha': 0.00023863133334491772}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005645 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:33,591] Trial 78 finished with value: 0.574014598540146 and parameters: {'max_depth': 10, 'scale_pos_weight': 7.481294008504661, 'reg_lambda': 2.884513370868218, 'reg_alpha': 0.00012496953714717958}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.073955 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:34,921] Trial 79 finished with value: 0.0 and parameters: {'max_depth': 11, 'scale_pos_weight': 8.230362925966384, 'reg_lambda': 5.631942116278893, 'reg_alpha': 3.9645876413158894e-05}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005757 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:35,704] Trial 80 finished with value: 0.0 and parameters: {'max_depth': 10, 'scale_pos_weight': 8.89648472340216, 'reg_lambda': 0.36759650314535486, 'reg_alpha': 0.0004616503578698285}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011991 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:37,881] Trial 81 finished with value: 0.602919708029197 and parameters: {'max_depth': 10, 'scale_pos_weight': 7.805153419156508, 'reg_lambda': 0.16755501667855388, 'reg_alpha': 8.222466164727654e-05}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015949 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:38,993] Trial 82 finished with value: 0.5424817518248175 and parameters: {'max_depth': 9, 'scale_pos_weight': 7.1584826835780655, 'reg_lambda': 0.4924666696220804, 'reg_alpha': 6.924975115702649e-05}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007500 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:39,772] Trial 83 finished with value: 0.6032116788321168 and parameters: {'max_depth': 11, 'scale_pos_weight': 7.889010958174492, 'reg_lambda': 1.4028857883967776, 'reg_alpha': 0.00025197685179391036}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007157 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:40,532] Trial 84 finished with value: 0.5886131386861314 and parameters: {'max_depth': 11, 'scale_pos_weight': 7.602651961788849, 'reg_lambda': 2.3359688512610326, 'reg_alpha': 0.00020951742793310716}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006113 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:41,254] Trial 85 finished with value: 0.0 and parameters: {'max_depth': 11, 'scale_pos_weight': 8.565920572764668, 'reg_lambda': 1.2366721692314002, 'reg_alpha': 0.0003761453918835419}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006756 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:42,022] Trial 86 finished with value: 0.494014598540146 and parameters: {'max_depth': 11, 'scale_pos_weight': 6.737612085468752, 'reg_lambda': 0.06366006932916675, 'reg_alpha': 0.0001090087994260334}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.326379 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:43,085] Trial 87 finished with value: 0.0 and parameters: {'max_depth': 3, 'scale_pos_weight': 8.144670758866617, 'reg_lambda': 0.18819024195881853, 'reg_alpha': 0.000813293897743886}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008177 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:43,877] Trial 88 finished with value: 0.42656934306569344 and parameters: {'max_depth': 10, 'scale_pos_weight': 6.325309673865998, 'reg_lambda': 0.998989802218437, 'reg_alpha': 4.905196733339637e-05}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016497 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:44,635] Trial 89 finished with value: 0.6002919708029197 and parameters: {'max_depth': 11, 'scale_pos_weight': 7.7993910055973545, 'reg_lambda': 6.171060302870469, 'reg_alpha': 0.00021873424548637526}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006036 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:45,413] Trial 90 finished with value: 0.5392700729927007 and parameters: {'max_depth': 10, 'scale_pos_weight': 7.139463120755624, 'reg_lambda': 3.3170683178430673, 'reg_alpha': 2.9164757404188786e-05}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008203 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:46,212] Trial 91 finished with value: 0.6014598540145986 and parameters: {'max_depth': 10, 'scale_pos_weight': 7.854320181505575, 'reg_lambda': 1.768374355673571, 'reg_alpha': 1.2802510124049133e-05}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021330 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:47,191] Trial 92 finished with value: 0.0 and parameters: {'max_depth': 9, 'scale_pos_weight': 8.286593990771946, 'reg_lambda': 0.5701419317666285, 'reg_alpha': 7.845510313417208e-05}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005296 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:47,853] Trial 93 finished with value: 0.5783941605839416 and parameters: {'max_depth': 10, 'scale_pos_weight': 7.4378140638152805, 'reg_lambda': 0.8102043717334427, 'reg_alpha': 0.00010299535571464251}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008169 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:48,473] Trial 94 finished with value: 0.0 and parameters: {'max_depth': 11, 'scale_pos_weight': 8.759308154042156, 'reg_lambda': 0.3479682764541228, 'reg_alpha': 0.00034636923351772373}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006145 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:49,139] Trial 95 finished with value: 0.0 and parameters: {'max_depth': 10, 'scale_pos_weight': 7.956820332151581, 'reg_lambda': 0.18778937302358178, 'reg_alpha': 0.0005358925044599509}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005980 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:49,761] Trial 96 finished with value: 0.0 and parameters: {'max_depth': 11, 'scale_pos_weight': 9.284191397977573, 'reg_lambda': 1.5825777615844385, 'reg_alpha': 0.00016834777761329364}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006481 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:50,430] Trial 97 finished with value: 0.0 and parameters: {'max_depth': 9, 'scale_pos_weight': 8.393648848275925, 'reg_lambda': 0.7759674301651267, 'reg_alpha': 2.654376931541534e-05}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007238 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:53,307] Trial 98 finished with value: 0.5813138686131387 and parameters: {'max_depth': 5, 'scale_pos_weight': 7.563833580644345, 'reg_lambda': 0.2594434653133516, 'reg_alpha': 0.0013742018436481433}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005726 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-03 11:31:54,025] Trial 99 finished with value: 0.548029197080292 and parameters: {'max_depth': 10, 'scale_pos_weight': 7.2179807314460405, 'reg_lambda': 0.1147236492477531, 'reg_alpha': 5.395714032260445e-05}. Best is trial 41 with value: 0.6154744525547445.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "CPU times: user 11min 25s, sys: 1.19 s, total: 11min 26s\n",
            "Wall time: 1min 48s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "í•´ë‹¹ ì½”ë“œëŠ” `optuna` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ `optimize_lgb` í•¨ìˆ˜ì˜ ê²°ê³¼ë¥¼ ìµœì í™”í•˜ëŠ” ê³¼ì •ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. `optuna`ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”ë¥¼ ìœ„í•œ Python ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.\n",
        "\n",
        "### ì½”ë“œ ë¶„ì„:\n",
        "\n",
        "1. **ì‹œê°„ ì¸¡ì •**:\n",
        "```python\n",
        "%%time\n",
        "```\n",
        "- `%%time`ì€ Jupyter ë…¸íŠ¸ë¶ì˜ ë§¤ì§ ëª…ë ¹ì–´ë¡œ, í•´ë‹¹ ì…€ì˜ ì‹¤í–‰ ì‹œê°„ì„ ì¸¡ì •í•©ë‹ˆë‹¤.\n",
        "\n",
        "2. **Optuna ìŠ¤í„°ë”” ìƒì„±**:\n",
        "```python\n",
        "opt_study = optuna.create_study(direction='maximize')\n",
        "```\n",
        "- `optuna.create_study` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒˆë¡œìš´ ìµœì í™” ìŠ¤í„°ë””ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "- `direction='maximize'`ëŠ” ìµœì í™”ì˜ ëª©í‘œê°€ ìµœëŒ€í™”ì„ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì¦‰, `optimize_lgb` í•¨ìˆ˜ì˜ ë°˜í™˜ê°’ì„ ìµœëŒ€ë¡œ ë§Œë“œëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•©ì„ ì°¾ëŠ” ê²ƒì´ ëª©í‘œì…ë‹ˆë‹¤.\n",
        "\n",
        "3. **ìµœì í™” ì‹œì‘**:\n",
        "```python\n",
        "opt_study.optimize(optimize_lgb, n_trials=100)\n",
        "```\n",
        "- `opt_study.optimize` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ `optimize_lgb` í•¨ìˆ˜ë¥¼ ëŒ€ìƒìœ¼ë¡œ ìµœì í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.\n",
        "- `n_trials=100`ì€ ì´ 100ë²ˆì˜ ì‹œë„(trial)ë¥¼ í†µí•´ ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ëŠ”ë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤. ê° ì‹œë„ë§ˆë‹¤ `optimize_lgb` í•¨ìˆ˜ëŠ” ë‹¤ë¥¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•©ìœ¼ë¡œ í˜¸ì¶œë˜ë©°, ê·¸ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì ì˜ ì¡°í•©ì„ íƒìƒ‰í•©ë‹ˆë‹¤.\n",
        "\n",
        "### ê²°ë¡ :\n",
        "\n",
        "ì´ ì½”ë“œëŠ” `optuna`ë¥¼ ì‚¬ìš©í•˜ì—¬ `optimize_lgb` í•¨ìˆ˜ê°€ ë°˜í™˜í•˜ëŠ” ê°’ì„ ìµœëŒ€ë¡œ í•˜ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•©ì„ ì°¾ëŠ” ê³¼ì •ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ìµœì í™” ê³¼ì •ì€ ì´ 100ë²ˆì˜ ì‹œë„ë¥¼ í†µí•´ ì´ë£¨ì–´ì§€ë©°, ê° ì‹œë„ì˜ ì‹¤í–‰ ì‹œê°„ì„ `%%time`ì„ í†µí•´ ì¸¡ì •í•©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "9zxmUc7qWhMa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`study`ê°€ ëë‚˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ ìµœìƒì˜ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆëŠ” `model hyperparameter`ë¥¼ ì¶œë ¥í•  ìˆ˜ ìˆë‹¤."
      ],
      "metadata": {
        "id": "_qUyODylTYt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_params = opt_study.best_params\n",
        "print(best_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvNwR7FePjF_",
        "outputId": "ff7aaa86-fc2b-4166-cb6f-309e55940b8c"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'max_depth': 10, 'scale_pos_weight': 7.926867998628939, 'reg_lambda': 0.8284413271486493, 'reg_alpha': 0.00010748920048463551}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1-7-3 Train Tuned Model"
      ],
      "metadata": {
        "id": "cHkAoDT1TkVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "clf = lgb.LGBMClassifier(random_state=rand, n_jobs=-1,\\\n",
        "                         **best_params)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "_, y_prob, y_pred = evaluate_class_mdl(clf, X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 764
        },
        "id": "qgMVyYQzPjDX",
        "outputId": "249f1062-baca-496d-e9da-397d0883f02e"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 7932, number of negative: 63304\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005038 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 319\n",
            "[LightGBM] [Info] Number of data points in the train set: 71236, number of used features: 53\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111348 -> initscore=-2.077043\n",
            "[LightGBM] [Info] Start training from score -2.077043\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "Accuracy_train:  0.6379\t\tAccuracy_test:   0.6103\n",
            "Precision_test:  0.1661\t\tRecall_test:     0.6155\n",
            "ROC-AUC_test:    0.6480\t\tF1_test:         0.2616\t\tMCC_test: 0.1441\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ğŸ¥ˆSession 2**  \n",
        "**â”— Error analysis**  \n",
        "---"
      ],
      "metadata": {
        "id": "K_TL7jjpUKAY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) Error Analysis ì†Œê°œ"
      ],
      "metadata": {
        "id": "4rUtWJ9QULY3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2-1. í˜„ì¬ê¹Œì§€ì˜ ê²°ê³¼ê°€ ìµœì„ ì¼ê¹Œ?"
      ],
      "metadata": {
        "id": "GpEu7Oo0UMxH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Confusion Matrix`ë¥¼ ìì„¸íˆ ì‚´í´ë³´ë©´ `False Positive`ë³´ë‹¤ `True Positive`ê°€ 60% ë” ë§ì§€ë§Œ `False Positive`ê°€ ê²°ê³¼ì ìœ¼ë¡œ ë” ì‹¬ê°í•œ ë¹„ìš©ì„ ë°œìƒì‹œí‚¨ë‹¤.\n",
        "ê·¸ëŸ¬ë‚˜ ìš°ë¦¬ê°€ ì„¤ì •í•œ ì œì•½ìœ¼ë¡œ ì¸í•´ `True Positive`ì˜ ì ˆë°˜ ì´ìƒì´ ê²°êµ­ ì¬ì…ì›ìœ¼ë¡œ ë¶„ë¥˜ë  ê²ƒë‹¤. (30ì¼ ì´ìƒ ì§€ë‚œ í™˜ìë„ ì¬ë¶„ë¥˜í•˜ëŠ” ì œì•½)"
      ],
      "metadata": {
        "id": "ieYxPv_XUNxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_orig_test['readmitted'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXLFwIvXPjAu",
        "outputId": "fdadff73-c550-48f8-b270-666b08a16700"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NO     16461\n",
              ">30    10644\n",
              "<30     3425\n",
              "Name: readmitted, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ëŒ€ë¶€ë¶„ì˜ ëª¨ë¸ ëª©í‘œê°€ 30ì¼ ì´ìƒ ì¬ì…ì›ì„ ì˜ˆìƒí•˜ë¯€ë¡œ ê¸ì •ì ì¸ í´ë˜ìŠ¤ê°€ ì´ ê·¸ë£¹ì„ ë…ì ì ìœ¼ë¡œ ë‚˜íƒ€ë‚¸ë‹¤.\n",
        "30ì¼ ì´ìƒ ì¬ì…ì› í•˜ì§€ ì•Šì€ ê²ƒì— ëŒ€í•œ `False Positive` ì˜¤ë¶„ë¥˜ëŠ” `NO` ì— ëŒ€í•œ ì˜¤ë¶„ë¥˜ë§Œí¼ ë‚˜ì˜ì§€ ì•Šë‹¤.\n",
        "ê·¸ë ‡ë‹¤ë©´ 30ì¼ ì´í›„ ì¬ì…ì›ì— ëŒ€í•œ ì˜¤íƒì§€ ë¹„ìœ¨ì€ ì–¼ë§ˆì¼ê¹Œìš”?"
      ],
      "metadata": {
        "id": "LN6GExJ8U-Tg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds_df = pd.DataFrame({'readmitted':X_orig_test['readmitted'],\\\n",
        "                         'y_true':y_test.astype(int),\\\n",
        "                         'y_pred':y_pred})\n",
        "preds_df[(preds_df.y_true==0) & (preds_df.y_pred==1)].value_counts(normalize=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhQmctydPi97",
        "outputId": "1fa73263-fc23-4674-f29e-fed2a56bd141"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "readmitted  y_true  y_pred\n",
              "NO          0       1         0.503639\n",
              ">30         0       1         0.496361\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### ì½”ë“œ ë¶„ì„:\n",
        "\n",
        "1. **ë°ì´í„°í”„ë ˆì„ ìƒì„±**:\n",
        "```python\n",
        "preds_df = pd.DataFrame({'readmitted':X_orig_test['readmitted'],\n",
        "                         'y_true':y_test.astype(int),\n",
        "                         'y_pred':y_pred})\n",
        "```\n",
        "- ìƒˆë¡œìš´ `preds_df` ë°ì´í„°í”„ë ˆì„ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "- ì´ ë°ì´í„°í”„ë ˆì„ì—ëŠ” 3ê°œì˜ ì—´ì´ í¬í•¨ë©ë‹ˆë‹¤:\n",
        "    - `readmitted`: `X_orig_test` ë°ì´í„°í”„ë ˆì„ì—ì„œ `readmitted` ì—´ì˜ ê°’ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
        "    - `y_true`: ì‹¤ì œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ë¼ë²¨ ê°’ì„ ì •ìˆ˜ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤.\n",
        "    - `y_pred`: ëª¨ë¸ë¡œë¶€í„° ì–»ì€ ì˜ˆì¸¡ê°’ì„ ì €ì¥í•©ë‹ˆë‹¤.\n",
        "\n",
        "2. **ì¡°ê±´ì— ë§ëŠ” ë°ì´í„° í•„í„°ë§**:\n",
        "```python\n",
        "preds_df[(preds_df.y_true==0) & (preds_df.y_pred==1)]\n",
        "```\n",
        "- `preds_df`ì—ì„œ ì‹¤ì œ ê°’(`y_true`)ì´ 0ì´ê³  ì˜ˆì¸¡ê°’(`y_pred`)ì´ 1ì¸ í–‰ë§Œ ì„ íƒí•©ë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì´ Negative(0)ë¡œ ë¶„ë¥˜í•´ì•¼ í•˜ëŠ” ìƒ˜í”Œì„ Positive(1)ë¡œ ì˜ëª» ì˜ˆì¸¡í•œ ê²½ìš°ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n",
        "\n",
        "3. **ê°’ì˜ ë¹ˆë„ ê³„ì‚°**:\n",
        "```python\n",
        ".value_counts(normalize=True)\n",
        "```\n",
        "- ì„ íƒëœ í–‰ì˜ ê° ê°’ì˜ ë¹ˆë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
        "- `normalize=True` ì˜µì…˜ì€ ê²°ê³¼ë¥¼ ì „ì²´ í–‰ì˜ ìˆ˜ë¡œ ë‚˜ëˆ  ë°±ë¶„ìœ¨ë¡œ ë°˜í™˜í•˜ë„ë¡ í•©ë‹ˆë‹¤.\n",
        "\n",
        "### ê²°ë¡ :\n",
        "\n",
        "ì´ ì½”ë“œëŠ” ì£¼ì–´ì§„ ì˜ˆì¸¡ ê²°ê³¼ì™€ ì‹¤ì œ ë¼ë²¨ì„ ë°”íƒ•ìœ¼ë¡œ ëª¨ë¸ì´ Negative í´ë˜ìŠ¤ë¥¼ Positiveë¡œ ì˜ëª» ë¶„ë¥˜í•œ ê²½ìš°ì˜ ë¹ˆë„ë¥¼ ë°±ë¶„ìœ¨ë¡œ ê³„ì‚°í•˜ê³  ì¶œë ¥í•©ë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì˜ False Positive ë¹„ìœ¨ì„ íŒŒì•…í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "0aog_mSTW2f7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ì‹¤ì œë¡œ ì˜¤íƒì§€ì˜ ê±°ì˜ ì ˆë°˜(ì „ì²´ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œì˜ 34.72%)ì´ ê²°êµ­ ì¬ì…ì›í•˜ì˜€ìœ¼ë¯€ë¡œ ì‚¬ì „ ì˜ˆë°© ì¡°ì¹˜ë¥¼ ì·¨í•˜ëŠ” ê²ƒì´ ë‚˜ìœ ê²ƒì´ ì•„ë‹ˆë¼ê³  íŒë‹¨ëœë‹¤.\n",
        "ìš°ë¦¬ê°€ ê³ ë ¤í•´ì•¼ í•  ì˜ëª»ëœ ê²°ê³¼ëŠ” ë‚˜ë¨¸ì§€ ì ˆë°˜ì— ëŒ€í•œ ë¶€ë¶„ì´ë‹¤."
      ],
      "metadata": {
        "id": "PLKvcIJ-V3iJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2-2. Errorë¥¼ ì£¼ì œ ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆ ë³´ì"
      ],
      "metadata": {
        "id": "xm_LT52DWFHm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2-2-1. Errorë¥¼ ê·¸ë£¹ë³„ë¡œ ë¹„êµí•´ë³´ê¸° ìœ„í•´ í•¨ìˆ˜ë¥¼ í•˜ë‚˜ ë§Œë“¤ì–´ë³´ì"
      ],
      "metadata": {
        "id": "z_e2oj1vWGed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_header_font():\n",
        "    return [dict(selector=\"th\", props=[(\"font-size\", \"14pt\")])]\n",
        "\n",
        "def metrics_by_group(s):\n",
        "    accuracy = metrics.accuracy_score(s.y_true, s.y_pred) * 100\n",
        "    precision = metrics.precision_score(s.y_true, s.y_pred) * 100\n",
        "    recall = metrics.recall_score(s.y_true, s.y_pred) * 100\n",
        "    f1 = metrics.f1_score(s.y_true, s.y_pred) * 100\n",
        "    roc_auc, fnr, fpr = np.nan, np.nan, np.nan\n",
        "    if len(np.unique(s.y_true)) == 2:\n",
        "        roc_auc = metrics.roc_auc_score(s.y_true, s.y_prob) * 100\n",
        "        tn, fp, fn, tp = metrics.confusion_matrix(s.y_true, s.y_pred).ravel()\n",
        "        fnr = (fn/(tp+fn)) * 100\n",
        "        fpr = (fp/(tn+fp)) * 100\n",
        "    support = len(s.y_true)\n",
        "\n",
        "    return pd.Series((support, accuracy, precision, recall, f1, roc_auc, fnr, fpr),\\\n",
        "                 index=['support', 'accuracy', 'precision', 'recall', 'f1', 'roc-auc', 'fnr', 'fpr'])\n",
        "\n",
        "def error_breakdown_by_group(mdl, y_true, y_prob, y_pred, orig_df, group_col, exclude_groups=None):\n",
        "\n",
        "    print(f\"Error breakdown for group '{group_col}'\")\n",
        "\n",
        "    predict_df = pd.DataFrame({group_col: orig_df[group_col].tolist(),\\\n",
        "                              'y_true': y_true,\n",
        "                              'y_pred': y_pred,\n",
        "                              'y_prob': y_prob}, index=y_true.index)\n",
        "    if exclude_groups is not None:\n",
        "        predict_df = predict_df[~predict_df[group_col].isin(exclude_groups)]\n",
        "\n",
        "    group_metrics_df = predict_df.groupby([group_col]).apply(metrics_by_group)\n",
        "\n",
        "    html = group_metrics_df.sort_values(by='support', ascending=False).style.\\\n",
        "            format({'support':'{:,.0f}', 'accuracy':'{:.1f}%', 'precision':'{:.1f}%', 'recall':'{:.1f}%',\\\n",
        "                    'f1':'{:.1f}%', 'roc-auc':'{:.1f}%', 'fnr':'{:.1f}%', 'fpr':'{:.1f}%'}).\\\n",
        "            set_properties(**{'font-size': '13pt'}).set_table_styles(set_header_font()).\\\n",
        "            highlight_max(subset=['accuracy','precision','recall','f1','roc-auc']).\\\n",
        "            highlight_min(subset=['fnr','fpr'])\n",
        "\n",
        "    return html\n",
        "\n",
        "def compare_confusion_matrices(y_true_1, y_pred_1, y_true_2, y_pred_2, group_1, group_2,\\\n",
        "                               plot=True, compare_fpr=False):\n",
        "\n",
        "    conf_matrix_1 = metrics.confusion_matrix(y_true_1, y_pred_1)\n",
        "    conf_matrix_2 = metrics.confusion_matrix(y_true_2, y_pred_2)\n",
        "\n",
        "    if plot:\n",
        "        fig, ax = plt.subplots(1,2,figsize=(12,5))\n",
        "        sns.heatmap(conf_matrix_1/np.sum(conf_matrix_1), annot=True,\\\n",
        "                    fmt='.2%', cmap='Blues', annot_kws={'size':16}, ax=ax[0])\n",
        "        ax[0].set_title(group_1 + ' Confusion Matrix', fontsize=14)\n",
        "        ax[0].set_xlabel('Predicted', fontsize=12)\n",
        "        ax[0].set_ylabel('Observed', fontsize=12)\n",
        "        sns.heatmap(conf_matrix_2/np.sum(conf_matrix_2), annot=True,\\\n",
        "                    fmt='.2%', cmap='Blues', annot_kws={'size':16}, ax=ax[1])\n",
        "        ax[1].set_title(group_2 + ' Confusion Matrix', fontsize=14)\n",
        "        ax[1].set_xlabel('Predicted', fontsize=12)\n",
        "        ax[1].set_ylabel('Observed', fontsize=12)\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "QV5lK-oAPfa5"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### 1. `set_header_font` í•¨ìˆ˜\n",
        "```python\n",
        "def set_header_font():\n",
        "    return [dict(selector=\"th\", props=[(\"font-size\", \"14pt\")])]\n",
        "```\n",
        "- í…Œì´ë¸”ì˜ í—¤ë”ì— ëŒ€í•œ í°íŠ¸ ì‚¬ì´ì¦ˆë¥¼ ì„¤ì •í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.\n",
        "- \"14pt\" í¬ê¸°ì˜ í°íŠ¸ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •ê°’ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
        "\n",
        "### 2. `metrics_by_group` í•¨ìˆ˜\n",
        "```python\n",
        "def metrics_by_group(s):\n",
        "```\n",
        "- ì£¼ì–´ì§„ ê·¸ë£¹ì˜ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ì–‘í•œ í‰ê°€ ì§€í‘œë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.\n",
        "\n",
        "```python\n",
        "accuracy = metrics.accuracy_score(s.y_true, s.y_pred) * 100\n",
        "precision = metrics.precision_score(s.y_true, s.y_pred) * 100\n",
        "recall = metrics.recall_score(s.y_true, s.y_pred) * 100\n",
        "f1 = metrics.f1_score(s.y_true, s.y_pred) * 100\n",
        "```\n",
        "- ì •í™•ë„, ì •ë°€ë„, ì¬í˜„ìœ¨, F1 ìŠ¤ì½”ì–´ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
        "\n",
        "```python\n",
        "if len(np.unique(s.y_true)) == 2:\n",
        "    ...\n",
        "```\n",
        "- ë¼ë²¨ ê°’ì´ 2ê°œì¸ ê²½ìš° (ì´ì§„ ë¶„ë¥˜)ì—ë§Œ ROC-AUC, FNR, FPRì„ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
        "\n",
        "### 3. `error_breakdown_by_group` í•¨ìˆ˜\n",
        "```python\n",
        "def error_breakdown_by_group(mdl, y_true, y_prob, y_pred, orig_df, group_col, exclude_groups=None):\n",
        "```\n",
        "- ì£¼ì–´ì§„ ê·¸ë£¹ë³„ë¡œ ì˜ˆì¸¡ ì˜¤ë¥˜ë¥¼ ë¶„ì„í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.\n",
        "\n",
        "```python\n",
        "predict_df = pd.DataFrame({...})\n",
        "```\n",
        "- ì˜ˆì¸¡ ê²°ê³¼ì™€ ì‹¤ì œ ë¼ë²¨, ê·¸ë£¹ ì •ë³´ë¥¼ í¬í•¨í•˜ëŠ” ìƒˆë¡œìš´ ë°ì´í„°í”„ë ˆì„ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "\n",
        "```python\n",
        "if exclude_groups is not None:\n",
        "    ...\n",
        "```\n",
        "- ì œì™¸í•  ê·¸ë£¹ì´ ì§€ì •ëœ ê²½ìš° í•´ë‹¹ ê·¸ë£¹ì„ ë°ì´í„°í”„ë ˆì„ì—ì„œ ì œì™¸í•©ë‹ˆë‹¤.\n",
        "\n",
        "```python\n",
        "group_metrics_df = predict_df.groupby([group_col]).apply(metrics_by_group)\n",
        "```\n",
        "- ê·¸ë£¹ë³„ë¡œ `metrics_by_group` í•¨ìˆ˜ë¥¼ ì ìš©í•˜ì—¬ ê° ê·¸ë£¹ì˜ í‰ê°€ ì§€í‘œë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
        "\n",
        "### 4. `compare_confusion_matrices` í•¨ìˆ˜\n",
        "```python\n",
        "def compare_confusion_matrices(y_true_1, y_pred_1, y_true_2, y_pred_2, group_1, group_2, plot=True, compare_fpr=False):\n",
        "```\n",
        "- ë‘ ê°œì˜ ê·¸ë£¹ì˜ í˜¼ë™ í–‰ë ¬ì„ ë¹„êµí•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.\n",
        "\n",
        "```python\n",
        "conf_matrix_1 = metrics.confusion_matrix(y_true_1, y_pred_1)\n",
        "conf_matrix_2 = metrics.confusion_matrix(y_true_2, y_pred_2)\n",
        "```\n",
        "- ë‘ ê·¸ë£¹ì— ëŒ€í•œ í˜¼ë™ í–‰ë ¬ì„ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
        "\n",
        "```python\n",
        "if plot:\n",
        "    ...\n",
        "```\n",
        "- `plot=True`ì¸ ê²½ìš°, ë‘ ê·¸ë£¹ì˜ í˜¼ë™ í–‰ë ¬ì„ íˆíŠ¸ë§µìœ¼ë¡œ ì‹œê°í™”í•˜ì—¬ í‘œì‹œí•©ë‹ˆë‹¤.\n",
        "\n",
        "### ê²°ë¡ :\n",
        "ì´ ì½”ë“œëŠ” ì£¼ì–´ì§„ ë°ì´í„°ì˜ ê·¸ë£¹ë³„ ì˜ˆì¸¡ ì˜¤ë¥˜ì™€ ì„±ëŠ¥ ì§€í‘œë¥¼ ë¶„ì„í•˜ê³  ì‹œê°í™”í•˜ëŠ” ì—¬ëŸ¬ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ì„ ì •ì˜í•©ë‹ˆë‹¤. ê° í•¨ìˆ˜ëŠ” íŠ¹ì • í‰ê°€ ì§€í‘œë¥¼ ê³„ì‚°í•˜ê±°ë‚˜, ê·¸ë£¹ë³„ ì˜ˆì¸¡ ì˜¤ë¥˜ë¥¼ ë¶„ì„í•˜ê±°ë‚˜, í˜¼ë™ í–‰ë ¬ì„ ë¹„êµ ë° ì‹œê°í™”í•˜ëŠ” ì—­í• ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "0m0-ZJ7kXgjs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2-2-2. ê·¸ë£¹ ë³„ ì—ëŸ¬ ë¶„ì„"
      ],
      "metadata": {
        "id": "us-X0rJGXNfI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FgW-D15mSin_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LtJ6ZcFxSilj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VJAIivQiSijI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JmjFwdoNSigb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}